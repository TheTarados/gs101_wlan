diff --git a/arch/arm64/boot/dts/exynos/google/gs101-oriole.dts b/arch/arm64/boot/dts/exynos/google/gs101-oriole.dts
index e58881c61..9a2afc055 100644
--- a/arch/arm64/boot/dts/exynos/google/gs101-oriole.dts
+++ b/arch/arm64/boot/dts/exynos/google/gs101-oriole.dts
@@ -278,3 +278,27 @@ &watchdog_cl0 {
 	timeout-sec = <30>;
 	status = "okay";
 };
+
+&pcie_1 {	/* pcie ch1 used for the connection with WiFi on Slider */
+	status = "okay";
+	num-lanes = <1>;
+	use-sicd = "true";
+	use-ia = "true";
+	use-l1ss = "true";
+	use-phy-isol-en = "false";
+	max-link-speed = <2>;
+	ep-device-type = <1>;
+	pcie,wlan-gpio = <&gpp14 2 0x1 /* WLAN_EN */ >;
+	vreg1-supply = <&reg_placeholder>;//<&m_ldo17_reg>;
+	vreg2-supply = <&reg_placeholder>;
+	pinctrl-0 = <&pcie1_clkreq &pcie1_perst &cfg_wlanen &wlan_host_wake &wlan_dev_wake>;
+
+	wlan {
+		compatible = "android,bcmdhd_wlan";
+		wl_reg_on = <&gpp14 2 0x1>; /* wlan reg_on pin */
+		wl_host_wake = <&gpa7 0 0x1>; /* wlan oob pin */
+		wl_dev_wake = <&gph2 5 0x1>; /* wlan dev pin */
+		ch-num = <1>;
+		status = "okay";
+	};
+};
diff --git a/arch/arm64/boot/dts/exynos/google/gs101-pinctrl.dtsi b/arch/arm64/boot/dts/exynos/google/gs101-pinctrl.dtsi
index a675f822a..722e34f31 100644
--- a/arch/arm64/boot/dts/exynos/google/gs101-pinctrl.dtsi
+++ b/arch/arm64/boot/dts/exynos/google/gs101-pinctrl.dtsi
@@ -202,6 +202,15 @@ gpa11: gpa11-gpio-bank {
 		interrupts = <GIC_SPI 65 IRQ_TYPE_LEVEL_HIGH 0>,
 			     <GIC_SPI 66 IRQ_TYPE_LEVEL_HIGH 0>;
 	};
+
+	wlan_host_wake: wlan_host_wake {
+		samsung,pins = "gpa7-0";
+		samsung,pin-function = <GS101_PIN_FUNC_EINT>;
+		samsung,pin-pud = <GS101_PIN_PULL_DOWN>;
+		samsung,pin-drv = <GS101_PIN_DRV_2_5_MA>;
+		samsung,pin-con-pdn = <GS101_PIN_PDN_PREV>;
+		samsung,pin-pud-pdn = <GS101_PIN_PULL_DOWN>;
+	};
 };
 
 &pinctrl_gsacore {
@@ -377,6 +386,15 @@ pcie1_perst: pcie1-perst-pins {
 		samsung,pin-drv = <GS101_PIN_DRV_10_MA>;
 		samsung,pin-con-pdn = <GS101_PIN_PDN_PREV>;
 	};
+
+	wlan_dev_wake: wlan_dev_wake {
+		samsung,pins = "gph2-5";
+		samsung,pin-function = <GS101_PIN_FUNC_INPUT>;
+		samsung,pin-pud = <GS101_PIN_PULL_DOWN>;
+		samsung,pin-drv = <GS101_PIN_DRV_2_5_MA>;
+		samsung,pin-con-pdn = <GS101_PIN_PDN_PREV>;
+		samsung,pin-pud-pdn = <GS101_PIN_PULL_DOWN>;
+	};
 };
 
 &pinctrl_peric0 {
@@ -971,6 +989,15 @@ spi1_cs_func: spi1-cs-func-pins {
 		samsung,pin-pud = <GS101_PIN_PULL_NONE>;
 		samsung,pin-drv = <GS101_PIN_DRV_2_5_MA>;
 	};
+
+	cfg_wlanen: cfg_wlanen {
+		samsung,pins = "gpp14-2";
+		samsung,pin-function = <GS101_PIN_FUNC_OUTPUT>;
+		samsung,pin-pud = <GS101_PIN_PULL_NONE>;
+		samsung,pin-drv = <GS101_PIN_DRV_2_5_MA>;
+		samsung,pin-con-pdn = <GS101_PIN_PDN_PREV>;
+		samsung,pin-pud-pdn = <GS101_PIN_PULL_UP>;
+	};
 };
 
 &pinctrl_peric1 {
diff --git a/arch/arm64/boot/dts/exynos/google/gs101.dtsi b/arch/arm64/boot/dts/exynos/google/gs101.dtsi
index c5335dd59..cc0c395d7 100644
--- a/arch/arm64/boot/dts/exynos/google/gs101.dtsi
+++ b/arch/arm64/boot/dts/exynos/google/gs101.dtsi
@@ -1474,6 +1474,50 @@ timer {
 		   <GIC_PPI 11 (GIC_CPU_MASK_SIMPLE(8) | IRQ_TYPE_LEVEL_LOW) 0>,
 		   <GIC_PPI 10 (GIC_CPU_MASK_SIMPLE(8) | IRQ_TYPE_LEVEL_LOW) 0>;
 	};
+
+	/* HSI2 GEN4_1 */
+	pcie_1:	pcie@14520000 {
+		compatible = "samsung,exynos-pcie-rc";
+		gpios = <&gph2 0 0x1 /* PERST */>;
+		reg = <0x0 0x14520000 0x2000	/* elbi base */
+			0x0 0x14550000 0x2000	/* phy base */
+			0x0 0x14420000 0x2000	/* sysreg base */
+			0x0 0x14800000 0x301000	/* DBI base */
+			0x0 0x14540000 0x1000	/* phy pcs base */
+			0x0 0x60FFE000 0x2000	/* configuration space */
+			0x0 0x14500000 0x1000>;	/* I/A space */
+		reg-names = "elbi", "phy", "sysreg", "dbi", "pcs", "config", "ia";
+		interrupts = <GIC_SPI 509 IRQ_TYPE_LEVEL_HIGH 0>;
+		#interrupt-cells = <1>;
+		interrupt-map-mask = <0 0 0 0>;
+		interrupt-map = <0 0 0 0 &gic 0 509 0x4 0>;
+		samsung,syscon-phandle = <&pmu_system_controller>;
+		pinctrl-names = "active";
+		pinctrl-0 = <&pcie1_clkreq &pcie1_perst>;
+		#address-cells = <3>;
+		#size-cells = <2>;
+		device_type = "pci";
+		/* non-prefetchable memory */
+		ranges = <0x82000000 0 0x60000000 0 0x60000000 0 0xFF0000>;
+		ip-ver = <0x984500>;	/* gs101 */
+		num-lanes = <2>;
+		ch-num = <1>;
+		pcie-clk-num = <0>;
+		phy-clk-num = <0>;
+		pcie-pm-qos-int = <200000>;
+		use-cache-coherency = "false";
+		use-pcieon-sleep = "false";
+		use-msi = "false";
+		use-sicd = "false";
+		use-sysmmu = "false";
+		use-ia = "false";
+		use-l1ss = "false";
+		use-secure-atu = "false";
+		pmu-offset = <0x3ec4>;
+		max-link-speed = <3>;
+		//s2mpu = <&s2mpu_hsi2>;
+		status = "disabled";
+	};
 };
 
 #include "gs101-pinctrl.dtsi"
diff --git a/drivers/iommu/exynos-pcie-iommu-exp.h b/drivers/iommu/exynos-pcie-iommu-exp.h
new file mode 100644
index 000000000..9b92c4327
--- /dev/null
+++ b/drivers/iommu/exynos-pcie-iommu-exp.h
@@ -0,0 +1,23 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * PCIe Exynos IOMMU driver header file
+ *
+ * Copyright (C) 2021 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ *
+ */
+
+#ifndef _EXYNOS_PCIE_IOMMU_EXP_H_
+#define _EXYNOS_PCIE_IOMMU_EXP_H_
+
+int pcie_iommu_map(unsigned long iova, phys_addr_t paddr, size_t size,
+		   int prot, int hsi_block_num);
+size_t pcie_iommu_unmap(unsigned long iova, size_t size, int hsi_block_num);
+
+void pcie_sysmmu_set_use_iocc(int hsi_block_num);
+void pcie_sysmmu_enable(int hsi_block_num);
+void pcie_sysmmu_disable(int hsi_block_num);
+void pcie_sysmmu_all_buff_free(int hsi_block_num);
+void print_pcie_sysmmu_tlb(int hsi_block_num);
+
+#endif /* _EXYNOS_PCIE_IOMMU_EXP_H_ */
diff --git a/drivers/pci/controller/dwc/Kconfig b/drivers/pci/controller/dwc/Kconfig
index b6d6778b0..0a62e9e5f 100644
--- a/drivers/pci/controller/dwc/Kconfig
+++ b/drivers/pci/controller/dwc/Kconfig
@@ -352,6 +352,19 @@ config PCI_EXYNOS
 	  hardware and therefore the driver re-uses the DesignWare core
 	  functions to implement the driver.
 
+config PCIE_EXYNOS_RC
+	tristate "Samsung Exynos PCIe root controller"
+	depends on PCI
+	depends on OF
+	select PCIEPORTBUS
+	select PCIE_DW_HOST
+
+config PCI_EXYNOS_CAL_GS101
+	tristate "Samsung Exynos PCIe PHY CAL for GS101"
+	depends on PCI
+	depends on OF
+	select PCIE_DW_HOST
+
 config PCIE_FU740
 	bool "SiFive FU740 PCIe controller"
 	depends on PCI_MSI
diff --git a/drivers/pci/controller/dwc/Makefile b/drivers/pci/controller/dwc/Makefile
index a8308d9ea..2945d7784 100644
--- a/drivers/pci/controller/dwc/Makefile
+++ b/drivers/pci/controller/dwc/Makefile
@@ -6,6 +6,8 @@ obj-$(CONFIG_PCIE_DW_PLAT) += pcie-designware-plat.o
 obj-$(CONFIG_PCIE_BT1) += pcie-bt1.o
 obj-$(CONFIG_PCI_DRA7XX) += pci-dra7xx.o
 obj-$(CONFIG_PCI_EXYNOS) += pci-exynos.o
+obj-$(CONFIG_PCIE_EXYNOS_RC) += pcie-exynos-rc.o
+obj-$(CONFIG_PCI_EXYNOS_CAL_GS101) += pcie-exynos-gs101-rc-cal.o
 obj-$(CONFIG_PCIE_FU740) += pcie-fu740.o
 obj-$(CONFIG_PCI_IMX6) += pci-imx6.o
 obj-$(CONFIG_PCIE_SPEAR13XX) += pcie-spear13xx.o
diff --git a/drivers/pci/controller/dwc/pcie-designware-host.c b/drivers/pci/controller/dwc/pcie-designware-host.c
index ffaded8f2..b4776c8e8 100644
--- a/drivers/pci/controller/dwc/pcie-designware-host.c
+++ b/drivers/pci/controller/dwc/pcie-designware-host.c
@@ -86,6 +86,8 @@ irqreturn_t dw_handle_msi_irq(struct dw_pcie_rp *pp)
 	return ret;
 }
 
+EXPORT_SYMBOL_GPL(dw_handle_msi_irq);
+
 /* Chained MSI interrupt service routine */
 static void dw_chained_msi_isr(struct irq_desc *desc)
 {
diff --git a/drivers/pci/controller/dwc/pcie-exynos-common.h b/drivers/pci/controller/dwc/pcie-exynos-common.h
new file mode 100644
index 000000000..ff316aa12
--- /dev/null
+++ b/drivers/pci/controller/dwc/pcie-exynos-common.h
@@ -0,0 +1,385 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe host controller driver for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ */
+
+#ifndef __PCIE_EXYNOS_H
+#define __PCIE_EXYNOS_H
+
+#if IS_ENABLED(CONFIG_SOC_EXYNOS8890)
+#define PCI_DEVICE_ID_EXYNOS	0xa544
+#define GPIO_DEBUG_SFR		0x15601068
+#else
+#define PCI_DEVICE_ID_EXYNOS	0xecec
+#define GPIO_DEBUG_SFR		0x0
+#endif
+
+#define MAX_TIMEOUT		4800	/* about 24ms(link up wait time) x 2 */
+#define MAX_TIMEOUT_SPEEDCHANGE	10000	/* about 1s(link recovery wait time) */
+#define MAX_L2_TIMEOUT		2000
+#define MAX_L1_EXIT_TIMEOUT	300
+#define LNKRCVYWAIT_TIMEOUT	500
+#define PLL_LOCK_TIMEOUT	500
+#define RX_OC_TIMEOUT		500
+#define ID_MASK			0xffff
+#define TPUT_THRESHOLD		150
+#define MAX_RC_NUM		2
+#define PHY_VREG_ON		1
+#define PHY_VREG_OFF		0
+
+#define to_exynos_pcie(x)	dev_get_drvdata((x)->dev)
+
+#define PCIE_BUS_PRIV_DATA(pdev) \
+	((struct dw_pcie_rp *)(pdev)->bus->sysdata)
+
+#define MAX_PCIE_PIN_STATE	2
+#define PCIE_PIN_ACTIVE		0
+#define PCIE_PIN_IDLE		1
+
+#define APP_REQ_EXIT_L1_MODE		0x1
+
+#define CAP_NEXT_OFFSET_MASK		(0xFF << 8)
+#define CAP_ID_MASK			0xFF
+/* add on PCI spec 4.0 */
+#define PCI_EXT_CAP_ID_DLINK_FEATURE	0x25
+#define PCI_EXT_CAP_ID_PHYLYR16		0x26
+#define PCI_EXT_CAP_ID_MRGN_EXT		0x27
+
+#define PCIE_PHY_ISOLATION	(0)
+#define PCIE_PHY_BYPASS		(1)
+
+/*
+ * these defines are exported in a sysfs for user space functions to
+ * have common values for link states.
+ */
+
+enum link_states {
+	L0 = 0,
+	L0S,
+	L1,
+	L11,
+	L12,
+	L2,
+	L2V,
+	UNKNOWN
+};
+
+enum __ltssm_states {
+	S_DETECT_QUIET                  = 0x00,
+	S_DETECT_ACT                    = 0x01,
+	S_POLL_ACTIVE                   = 0x02,
+	S_POLL_COMPLIANCE               = 0x03,
+	S_POLL_CONFIG                   = 0x04,
+	S_PRE_DETECT_QUIET              = 0x05,
+	S_DETECT_WAIT                   = 0x06,
+	S_CFG_LINKWD_START              = 0x07,
+	S_CFG_LINKWD_ACEPT              = 0x08,
+	S_CFG_LANENUM_WAIT              = 0x09,
+	S_CFG_LANENUM_ACEPT             = 0x0A,
+	S_CFG_COMPLETE                  = 0x0B,
+	S_CFG_IDLE			= 0x0C,
+	S_RCVRY_LOCK                    = 0x0D,
+	S_RCVRY_SPEED                   = 0x0E,
+	S_RCVRY_RCVRCFG                 = 0x0F,
+	S_RCVRY_IDLE                    = 0x10,
+	S_L0				= 0x11,
+	S_L0S				= 0x12,
+	S_L123_SEND_EIDLE               = 0x13,
+	S_L1_IDLE			= 0x14,
+	S_L2_IDLE			= 0x15,
+	S_L2_WAKE			= 0x16,
+	S_DISABLED_ENTRY                = 0x17,
+	S_DISABLED_IDLE                 = 0x18,
+	S_DISABLED			= 0x19,
+	S_LPBK_ENTRY                    = 0x1A,
+	S_LPBK_ACTIVE                   = 0x1B,
+	S_LPBK_EXIT			= 0x1C,
+	S_LPBK_EXIT_TIMEOUT             = 0x1D,
+	S_HOT_RESET_ENTRY               = 0x1E,
+	S_HOT_RESET			= 0x1F,
+	GEN3_LINKUP                     = 0x91,
+};
+
+#define LINK_STATE_DISP(state)	\
+	((state) == S_DETECT_QUIET)       ? "DETECT QUIET" : \
+	((state) == S_DETECT_ACT)         ? "DETECT ACT" : \
+	((state) == S_POLL_ACTIVE)        ? "POLL ACTIVE" : \
+	((state) == S_POLL_COMPLIANCE)    ? "POLL COMPLIANCE" : \
+	((state) == S_POLL_CONFIG)        ? "POLL CONFIG" : \
+	((state) == S_PRE_DETECT_QUIET)   ? "PRE DETECT QUIET" : \
+	((state) == S_DETECT_WAIT)        ? "DETECT WAIT" : \
+	((state) == S_CFG_LINKWD_START)   ? "CFG LINKWD START" : \
+	((state) == S_CFG_LINKWD_ACEPT)   ? "CFG LINKWD ACEPT" : \
+	((state) == S_CFG_LANENUM_WAIT)   ? "CFG LANENUM WAIT" : \
+	((state) == S_CFG_LANENUM_ACEPT)  ? "CFG LANENUM ACEPT" : \
+	((state) == S_CFG_COMPLETE)       ? "CFG COMPLETE" : \
+	((state) == S_CFG_IDLE)           ? "CFG IDLE" : \
+	((state) == S_RCVRY_LOCK)         ? "RCVRY LOCK" : \
+	((state) == S_RCVRY_SPEED)        ? "RCVRY SPEED" : \
+	((state) == S_RCVRY_RCVRCFG)      ? "RCVRY RCVRCFG" : \
+	((state) == S_RCVRY_IDLE)         ? "RCVRY IDLE" : \
+	((state) == S_L0)                 ? "L0" : \
+	((state) == S_L0S)                ? "L0s" : \
+	((state) == S_L123_SEND_EIDLE)    ? "L123 SEND EIDLE" : \
+	((state) == S_L1_IDLE)            ? "L1 IDLE " : \
+	((state) == S_L2_IDLE)            ? "L2 IDLE"  : \
+	((state) == S_L2_WAKE)            ? "L2 _WAKE" : \
+	((state) == S_DISABLED_ENTRY)     ? "DISABLED ENTRY" : \
+	((state) == S_DISABLED_IDLE)      ? "DISABLED IDLE" : \
+	((state) == S_DISABLED)           ? "DISABLED" : \
+	((state) == S_LPBK_ENTRY)         ? "LPBK ENTRY " : \
+	((state) == S_LPBK_ACTIVE)        ? "LPBK ACTIVE" : \
+	((state) == S_LPBK_EXIT)          ? "LPBK EXIT" : \
+	((state) == S_LPBK_EXIT_TIMEOUT)  ? "LPBK EXIT TIMEOUT" : \
+	((state) == S_HOT_RESET_ENTRY)    ? "HOT RESET ENTRY" : \
+	((state) == S_HOT_RESET)          ? "HOT RESET" : \
+	((state) == GEN3_LINKUP)		? "GEN3_L0" : \
+	" Unknown state..!! "
+
+#define CAP_ID_NAME(id)	\
+	((id) == PCI_CAP_ID_PM)	?	"Power Management" :	\
+	((id) == PCI_CAP_ID_MSI)	?	"Message Signalled Interrupts" :	\
+	((id) == PCI_CAP_ID_EXP)	?	"PCI Express" :	\
+	" Unknown id..!!"
+
+#define EXT_CAP_ID_NAME(id)	\
+	((id) == PCI_EXT_CAP_ID_ERR)	?	"Advanced Error Reporting" :	\
+	((id) == PCI_EXT_CAP_ID_VC)	?	"Virtual Channel Capability" :	\
+	((id) == PCI_EXT_CAP_ID_DSN)	?	"Device Serial Number" :	\
+	((id) == PCI_EXT_CAP_ID_PWR)	?	"Power Budgeting" :	\
+	((id) == PCI_EXT_CAP_ID_RCLD)	?	"RC Link Declaration" :	\
+	((id) == PCI_EXT_CAP_ID_SECPCI)	?	"Secondary PCIe Capability" :	\
+	((id) == PCI_EXT_CAP_ID_L1SS)	?	"L1 PM Substates" :	\
+	((id) == PCI_EXT_CAP_ID_DLINK_FEATURE)	?	"Data Link Feature" :	\
+	((id) == PCI_EXT_CAP_ID_PHYLYR16)	?	"Physical Layer 16GT/s"	:	\
+	((id) == PCI_EXT_CAP_ID_MRGN_EXT)	?	"Physical Layer 16GT/s Margining" :	\
+	" Unknown id ..!!"
+
+struct regmap;
+
+struct power_stats {
+	u64	count;
+	u64	duration;
+	u64	last_entry_ts;
+};
+
+#define LINK_STATS_AVG_SAMPLE_SIZE 50
+
+struct link_stats {
+	u32 link_down_irq_count;
+	u32 link_down_irq_count_reported;
+	u32 cmpl_timeout_irq_count;
+	u32 cmpl_timeout_irq_count_reported;
+	u32 link_up_failure_count;
+	u32 link_up_failure_count_reported;
+	u32 link_recovery_failure_count;
+	u32 link_recovery_failure_count_reported;
+	u32 pll_lock_time_avg;
+	u32 link_up_time_avg;
+};
+
+struct exynos_pcie_clks {
+	struct clk	*pcie_clks[10];
+	struct clk	*phy_clks[3];
+};
+
+enum exynos_pcie_state {
+	STATE_LINK_DOWN = 0,
+	STATE_LINK_UP_TRY,
+	STATE_LINK_DOWN_TRY,
+	STATE_LINK_UP,
+};
+
+#define EXYNOS_PCIE_STATE_NAME(state)						\
+	((state) == STATE_LINK_DOWN)	?	"LINK_DOWN" :			\
+	((state) == STATE_LINK_UP_TRY)	?	"LINK_UP_TRY" :			\
+	((state) == STATE_LINK_DOWN_TRY)	?	"LINK_DOWN_TRY" :	\
+	((state) == STATE_LINK_UP)	?	"LINK_UP"	:		\
+	" Unknown state ...!!"
+
+#define PRINT_STATUS(buf, ret, status, count, duration, last_ts) ( \
+scnprintf((buf) + (ret), PAGE_SIZE - (ret), \
+	  "Link %s:\n" \
+	  "  Cumulative count: 0x%llx\n" \
+	  "  Cumulative duration msec: 0x%llx\n" \
+	  "  Last entry timestamp msec: 0x%llx\n", \
+	  status, count, duration, last_ts) \
+)
+
+struct exynos_pcie;
+
+struct pcie_phyops {
+	void (*phy_check_rx_elecidle)(void *phy_pcs_base_regs, int val, int ch_num);
+	void (*phy_all_pwrdn)(struct exynos_pcie *exynos_pcie, int ch_num);
+	void (*phy_all_pwrdn_clear)(struct exynos_pcie *exynos_pcie, int ch_num);
+	void (*phy_config)(struct exynos_pcie *exynos_pcie, int ch_num);
+	void (*phy_config_regmap)(void *phy_base_regs, void *phy_pcs_base_regs,
+				  struct regmap *sysreg_phandle, void *elbi_base_regs, int ch_num);
+	int (*phy_eom)(struct device *dev, void *phy_base_regs);
+};
+
+struct exynos_pcie_ops {
+	int (*poweron)(int ch_num);
+	void (*poweroff)(int ch_num);
+	int (*rd_own_conf)(struct dw_pcie_rp *pp, int where, int size, u32 *val);
+	int (*wr_own_conf)(struct dw_pcie_rp *pp, int where, int size, u32 val);
+	int (*rd_other_conf)(struct dw_pcie_rp *pp, struct pci_bus *bus, u32 devfn, int where,
+			     int size, u32 *val);
+	int (*wr_other_conf)(struct dw_pcie_rp *pp, struct pci_bus *bus, u32 devfn, int where,
+			     int size, u32 val);
+};
+
+struct exynos_pcie {
+	struct dw_pcie		*pci;
+#if IS_ENABLED(CONFIG_GS_S2MPU)
+	struct list_head	phys_mem_list;
+#endif
+	struct s2mpu_info	*s2mpu;
+	struct pci_dev		*ep_pci_dev;
+	void __iomem		*elbi_base;
+	void __iomem		*udbg_base;
+	void __iomem		*phy_base;
+	void __iomem		*sysreg_base;
+	void __iomem		*rc_dbi_base;
+	void __iomem		*phy_pcs_base;
+	void __iomem		*ia_base;
+	u32			*pma_regs;
+	u32			elbi_base_physical_addr;
+	u32			phy_base_physical_addr;
+	u32			ia_base_physical_addr;
+	u32			ep_l1ss_cap_off;
+	u32			ep_link_ctrl_off;
+	u32			ep_l1ss_ctrl1_off;
+	u32			ep_l1ss_ctrl2_off;
+	unsigned int		pci_cap[48];
+	unsigned int		pci_ext_cap[48];
+	struct regmap		*pmureg;
+	phys_addr_t		pmu_alive_pa;
+	struct regmap		*sysreg;
+	int			perst_gpio;
+	int			num_lanes;
+	int			ch_num;
+	int			pcie_clk_num;
+	int			phy_clk_num;
+	enum exynos_pcie_state	state;
+	int			probe_ok;
+	int			l1ss_enable;
+	int			linkdown_cnt;
+	int			idle_ip_index;
+	int			separated_msi;
+	bool			use_msi;
+	bool use_secure_atu;
+	bool			use_cache_coherency;
+	bool			use_sicd;
+	bool			use_pcieon_sleep;
+	bool			atu_ok;
+	bool			use_sysmmu;
+	bool			use_ia;
+	bool			use_l1ss;
+	bool			use_nclkoff_en;
+	bool                    cpl_timeout_recovery;
+	bool			sudden_linkdown;
+	bool			pma_regs_valid;
+	spinlock_t		conf_lock;		/* pcie config - link status change */
+	spinlock_t		reg_lock;		/* pcie config - reg_lock(reserved) */
+	spinlock_t		pcie_l1_exit_lock;	/* pcie l1.2 exit - ctrl_id_state */
+	spinlock_t		power_stats_lock;	/* pcie config - power state change */
+	spinlock_t		s2mpu_refcnt_lock;
+	struct workqueue_struct	*pcie_wq;
+	struct exynos_pcie_clks	clks;
+	struct pci_dev		*pci_dev;
+	struct pci_saved_state	*pci_saved_configs;
+	struct notifier_block	power_mode_nb;
+	struct notifier_block   ss_dma_mon_nb;
+	struct delayed_work	dislink_work;
+	struct delayed_work	cpl_timeout_work;
+	struct exynos_pcie_register_event *event_reg;
+#if IS_ENABLED(CONFIG_PM_DEVFREQ)
+	unsigned int            int_min_lock;
+#endif
+	u32			ip_ver;
+	struct pcie_phyops	phy_ops;
+	struct exynos_pcie_ops	exynos_pcie_ops;
+	int			l1ss_ctrl_id_state;
+	struct workqueue_struct *pcie_wq_l1ss;
+	struct delayed_work     l1ss_boot_delay_work;
+	int			boot_cnt;
+	int			work_l1ss_cnt;
+	int			ep_device_type;
+	int			max_link_speed;
+	struct power_stats	link_up;
+	struct power_stats	link_down;
+	struct link_stats	link_stats;
+
+	struct pinctrl		*pcie_pinctrl;
+	struct pinctrl_state	*pin_state[MAX_PCIE_PIN_STATE];
+	struct pcie_eom_result **eom_result;
+
+	int wlan_gpio;
+	int ssd_gpio;
+	u32 pmu_offset;
+	u32 linkup_offset;
+	/* evt0 : 0, evt1: 1 .. */
+	u32 chip_ver;
+
+	u32 app_req_exit_l1;
+	u32 app_req_exit_l1_mode;
+
+	u32 btl_target_addr;
+	u32 btl_offset;
+	u32 btl_size;
+
+	bool use_phy_isol_con;
+	int phy_control;
+
+	bool pcie_must_resume;
+	int pcieon_sleep_enable_cnt;
+
+	struct mutex power_onoff_lock;
+};
+
+#define PCIE_MAX_MSI_NUM	(8)
+#define PCIE_MAX_SEPA_IRQ_NUM	(5)
+#define PCIE_START_SEP_MSI_VEC	(1)
+#define PCIE_MSI_MAX_VEC_NUM	(32)
+#define PCIE_DOMAIN_MAX_IRQ	(256)
+
+struct separated_msi_vector {
+	int is_used;
+	int irq;
+	void *context;
+	irq_handler_t msi_irq_handler;
+	int flags;
+};
+
+#define PCIE_EXYNOS_OP_READ(base, type)						\
+static inline type exynos_##base##_read(struct exynos_pcie *pcie, u32 reg)	\
+{										\
+		u32 data = 0;							\
+		data = readl((pcie->base##_base) + reg);			\
+		return (type)data;						\
+}										\
+
+#define PCIE_EXYNOS_OP_WRITE(base, type)							\
+static inline void exynos_##base##_write(struct exynos_pcie *pcie, type value, type reg)	\
+{												\
+		writel(value, pcie->base##_base + reg);						\
+}
+
+PCIE_EXYNOS_OP_READ(elbi, u32);
+PCIE_EXYNOS_OP_READ(udbg, u32);
+PCIE_EXYNOS_OP_READ(phy, u32);
+PCIE_EXYNOS_OP_READ(phy_pcs, u32);
+PCIE_EXYNOS_OP_READ(sysreg, u32);
+PCIE_EXYNOS_OP_READ(ia, u32);
+PCIE_EXYNOS_OP_WRITE(elbi, u32);
+PCIE_EXYNOS_OP_WRITE(udbg, u32);
+PCIE_EXYNOS_OP_WRITE(phy, u32);
+PCIE_EXYNOS_OP_WRITE(phy_pcs, u32);
+PCIE_EXYNOS_OP_WRITE(sysreg, u32);
+PCIE_EXYNOS_OP_WRITE(ia, u32);
+
+#endif
diff --git a/drivers/pci/controller/dwc/pcie-exynos-gs101-rc-cal.c b/drivers/pci/controller/dwc/pcie-exynos-gs101-rc-cal.c
new file mode 100644
index 000000000..3df0fa058
--- /dev/null
+++ b/drivers/pci/controller/dwc/pcie-exynos-gs101-rc-cal.c
@@ -0,0 +1,488 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe phy driver for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ *
+ * Author: Hongseock Kim <hongpooh.kim@samsung.com>
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/of_gpio.h>
+#include <linux/pci.h>
+#include <linux/delay.h>
+#include <linux/exynos-pci-noti.h>
+#include <linux/regmap.h>
+#include "pcie-designware.h"
+#include "pcie-exynos-common.h"
+#include "pcie-exynos-rc.h"
+
+#if IS_ENABLED(CONFIG_EXYNOS_OTP)
+#include <linux/exynos_otp.h>
+#endif
+
+/* avoid checking rx elecidle when access DBI */
+static void exynos_pcie_rc_phy_check_rx_elecidle(void *phy_pcs_base_regs, int val, int ch_num)
+{
+	/*
+	 * Todo: need guide
+	 */
+}
+
+/* PHY all power down */
+static void exynos_pcie_rc_phy_all_pwrdn(struct exynos_pcie *exynos_pcie, int ch_num)
+{
+	void __iomem *phy_base_regs = exynos_pcie->phy_base;
+	u32 val;
+
+	writel(0xA8, phy_base_regs + 0x404);
+	writel(0x20, phy_base_regs + 0x408);
+	writel(0x0A, phy_base_regs + 0x40C);
+
+	writel(0x0A, phy_base_regs + 0x800);
+	writel(0xBF, phy_base_regs + 0x804);
+	writel(0x02, phy_base_regs + 0xA40);
+	writel(0x2A, phy_base_regs + 0xA44);
+	writel(0xAA, phy_base_regs + 0xA48);
+	writel(0xA8, phy_base_regs + 0xA4C);
+	writel(0x80, phy_base_regs + 0xA50);
+
+	writel(0x0A, phy_base_regs + 0x1000);
+	writel(0xBF, phy_base_regs + 0x1004);
+	writel(0x02, phy_base_regs + 0x1240);
+	writel(0x2A, phy_base_regs + 0x1244);
+	writel(0xAA, phy_base_regs + 0x1248);
+	writel(0xA8, phy_base_regs + 0x124C);
+	writel(0x80, phy_base_regs + 0x1250);
+
+	/* Disable PHY PMA */
+	val = readl(phy_base_regs + 0x400);
+	val &= ~(0x1 << 7);
+	writel(val, phy_base_regs + 0x400);
+}
+
+/* PHY all power down clear */
+static void exynos_pcie_rc_phy_all_pwrdn_clear(struct exynos_pcie *exynos_pcie, int ch_num)
+{
+	void __iomem *phy_base_regs = exynos_pcie->phy_base;
+
+	writel(0x28, phy_base_regs + 0xD8);
+	mdelay(1);
+
+	writel(0x00, phy_base_regs + 0x404);
+	writel(0x00, phy_base_regs + 0x408);
+	writel(0x00, phy_base_regs + 0x40C);
+
+	writel(0x00, phy_base_regs + 0x800);
+	writel(0x00, phy_base_regs + 0x804);
+	writel(0x00, phy_base_regs + 0xA40);
+	writel(0x00, phy_base_regs + 0xA44);
+	writel(0x00, phy_base_regs + 0xA48);
+	writel(0x00, phy_base_regs + 0xA4C);
+	writel(0x00, phy_base_regs + 0xA50);
+
+	writel(0x00, phy_base_regs + 0x1000);
+	writel(0x00, phy_base_regs + 0x1004);
+	writel(0x00, phy_base_regs + 0x1240);
+	writel(0x00, phy_base_regs + 0x1244);
+	writel(0x00, phy_base_regs + 0x1248);
+	writel(0x00, phy_base_regs + 0x124C);
+	writel(0x00, phy_base_regs + 0x1250);
+}
+
+#if IS_ENABLED(CONFIG_EXYNOS_OTP)
+void exynos_pcie_rc_pcie_phy_otp_config(void *phy_base_regs, int ch_num)
+{
+	/* To be updated */
+}
+#endif
+
+#define LCPLL_REF_CLK_SEL	(0x3 << 4)
+
+static void exynos_pcie_rc_pcie_phy_config(struct exynos_pcie *exynos_pcie, int ch_num)
+{
+	void __iomem *elbi_base_regs = exynos_pcie->elbi_base;
+	void __iomem *phy_base_regs = exynos_pcie->phy_base;
+	void __iomem *phy_pcs_base_regs = exynos_pcie->phy_pcs_base;
+	int num_lanes = exynos_pcie->num_lanes;
+	u32 val;
+	u32 i;
+
+	/* init. input clk path */
+	writel(0x28, phy_base_regs + 0xD8);
+
+	/* PCS MUX glitch W/A */
+	val = readl(exynos_pcie->phy_pcs_base + 0x008);
+	val |= (1 << 7);
+	writel(val, phy_pcs_base_regs + 0x008);
+	if (num_lanes == 2) {
+		val = readl(exynos_pcie->phy_pcs_base + 0x808);
+		val |= (1 << 7);
+		writel(val, phy_pcs_base_regs + 0x808);
+	}
+
+	/* PHY CMN_RST, INIT_RST, PORT_RST Assert */
+	writel(0x1, elbi_base_regs + 0x1404);
+	writel(0x1, elbi_base_regs + 0x1408);
+	writel(0x1, elbi_base_regs + 0x1400);
+	writel(0x0, elbi_base_regs + 0x1404);
+	writel(0x0, elbi_base_regs + 0x1408);
+	writel(0x0, elbi_base_regs + 0x1400);
+	udelay(10);
+
+	writel(0x1, elbi_base_regs + 0x1404);
+	udelay(10);
+
+	/* pma_setting */
+	/* Common */
+	writel(0x50, phy_base_regs + 0x018);
+	writel(0x33, phy_base_regs + 0x048);
+	writel(0x01, phy_base_regs + 0x068);
+	writel(0x12, phy_base_regs + 0x070);
+	writel(0x00, phy_base_regs + 0x08C);
+	writel(0x21, phy_base_regs + 0x090);
+	writel(0x14, phy_base_regs + 0x0B0);
+	writel(0x50, phy_base_regs + 0x0B8);
+	writel(0x51, phy_base_regs + 0x0E0);
+	writel(0x00, phy_base_regs + 0x100);
+	writel(0x80, phy_base_regs + 0x104);
+	writel(0x38, phy_base_regs + 0x140);
+	writel(0xA4, phy_base_regs + 0x180);
+	writel(0x03, phy_base_regs + 0x188); /* LCPLL 100MHz no divide */
+	writel(0x38, phy_base_regs + 0x2A8);
+	writel(0x12, phy_base_regs + 0x2E4);
+	/* PMA enable and aggregation mode(bifurcation mode: 0xC0) */
+	writel(0x80, phy_base_regs + 0x400);
+	writel(0x20, phy_base_regs + 0x408);
+	writel(0x00, phy_base_regs + 0x550);
+	writel(0x00, phy_base_regs + 0x5A8);
+	writel(0xFF, phy_base_regs + 0x5EC);
+
+	/* PCIe_TYPE: RC */
+	writel(0x02, phy_base_regs + 0x458); /* 100MHz CLK on */
+	writel(0x34, phy_base_regs + 0x5B0); /* diff,control REFCLK source */
+	writel(0x20, phy_base_regs + 0x450); /* when entering L1.2, add delay */
+
+	for (i = 0; i < num_lanes; i++) {
+		phy_base_regs += (i * 0x800);
+
+		writel(0x08, phy_base_regs + 0x82C);
+		writel(0x24, phy_base_regs + 0x830);
+		writel(0x80, phy_base_regs + 0x878);
+		writel(0x40, phy_base_regs + 0x894);
+		writel(0x00, phy_base_regs + 0x8C0);
+		writel(0x30, phy_base_regs + 0x8F4);
+		writel(0x05, phy_base_regs + 0x908);
+		writel(0xE0, phy_base_regs + 0x90C);
+		writel(0xD4, phy_base_regs + 0x914);
+		writel(0xD3, phy_base_regs + 0x91C);
+		writel(0xCE, phy_base_regs + 0x920);
+		writel(0x01, phy_base_regs + 0x924);
+		writel(0x35, phy_base_regs + 0x928);
+		writel(0xBA, phy_base_regs + 0x92C);
+		writel(0x41, phy_base_regs + 0x930);
+		writel(0x15, phy_base_regs + 0x934);
+		writel(0x13, phy_base_regs + 0x938);
+		writel(0x4E, phy_base_regs + 0x93C);
+		writel(0x43, phy_base_regs + 0x948);
+		writel(0xFC, phy_base_regs + 0x94C);
+		writel(0x10, phy_base_regs + 0x954);
+		writel(0x69, phy_base_regs + 0x958);
+		writel(0x40, phy_base_regs + 0x964);
+		writel(0xF6, phy_base_regs + 0x9B4);
+		writel(0x2D, phy_base_regs + 0x9C0);
+		writel(0xB7, phy_base_regs + 0x9C4);
+		writel(0x3C, phy_base_regs + 0x9CC);
+		writel(0x7E, phy_base_regs + 0x9DC);
+		writel(0x02, phy_base_regs + 0xA40);
+		writel(0x26, phy_base_regs + 0xA70);
+		writel(0x00, phy_base_regs + 0xA74);
+		writel(0x06, phy_base_regs + 0xB40);	/* hf_init */
+		writel(0x06, phy_base_regs + 0xB44);
+		writel(0x04, phy_base_regs + 0xB48);	/* value changed: 0x06 -> 0x04 */
+		writel(0x03, phy_base_regs + 0xB4C);	/* value changed: 0x04 -> 0x03 */
+		writel(0x03, phy_base_regs + 0xB50);	/* mf_init */
+		writel(0x03, phy_base_regs + 0xB54);
+		writel(0x03, phy_base_regs + 0xB58);
+		writel(0x03, phy_base_regs + 0xB5C);
+		writel(0x1B, phy_base_regs + 0xC10);
+		writel(0x10, phy_base_regs + 0xC44);
+		writel(0x10, phy_base_regs + 0xC48);
+		writel(0x10, phy_base_regs + 0xC4C);
+		writel(0x10, phy_base_regs + 0xC50);
+		writel(0x02, phy_base_regs + 0xC54);
+		writel(0x02, phy_base_regs + 0xC58);
+		writel(0x02, phy_base_regs + 0xC5C);
+		writel(0x02, phy_base_regs + 0xC60);
+		writel(0x02, phy_base_regs + 0xC6C);
+		writel(0x02, phy_base_regs + 0xC70);
+		writel(0xE7, phy_base_regs + 0xCA8);
+		writel(0x00, phy_base_regs + 0xCAC);
+		writel(0x0E, phy_base_regs + 0xCB0);
+		writel(0x1C, phy_base_regs + 0xCCC);
+		writel(0x05, phy_base_regs + 0xCD4);
+		writel(0x77, phy_base_regs + 0xCD8);
+		writel(0x7A, phy_base_regs + 0xCDC);
+		writel(0x2F, phy_base_regs + 0xDB4);
+
+		/* for GEN4 TX termination resistor? */
+		writel(0x00, phy_base_regs + 0x9CC);
+		writel(0xFF, phy_base_regs + 0xCD8);
+		writel(0x6E, phy_base_regs + 0xCDC);
+
+		/* TX idrv for termination resistor (only EVT0) */
+		writel(0x0F, phy_base_regs + 0x82C);
+		writel(0x60, phy_base_regs + 0x830);
+		writel(0x7E, phy_base_regs + 0x834);
+
+		/* TX pre_lvl */
+		writel(0x10, phy_base_regs + 0x824); /* pre */
+		writel(0x4F, phy_base_regs + 0x82C); /* [7:4]=g4 */
+		writel(0x1F, phy_base_regs + 0x818); /* post */
+		writel(0x00, phy_base_regs + 0x820); /* [3:0] */
+
+		/* RX tuning */
+		writel(0x3D, phy_base_regs + 0x928);
+		writel(0xCA, phy_base_regs + 0x920);
+		writel(0x2F, phy_base_regs + 0xA08);
+		writel(0xB8, phy_base_regs + 0x92C);
+		writel(0xF4, phy_base_regs + 0x914);
+		writel(0x55, phy_base_regs + 0x96C);
+		writel(0x78, phy_base_regs + 0x988);
+		writel(0x17, phy_base_regs + 0x934);
+		writel(0x4C, phy_base_regs + 0x93C);
+		writel(0x3B, phy_base_regs + 0x994);
+		writel(0x73, phy_base_regs + 0x948);
+		writel(0x3F, phy_base_regs + 0xB9C);
+		writel(0x20, phy_base_regs + 0x9C8);
+		writel(0xFF, phy_base_regs + 0x9C4);
+		writel(0x05, phy_base_regs + 0xC08);
+		writel(0x04, phy_base_regs + 0xC3C);
+		writel(0x04, phy_base_regs + 0xC40);
+	}
+
+	/* PCS setting: pcie_pcs_setting() in F/W code */
+
+	/* aggregation mdoe(bifurcation disabled) */
+	writel(0x00, phy_pcs_base_regs + 0x004);
+	if (num_lanes == 2)
+		writel(0x00, phy_pcs_base_regs + 0x804);
+
+	/* if RC */
+	writel(0x700D5, phy_pcs_base_regs + 0x154);
+	if (num_lanes == 2)
+		writel(0x700D5, phy_pcs_base_regs + 0x954);
+
+	/* add for L2 entry and power */
+	writel(0x300FF, phy_pcs_base_regs + 0x150);
+	if (num_lanes == 2)
+		writel(0x300FF, phy_pcs_base_regs + 0x950);
+
+	/* add L2 power down delay */
+	writel(0x40, phy_pcs_base_regs + 0x170);
+	if (num_lanes == 2)
+		writel(0x40, phy_pcs_base_regs + 0x970);
+
+	/* when entring L1.2, ERIO CLK gating */
+	val = readl(exynos_pcie->phy_pcs_base + 0x008);
+	val &= ~((1 << 4) | (1 << 5));
+	val |= (1 << 4);
+	writel(val, phy_pcs_base_regs + 0x008);
+	if (num_lanes == 2) {
+		val = readl(exynos_pcie->phy_pcs_base + 0x808);
+		val &= ~((1 << 4) | (1 << 5));
+		val |= (1 << 4);
+		writel(val, phy_pcs_base_regs + 0x808);
+	}
+
+	/* PHY CMN_RST, PORT_RST Release */
+	writel(0x1, elbi_base_regs + 0x1400);
+	writel(0x1, elbi_base_regs + 0x1408);
+
+	/* Additional PMA Configurations */
+	phy_base_regs = exynos_pcie->phy_base;
+	val = readl(phy_base_regs + 0x5D0);
+	val |= (0x1 << 4);
+	val &= ~(0x1 << 3);
+	writel(val, phy_base_regs + 0x5D0);
+	pr_debug("XO clock configuration : 0x%x\n", readl(phy_base_regs + 0x5D0));
+
+	/* AFC cal mode by default uses the calibrated value from a previous
+	 * run. However on some devices this causes a CDR failure because
+	 * the AFC done status is set prematurely. Setting the cal mode to
+	 * always start from an initial value (determined through simulation)
+	 * ensures that AFC has enough time to complete.
+	 */
+	dev_info(exynos_pcie->pci->dev, "AFC cal mode set to restart\n");
+	writel(0x4, phy_base_regs + 0xBF4);
+}
+
+static int exynos_pcie_rc_eom(struct device *dev, void *phy_base_regs)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	struct exynos_pcie_ops *pcie_ops = &exynos_pcie->exynos_pcie_ops;
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct device_node *np = dev->of_node;
+	unsigned int val;
+	unsigned int speed_rate, num_of_smpl;
+	unsigned int lane_width = 1;
+	int i, ret;
+	int test_cnt = 0;
+	struct pcie_eom_result **eom_result;
+
+	u32 phase_sweep = 0;
+	u32 phase_step = 1;
+	u32 phase_loop = 1;
+	u32 vref_sweep = 0;
+	u32 vref_step = 1;
+	u32 err_cnt = 0;
+	u32 cdr_value = 0;
+	u32 eom_done = 0;
+	u32 err_cnt_13_8;
+	u32 err_cnt_7_0;
+
+	dev_info(dev, "[%s] START!\n", __func__);
+
+	ret = of_property_read_u32(np, "num-lanes", &lane_width);
+	if (ret) {
+		dev_err(dev, "[%s] failed to get num of lane(lane width=0\n", __func__);
+		lane_width = 0;
+	} else {
+		dev_info(dev, "[%s] num-lanes : %d\n", __func__, lane_width);
+	}
+
+	/* eom_result[lane_num][test_cnt] */
+	eom_result = kcalloc(1, sizeof(struct pcie_eom_result *) * lane_width, GFP_KERNEL);
+	for (i = 0; i < lane_width; i++) {
+		eom_result[i] = kcalloc(1, sizeof(*eom_result[i]) *
+				EOM_PH_SEL_MAX * EOM_DEF_VREF_MAX, GFP_KERNEL);
+	}
+	if (!eom_result)
+		return -ENOMEM;
+
+	exynos_pcie->eom_result = eom_result;
+
+	pcie_ops->rd_own_conf(pp, PCIE_LINK_CTRL_STAT, 4, &val);
+	speed_rate = (val >> 16) & 0xf;
+
+	if (speed_rate == 1 || speed_rate == 2) {
+		dev_err(dev, "[%s] speed_rate(GEN%d) is not GEN3 or GEN4\n", __func__, speed_rate);
+		/* memory free 'eom_result' */
+		kfree(eom_result);
+
+		return -EINVAL;
+	}
+
+	num_of_smpl = 13;
+
+	for (i = 0; i < lane_width; i++) {
+		writel(0xE7, phy_base_regs + RX_EFOM_BIT_WIDTH_SEL);
+
+		val = readl(phy_base_regs + ANA_RX_DFE_EOM_PI_STR_CTRL);
+		val |= 0xF;
+		writel(val, phy_base_regs + ANA_RX_DFE_EOM_PI_STR_CTRL);
+
+		val = readl(phy_base_regs + ANA_RX_DFE_EOM_PI_DIVSEL_G12);
+		val |= (0x4 | 0x10);
+		writel(val, phy_base_regs + ANA_RX_DFE_EOM_PI_DIVSEL_G12);
+
+		val = readl(phy_base_regs + ANA_RX_DFE_EOM_PI_DIVSEL_G34);
+		val |= (0x4 | 0x20);	/* target sfr  changed: ANA_RC_...DIVSEL_G32 -> G34 */
+		writel(val, phy_base_regs + ANA_RX_DFE_EOM_PI_DIVSEL_G34);
+
+		val = readl(phy_base_regs + RX_CDR_LOCK) >> 2;
+		cdr_value = val & 0x1;
+		eom_done = readl(phy_base_regs + RX_EFOM_DONE) & 0x1;
+		dev_info(dev, "eom_done 0x%x , cdr_value : 0x%x\n", eom_done, cdr_value);
+
+		writel(0x0, phy_base_regs + RX_EFOM_NUMOF_SMPL_13_8);
+		writel(num_of_smpl, phy_base_regs + RX_EFOM_NUMOF_SMPL_7_0);
+
+		for (phase_sweep = 0; phase_sweep <= 0x47 * phase_loop;
+				phase_sweep = phase_sweep + phase_step) {
+			val = (phase_sweep % 72) << 1;
+			writel(val, phy_base_regs + RX_EFOM_EOM_PH_SEL);
+
+			for (vref_sweep = 0; vref_sweep <= 255;
+			     vref_sweep = vref_sweep + vref_step) {
+				/* malfunction code: writel(0x12, phy_base_regs + RX_EFOM_MODE); */
+				val = readl(phy_base_regs + RX_EFOM_MODE);
+				val &= ~(0x1f);
+				val |= (0x2 | 0x10);
+				writel(val, phy_base_regs + RX_EFOM_MODE);
+
+				writel(vref_sweep, phy_base_regs + RX_EFOM_DFE_VREF_CTRL);
+
+				/* malfunction code:  writel(0x13, phy_base_regs + RX_EFOM_MODE); */
+				val = readl(phy_base_regs + RX_EFOM_MODE);
+				val |= 0x1;	/* value changed: 0x13 -> 0x1 */
+				writel(val, phy_base_regs + RX_EFOM_MODE);
+
+				val = readl(phy_base_regs + RX_EFOM_DONE) & 0x1;
+				while (val != 0x1) {
+					udelay(1);
+					val = readl(phy_base_regs +
+							RX_EFOM_DONE) & 0x1;
+				}
+
+				err_cnt_13_8 = readl(phy_base_regs +
+						MON_RX_EFOM_ERR_CNT_13_8) << 8;
+				err_cnt_7_0 = readl(phy_base_regs +
+						MON_RX_EFOM_ERR_CNT_7_0);
+				err_cnt = err_cnt_13_8 + err_cnt_7_0;
+
+				if (vref_sweep == 128)
+					dev_info(dev, "%d,%d : %d %d %d\n", i, test_cnt,
+						 phase_sweep, vref_sweep, err_cnt);
+
+				/* save result */
+				eom_result[i][test_cnt].phase = phase_sweep;
+				eom_result[i][test_cnt].vref = vref_sweep;
+				eom_result[i][test_cnt].err_cnt = err_cnt;
+
+				test_cnt++;
+			}
+		}
+		writel(0x21, phy_base_regs + 0xBA0); /* 0xBA0 */
+		writel(0x00, phy_base_regs + 0xCA0); /* RX_EFOM_MODE = 0xCA0 */
+
+		/* goto next lane */
+		phy_base_regs += 0x800;
+		test_cnt = 0;
+	}
+
+	return 0;
+}
+
+void exynos_pcie_rc_phy_init(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	dev_info(pci->dev, "Initialize PHY functions.\n");
+
+	exynos_pcie->phy_ops.phy_check_rx_elecidle =
+		exynos_pcie_rc_phy_check_rx_elecidle;
+	exynos_pcie->phy_ops.phy_all_pwrdn = exynos_pcie_rc_phy_all_pwrdn;
+	exynos_pcie->phy_ops.phy_all_pwrdn_clear =
+					exynos_pcie_rc_phy_all_pwrdn_clear;
+	exynos_pcie->phy_ops.phy_config = exynos_pcie_rc_pcie_phy_config;
+	exynos_pcie->phy_ops.phy_eom = exynos_pcie_rc_eom;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_phy_init);
+
+static void exynos_pcie_quirks(struct pci_dev *dev)
+{
+	device_disable_async_suspend(&dev->dev);
+	pr_info("[%s] async suspend disabled\n", __func__);
+}
+DECLARE_PCI_FIXUP_FINAL(PCI_ANY_ID, PCI_ANY_ID, exynos_pcie_quirks);
+
+MODULE_AUTHOR("Hongseock Kim <hongpooh.kim@samsung.com>");
+MODULE_DESCRIPTION("PCIe phy driver for gs101 SoC");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/pci/controller/dwc/pcie-exynos-rc.c b/drivers/pci/controller/dwc/pcie-exynos-rc.c
new file mode 100644
index 000000000..b368164b6
--- /dev/null
+++ b/drivers/pci/controller/dwc/pcie-exynos-rc.c
@@ -0,0 +1,5054 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe RC(RootComplex) controller driver for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ *
+ * Author: Hongseock Kim <hongpooh.kim@samsung.com>
+ */
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/gpio.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/mfd/syscon.h>
+#include <linux/module.h>
+#include <linux/of_gpio.h>
+#include <linux/of_address.h>
+#include <linux/pci.h>
+#include <linux/platform_device.h>
+#include <linux/regmap.h>
+#include <linux/resource.h>
+#include <linux/signal.h>
+#include <linux/types.h>
+#include <linux/pm_qos.h>
+#include <dt-bindings/pci/pci.h>
+#include <linux/exynos-pci-noti.h>
+#include <linux/exynos-pci-ctrl.h>
+#include <linux/soc/samsung/exynos-smc.h>
+#if IS_ENABLED(CONFIG_EXYNOS_ITMON)
+#include <soc/google/exynos-itmon.h>
+#endif
+#include <linux/regulator/consumer.h>
+#include <linux/pinctrl/consumer.h>
+#include <linux/regulator/driver.h>
+#include <linux/pm_runtime.h>
+#include <linux/kthread.h>
+#include <linux/random.h>
+#include <linux/irqdomain.h>
+
+#if 0//IS_ENABLED(CONFIG_CPU_IDLE)
+#include <soc/google/exynos-powermode.h>
+#include <soc/google/exynos-pm.h>
+#include <soc/google/exynos-cpupm.h>
+#endif
+#if 0//IS_ENABLED(CONFIG_PM_DEVFREQ)
+#include <soc/google/exynos_pm_qos.h>
+#endif
+
+#include <soc/google/shm_ipc.h>     /* to get Exynos Modem - MSI target addr. */
+#if IS_ENABLED(CONFIG_LINK_DEVICE_PCIE)
+#define MODIFY_MSI_ADDR
+#endif	/* CONFIG_LINK_DEVICE_PCIE */
+
+#include "pcie-designware.h"
+#include "pcie-exynos-common.h"
+#include "pcie-exynos-rc.h"
+
+#include <linux/dma-map-ops.h>
+#include <soc/google/s2mpu.h>
+#include "../../../iommu/exynos-pcie-iommu-exp.h"
+
+struct exynos_pcie g_pcie_rc[MAX_RC_NUM];
+int pcie_is_linkup;	/* checkpatch: do not initialise globals to 0 */
+static struct separated_msi_vector sep_msi_vec[MAX_RC_NUM][PCIE_MAX_SEPA_IRQ_NUM];
+/* currnet_cnt & current_cnt2 for EOM test */
+static int current_cnt;
+static int current_cnt2;
+//static bool is_vhook_registered;
+
+static struct pci_dev *exynos_pcie_get_pci_dev(struct dw_pcie_rp *pp);
+
+#if 0//IS_ENABLED(CONFIG_PM_DEVFREQ)
+static struct exynos_pm_qos_request exynos_pcie_int_qos[MAX_RC_NUM];
+#endif
+
+/*
+ * PCIe channel0 is in the HSI1 block.
+ * PCIe channel1 is in the HSI2 block.
+ */
+#define pcie_ch_to_hsi(ch_num)	((ch_num) + 1)
+#if 1//IS_ENABLED(CONFIG_GS_S2MPU) || IS_ENABLED(CONFIG_EXYNOS_PCIE_IOMMU)
+static const struct dma_map_ops pcie_dma_ops;
+static struct device fake_dma_dev;
+#define to_pci_dev_from_dev(dev) container_of((dev), struct pci_dev, dev)
+
+#define MODEM_CH_NUM    0
+#define WIFI_CH_NUM     1
+#define NUM_PMA_REGS	2048
+
+static int rmw_priv_reg(phys_addr_t reg, u32 mask, u32 val)
+{
+       struct arm_smccc_res res;
+
+       arm_smccc_smc(0x82000504,
+                     reg,
+                     2,
+                     mask, val, 0, 0, 0, &res);
+
+       return (int)res.a0;
+}
+#if IS_ENABLED(CONFIG_GS_S2MPU)
+struct phys_mem {
+	struct list_head list;
+	phys_addr_t start;
+	size_t size;
+	unsigned char *refcnt_array;
+};
+
+#define ALIGN_SIZE	0x1000UL
+#define REF_COUNT_UNDERFLOW 255
+
+
+unsigned char *s2mpu_get_refcnt_ptr(struct exynos_pcie *exynos_pcie,
+				    phys_addr_t addr)
+{
+	struct phys_mem *pm;
+
+	/* Find the memory region the address falls into, then determine the
+	 * offset into the corresponding refcnt_array.
+	 */
+	list_for_each_entry(pm, &exynos_pcie->phys_mem_list, list) {
+		if (addr >= pm->start && addr < (pm->start + pm->size))
+			return pm->refcnt_array
+			       + (addr - pm->start) / ALIGN_SIZE;
+	}
+	return NULL;
+}
+
+void s2mpu_get_alignment(dma_addr_t addr, size_t size,
+			 phys_addr_t *align_addr, size_t *align_size)
+{
+	*align_addr = ALIGN_DOWN(addr, ALIGN_SIZE);
+	*align_size = ALIGN(addr - *align_addr + size, ALIGN_SIZE);
+}
+
+unsigned char s2mpu_get_and_modify(struct exynos_pcie *exynos_pcie,
+				   unsigned char *refcnt_ptr,
+				   bool inc)
+{
+	unsigned char val;
+
+	val = (*refcnt_ptr);
+	if (inc) {
+		val++;
+		*refcnt_ptr = val;
+	} else {
+		// Check for underflow. Should never happen.
+		if (val == 0) {
+			val = REF_COUNT_UNDERFLOW;
+		} else {
+			val--;
+			*refcnt_ptr = val;
+		}
+	}
+
+	return val;
+}
+
+void s2mpu_update_refcnt(struct device *dev,
+			 dma_addr_t dma_addr, size_t size, bool incr,
+			 enum dma_data_direction dir)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[WIFI_CH_NUM];
+	phys_addr_t align_addr;
+	size_t align_size;
+	unsigned char *refcnt_ptr;
+	int ret;
+	unsigned char refcnt;
+	unsigned long flags;
+
+	/* Align to 4K as required by S2MPU */
+	s2mpu_get_alignment(dma_addr, size, &align_addr, &align_size);
+
+	/* Get the pointer into the ref count array used to keep track of
+	 * the number of map and unmap calls for 4K blocks
+	 * needed by the S2MPU.
+	 * This is needed because there may be a few skbs within one 4K
+	 * aligned block and we need to ensure that we don't prematurely
+	 * disable access to this skb by calling s2mpu_close.
+	 */
+	refcnt_ptr = s2mpu_get_refcnt_ptr(exynos_pcie, align_addr);
+	if (!refcnt_ptr) {
+		dev_err(dev,
+			"s2mpu refcnt_ptr failed addr=%pad, size=%zx\n",
+			&dma_addr, size);
+		return;
+	}
+
+	/* Put lock on while-loop to protect race condition on
+	 * s2mpu_open/s2mpu_close and the case of align_size over 4K */
+	spin_lock_irqsave(&exynos_pcie->s2mpu_refcnt_lock, flags);
+	while (align_size != 0) {
+		if (incr) {
+			refcnt = s2mpu_get_and_modify(exynos_pcie, refcnt_ptr,
+						      true);
+			/* Note that this will open the memory with read/write
+			 * permissions based on the first invocation. Subsequent
+			 * read/write permissions will be ignored.
+			 */
+			if (refcnt == 1) {
+				ret = s2mpu_open(exynos_pcie->s2mpu,
+						 align_addr, ALIGN_SIZE, dir);
+				if (ret) {
+					dev_err(dev,
+						"s2mpu_open failed addr=%pad, size=%zx\n",
+						&dma_addr, size);
+				}
+			}
+		} else {
+			refcnt = s2mpu_get_and_modify(exynos_pcie, refcnt_ptr,
+						      false);
+			if (refcnt == REF_COUNT_UNDERFLOW) {
+				dev_err(dev, "s2mpu error underflow in refcount\n");
+				spin_unlock_irqrestore(&exynos_pcie->s2mpu_refcnt_lock, flags);
+				return;
+			}
+			if (refcnt == 0) {
+				ret = s2mpu_close(exynos_pcie->s2mpu,
+						  align_addr, ALIGN_SIZE, dir);
+				if (ret) {
+					dev_err(dev,
+						"s2mpu_close failed addr=%pad, size=%zx\n",
+						&dma_addr, size);
+				}
+			}
+		}
+		align_addr += ALIGN_SIZE;
+		align_size -= ALIGN_SIZE;
+		refcnt_ptr++;
+	}
+	spin_unlock_irqrestore(&exynos_pcie->s2mpu_refcnt_lock, flags);
+}
+#endif
+
+static int get_ch_num(struct pci_dev *epdev)
+{
+	int ch_num = WIFI_CH_NUM;
+	if (epdev->vendor == PCI_VENDOR_ID_SAMSUNG)
+		ch_num = MODEM_CH_NUM;
+
+	return ch_num;
+}
+
+static void *pcie_dma_alloc_attrs(struct device *dev, size_t size,
+				  dma_addr_t *dma_handle, gfp_t flag,
+				  unsigned long attrs)
+{
+	void *cpu_addr;
+	struct pci_dev *epdev = to_pci_dev_from_dev(dev);
+	int ch_num = get_ch_num(epdev);
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	int ret;
+
+	cpu_addr = dma_alloc_attrs(&fake_dma_dev, size,
+				   dma_handle, flag, attrs);
+	if (exynos_pcie->s2mpu) {
+		s2mpu_update_refcnt(dev, *dma_handle, size, true, DMA_BIDIRECTIONAL);
+	} else if (exynos_pcie->use_sysmmu) {
+		ret = pcie_iommu_map(*dma_handle, *dma_handle, size,
+				     DMA_BIDIRECTIONAL, pcie_ch_to_hsi(ch_num));
+		if (ret != 0) {
+			pr_err("Can't map PCIe SysMMU table!\n");
+			dma_free_attrs(&fake_dma_dev, size,
+				       cpu_addr, *dma_handle, attrs);
+			return NULL;
+		}
+	}
+
+	return cpu_addr;
+}
+
+static void pcie_dma_free_attrs(struct device *dev, size_t size,
+				void *cpu_addr, dma_addr_t dma_addr,
+				unsigned long attrs)
+{
+	struct pci_dev *epdev = to_pci_dev_from_dev(dev);
+	int ch_num = get_ch_num(epdev);
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	dma_free_attrs(&fake_dma_dev, size, cpu_addr, dma_addr, attrs);
+	if (exynos_pcie->s2mpu)
+		s2mpu_update_refcnt(dev, dma_addr, size, false, DMA_BIDIRECTIONAL);
+	else if (exynos_pcie->use_sysmmu)
+		pcie_iommu_unmap(dma_addr, size, pcie_ch_to_hsi(ch_num));
+}
+
+static dma_addr_t pcie_dma_map_page(struct device *dev, struct page *page,
+				    size_t offset, size_t size,
+				    enum dma_data_direction dir,
+				    unsigned long attrs)
+{
+	struct pci_dev *epdev = to_pci_dev_from_dev(dev);
+	int ch_num = get_ch_num(epdev);
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	dma_addr_t dma_addr;
+	int ret;
+
+	dma_addr = dma_map_page_attrs(&fake_dma_dev, page, offset,
+				      size, dir, attrs);
+	if (exynos_pcie->s2mpu) {
+		s2mpu_update_refcnt(dev, dma_addr, size, true, dir);
+	} else if (exynos_pcie->use_sysmmu) {
+		ret = pcie_iommu_map(dma_addr, dma_addr, size,
+				     dir, pcie_ch_to_hsi(ch_num));
+		if (ret != 0) {
+			pr_err("DMA map - Can't map PCIe SysMMU table!!!\n");
+			return 0;
+		}
+	}
+	return dma_addr;
+}
+
+static void pcie_dma_unmap_page(struct device *dev, dma_addr_t dma_addr,
+				size_t size, enum dma_data_direction dir,
+				unsigned long attrs)
+{
+	struct pci_dev *epdev = to_pci_dev_from_dev(dev);
+	int ch_num = get_ch_num(epdev);
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	dma_unmap_page_attrs(&fake_dma_dev, dma_addr, size, dir, attrs);
+
+	if (exynos_pcie->s2mpu)
+		s2mpu_update_refcnt(dev, dma_addr, size, false, dir);
+	else if (exynos_pcie->use_sysmmu)
+		pcie_iommu_unmap(dma_addr, size, pcie_ch_to_hsi(ch_num));
+}
+
+static const struct dma_map_ops pcie_dma_ops = {
+	.alloc = pcie_dma_alloc_attrs,
+	.free = pcie_dma_free_attrs,
+	.mmap = NULL,
+	.get_sgtable = NULL,
+	.map_page = pcie_dma_map_page,
+	.unmap_page = pcie_dma_unmap_page,
+	.map_sg = NULL,
+	.unmap_sg = NULL,
+	.map_resource = NULL,
+	.unmap_resource = NULL,
+	.sync_single_for_cpu = NULL,
+	.sync_single_for_device = NULL,
+	.sync_sg_for_cpu = NULL,
+	.sync_sg_for_device = NULL,
+	.cache_sync = NULL,
+	.dma_supported = NULL,
+	.get_required_mask = NULL,
+	.max_mapping_size = NULL,
+	.get_merge_boundary = NULL,
+};
+#endif
+
+static void save_pma_regs(struct exynos_pcie *exynos_pcie)
+{
+	struct device *dev = exynos_pcie->pci->dev;
+	u32 *save;
+	int i;
+
+	if (!exynos_pcie)
+		return;
+
+	if (exynos_pcie->pma_regs_valid)
+		return;
+
+	save = exynos_pcie->pma_regs;
+	for (i = 0; i < NUM_PMA_REGS; i++) {
+		*save = exynos_phy_read(exynos_pcie, i * sizeof(u32));
+		dev_dbg(dev, "i=%d val=0x%08x\n", i, *save);
+		save++;
+	}
+	exynos_pcie->pma_regs_valid = true;
+}
+
+static void restore_pma_regs(struct exynos_pcie *exynos_pcie)
+{
+	struct device *dev = exynos_pcie->pci->dev;
+	u32 *save;
+	u32 val;
+	int i;
+
+	if (!exynos_pcie)
+		return;
+
+	if (!exynos_pcie->pma_regs_valid) {
+		dev_err(dev, "pma regs are not valid, restore skipped\n");
+		return;
+	}
+
+	save = exynos_pcie->pma_regs;
+	for (i = 0; i < NUM_PMA_REGS; i++) {
+		val = exynos_phy_read(exynos_pcie, i * sizeof(u32));
+		if (val != *save)
+			dev_err(dev, "diff i=%d curr_val=0x%08x save_val=0x%08x\n",
+				i, val, *save);
+		exynos_phy_write(exynos_pcie, *save, i * sizeof(u32));
+		save++;
+	}
+}
+
+static void exynos_pcie_phy_isolation(struct exynos_pcie *exynos_pcie, int val)
+{
+	struct device *dev = exynos_pcie->pci->dev;
+	int ret;
+
+	dev_info(dev, "PCIe PHY ISOLATION = %d\n", val);
+	exynos_pcie->phy_control = val;
+	ret = rmw_priv_reg(exynos_pcie->pmu_alive_pa +
+			   exynos_pcie->pmu_offset, PCIE_PHY_CONTROL_MASK, val);
+	if (ret)
+		regmap_update_bits(exynos_pcie->pmureg,
+				   exynos_pcie->pmu_offset,
+				   PCIE_PHY_CONTROL_MASK, val);
+}
+
+void exynos_pcie_set_perst_gpio(int ch_num, bool on)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	if (exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+		pr_debug("%s: force setting for abnormal state\n", __func__);
+		if (on) {
+			gpio_set_value(exynos_pcie->perst_gpio, 1);
+			pr_debug("%s: Set PERST to HIGH, gpio val = %d\n",
+				 __func__, gpio_get_value(exynos_pcie->perst_gpio));
+		} else {
+			gpio_set_value(exynos_pcie->perst_gpio, 0);
+			pr_debug("%s: Set PERST to LOW, gpio val = %d\n",
+				 __func__, gpio_get_value(exynos_pcie->perst_gpio));
+		}
+	}
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_set_perst_gpio);
+
+void exynos_pcie_set_ready_cto_recovery(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+
+	pr_info("[%s] ch_num:%d\n", __func__, ch_num);
+
+	disable_irq(pp->irq);
+
+	exynos_pcie_set_perst_gpio(ch_num, 0);
+
+	/* LTSSM disable */
+	exynos_elbi_write(exynos_pcie, PCIE_ELBI_LTSSM_DISABLE,
+			PCIE_APP_LTSSM_ENABLE);
+}
+EXPORT_SYMBOL(exynos_pcie_set_ready_cto_recovery);
+
+static ssize_t exynos_pcie_rc_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int ret = 0;
+
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, ">>>> PCIe Test <<<<\n");
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "0 : PCIe Unit Test\n");
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "1 : Link Test\n");
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "2 : DisLink Test\n");
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "14 : PCIe Hot Reset\n");
+
+	return ret;
+}
+
+static ssize_t exynos_pcie_rc_store(struct device *dev, struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int op_num;
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	int ret = 0;
+
+	if (sscanf(buf, "%10d", &op_num) == 0)
+		return -EINVAL;
+
+	if (exynos_pcie->use_phy_isol_con)
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_BYPASS);
+
+	switch (op_num) {
+	case 0:
+	case 1:
+	case 2:
+		dev_err(dev, "## Test not supported ##\n");
+		break;
+	case 3:
+		dev_info(dev, "## LTSSM ##\n");
+		ret = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		dev_info(dev, "LTSSM :0x%x\n", ret);
+		break;
+
+	case 13:
+		dev_info(dev, "%s: force perst setting\n", __func__);
+		exynos_pcie_set_perst_gpio(1, 0);
+
+		break;
+
+	case 14:
+		dev_info(dev, "%s:[hot reset] by pulsing app_init_rst(ch %d)\n",
+				__func__, exynos_pcie->ch_num);
+		dev_info(dev, "PM_POWER_STATE  = 0x%08x\n",
+			 exynos_phy_pcs_read(exynos_pcie, 0x188));
+		exynos_elbi_write(exynos_pcie, 0x1, APP_INIT_RST);
+		return count;
+
+	case 16:
+		exynos_pcie_rc_set_outbound_atu(1, 0x47200000, 0x0, SZ_1M);
+		break;
+
+	case 17:
+		dev_info(dev, "## Dump RC Registers ##\n");
+		exynos_pcie_rc_register_dump(exynos_pcie->ch_num);
+		break;
+
+	default:
+		dev_err(dev, "Unsupported Test Number(%d)...\n", op_num);
+	}
+	if (exynos_pcie->use_phy_isol_con)
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_ISOLATION);
+
+	return count;
+}
+
+static DEVICE_ATTR(pcie_rc_test, S_IWUSR | S_IWGRP | S_IRUSR | S_IRGRP,
+		   exynos_pcie_rc_show, exynos_pcie_rc_store);
+
+static ssize_t exynos_pcie_eom1_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	struct pcie_eom_result **eom_result = exynos_pcie->eom_result;
+	struct device_node *np = dev->of_node;
+	int len = 0;
+	u32 test_cnt = 0;
+	unsigned int lane_width = 1;
+	int i = 0, ret;
+
+	if (!eom_result) {
+		len += snprintf(buf + len, PAGE_SIZE, "eom_result structure is NULL !!!\n");
+
+		goto exit;
+	}
+
+	ret = of_property_read_u32(np, "num-lanes", &lane_width);
+	if (ret)
+		lane_width = 0;
+
+	while (current_cnt != EOM_PH_SEL_MAX * EOM_DEF_VREF_MAX) {
+		len += snprintf(buf + len, PAGE_SIZE,
+				"%u %u %lu\n",
+				eom_result[i][current_cnt].phase,
+				eom_result[i][current_cnt].vref,
+				eom_result[i][current_cnt].err_cnt);
+		current_cnt++;
+		test_cnt++;
+		if (test_cnt == 100)
+			break;
+	}
+
+	if (current_cnt == EOM_PH_SEL_MAX * EOM_DEF_VREF_MAX)
+		current_cnt = 0;
+
+exit:
+	return len;
+}
+
+static ssize_t exynos_pcie_eom1_store(struct device *dev, struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int op_num;
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	if (sscanf(buf, "%10d", &op_num) == 0)
+		return -EINVAL;
+	switch (op_num) {
+	case 0:
+		if (exynos_pcie->phy_ops.phy_eom)
+			exynos_pcie->phy_ops.phy_eom(dev, exynos_pcie->phy_base);
+
+		/* reset the counter before start eom_show() func. */
+		current_cnt = 0;
+		current_cnt2 = 0;
+
+		break;
+	}
+
+	return count;
+}
+
+static DEVICE_ATTR(eom1, S_IWUSR | S_IWGRP | S_IRUSR | S_IRGRP,
+		   exynos_pcie_eom1_show, exynos_pcie_eom1_store);
+
+static ssize_t exynos_pcie_eom2_store(struct device *dev, struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	/* prevent to print kerenl warning message
+	 * eom1_store function do all operation to get eom data
+	 */
+
+	return count;
+}
+
+static ssize_t exynos_pcie_eom2_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	struct pcie_eom_result **eom_result = exynos_pcie->eom_result;
+	struct device_node *np = dev->of_node;
+	int len = 0;
+	u32 test_cnt = 0;
+	unsigned int lane_width = 1;
+	int i = 1, ret;
+
+	if (!eom_result) {
+		len += snprintf(buf + len, PAGE_SIZE, "eom_result structure is NULL!!\n");
+
+		goto exit;
+	}
+
+	ret = of_property_read_u32(np, "num-lanes", &lane_width);
+	if (ret) {
+		lane_width = 0;
+		len += snprintf(buf + len, PAGE_SIZE,
+				"can't get num of lanes!!\n");
+		goto exit;
+	}
+
+	if (lane_width == 1) {
+		len += snprintf(buf + len, PAGE_SIZE,
+				"EOM2NULL\n");
+		goto exit;
+	}
+
+	while (current_cnt2 != EOM_PH_SEL_MAX * EOM_DEF_VREF_MAX) {
+		len += snprintf(buf + len, PAGE_SIZE,
+				"%u %u %lu\n",
+				eom_result[i][current_cnt2].phase,
+				eom_result[i][current_cnt2].vref,
+				eom_result[i][current_cnt2].err_cnt);
+		current_cnt2++;
+		test_cnt++;
+		if (test_cnt == 100)
+			break;
+	}
+
+	if (current_cnt2 == EOM_PH_SEL_MAX * EOM_DEF_VREF_MAX)
+		current_cnt2 = 0;
+
+exit:
+	return len;
+}
+
+static DEVICE_ATTR(eom2, S_IWUSR | S_IWGRP | S_IRUSR | S_IRGRP,
+		   exynos_pcie_eom2_show, exynos_pcie_eom2_store);
+
+static ssize_t l12_state_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	int  ret;
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	ret = scnprintf(buf, PAGE_SIZE, "l1ss_ctrl_id_state = 0x%08x\n",
+			exynos_pcie->l1ss_ctrl_id_state);
+	ret += scnprintf(buf, PAGE_SIZE, "LTSSM: 0x%08x, PM_STATE = 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP),
+			 exynos_phy_pcs_read(exynos_pcie, PM_POWER_STATE));
+	return ret;
+}
+
+static ssize_t l12_state_store(struct device *dev,
+			       struct device_attribute *attr,
+			       const char *buf, size_t count)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	int ch_num = exynos_pcie->ch_num;
+	int enable;
+
+	if (sscanf(buf, "%10d", &enable) == 0)
+		return -EINVAL;
+
+	if (enable == 1) {
+		dev_info(dev, "L1.2 Enable....on PCIe_ch%d\n", ch_num);
+		exynos_pcie_rc_l1ss_ctrl(1, PCIE_L1SS_CTRL_TEST, ch_num);
+	} else if (enable == 0) {
+		dev_info(dev, "L1.2 Disable....on PCIe_ch%d\n", ch_num);
+		exynos_pcie_rc_l1ss_ctrl(0, PCIE_L1SS_CTRL_TEST, ch_num);
+	} else {
+		dev_err(dev, "Value needs to be 0 or 1\n");
+		return -EINVAL;
+	}
+
+	return count;
+}
+
+static ssize_t link_speed_show(struct device *dev,
+			       struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "GEN%d\n", exynos_pcie->max_link_speed);
+}
+
+static ssize_t link_speed_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	int link_speed;
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	if (sscanf(buf, "%10d", &link_speed) == 0)
+		return -EINVAL;
+
+	if (link_speed < 1 || link_speed > 3) {
+		dev_info(dev, "Value needs to be between 1-3\n");
+		return -EINVAL;
+	}
+
+	dev_info(dev, "Change Link Speed: Target Link Speed = GEN%d\n", link_speed);
+	exynos_pcie->max_link_speed = link_speed;
+
+	return count;
+}
+
+static ssize_t link_state_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int ret = 0;
+	u32 val;
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	if (exynos_pcie->phy_control == PCIE_PHY_ISOLATION ||
+	    exynos_pcie->state != STATE_LINK_UP) {
+		val = L2;   /* Not linked is equivalent to L2 */
+		goto exit;
+	}
+
+	val = exynos_phy_pcs_read(exynos_pcie, PM_POWER_STATE);
+	val &= PM_STATE_MASK;
+
+	switch (val) {
+	case 0:
+		val = L0;
+		break;
+	case 1:
+		val = L0S;
+		break;
+	case 2:
+		val = L1;
+		break;
+	case 3:
+		val = L2V;
+		break;
+	case 4:
+		val = L2;
+		break;
+	case 5:
+		val = L11;
+		break;
+	case 6:
+		val = L12;
+		break;
+	default:
+		val = UNKNOWN;
+	}
+
+exit:
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%d\n", val);
+
+	return ret;
+}
+
+static u64 power_stats_get_ts(void)
+{
+	return ktime_to_ms(ktime_get_boottime());
+}
+
+static void power_stats_init(struct exynos_pcie *pcie)
+{
+	pcie->link_up.count = 0;
+	pcie->link_up.duration = 0;
+	pcie->link_up.last_entry_ts = 0;
+	pcie->link_down.count = 1;  // since system starts with link_down
+	pcie->link_down.duration = 0;
+	pcie->link_down.last_entry_ts = 0;
+}
+
+static void power_stats_update_up(struct exynos_pcie *pcie)
+{
+	u64 current_ts;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcie->power_stats_lock, flags);
+
+	current_ts = power_stats_get_ts();
+
+	pcie->link_up.count++;
+	pcie->link_up.last_entry_ts = current_ts;
+
+	pcie->link_down.duration += current_ts -
+				    pcie->link_down.last_entry_ts;
+
+	spin_unlock_irqrestore(&pcie->power_stats_lock, flags);
+}
+
+static void power_stats_update_down(struct exynos_pcie *pcie)
+{
+	u64 current_ts;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcie->power_stats_lock, flags);
+	current_ts = power_stats_get_ts();
+
+	pcie->link_down.count++;
+	pcie->link_down.last_entry_ts = current_ts;
+
+	pcie->link_up.duration += current_ts -
+				  pcie->link_up.last_entry_ts;
+	spin_unlock_irqrestore(&pcie->power_stats_lock, flags);
+}
+
+static ssize_t power_stats_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	int ret = 0;
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+	u64 current_ts;
+	u64 link_up_delta = 0;
+	u64 link_down_delta = 0;
+	struct power_stats link_up_copy;
+	struct power_stats link_down_copy;
+	enum exynos_pcie_state state_copy;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcie->power_stats_lock, flags);
+	memcpy(&link_up_copy, &pcie->link_up, sizeof(struct power_stats));
+	memcpy(&link_down_copy, &pcie->link_down, sizeof(struct power_stats));
+	state_copy = pcie->state;
+	current_ts = power_stats_get_ts();
+	spin_unlock_irqrestore(&pcie->power_stats_lock, flags);
+
+	if (state_copy == STATE_LINK_UP || state_copy == STATE_LINK_DOWN_TRY)
+		link_up_delta = current_ts - link_up_copy.last_entry_ts;
+
+	if (state_copy == STATE_LINK_DOWN || state_copy == STATE_LINK_UP_TRY)
+		link_down_delta = current_ts - link_down_copy.last_entry_ts;
+
+	ret = scnprintf(buf + ret, PAGE_SIZE - ret, "Version: 1\n");
+
+	ret += PRINT_STATUS(buf, ret, "up", link_up_copy.count,
+			    link_up_copy.duration + link_up_delta,
+			    link_up_copy.last_entry_ts);
+
+	ret += PRINT_STATUS(buf, ret, "down", link_down_copy.count,
+			    link_down_copy.duration + link_down_delta,
+			    link_down_copy.last_entry_ts);
+
+	return ret;
+}
+
+static DEVICE_ATTR_RW(l12_state);
+static DEVICE_ATTR_RW(link_speed);
+static DEVICE_ATTR_RO(link_state);
+static DEVICE_ATTR_RO(power_stats);
+
+/* percentage (0-100) to weight new datapoints in moving average */
+#define NEW_DATA_AVERAGE_WEIGHT  10
+static void link_stats_init(struct exynos_pcie *pcie)
+{
+	pcie->link_stats.link_down_irq_count = 0;
+	pcie->link_stats.link_down_irq_count_reported = 0;
+	pcie->link_stats.cmpl_timeout_irq_count = 0;
+	pcie->link_stats.cmpl_timeout_irq_count_reported = 0;
+	pcie->link_stats.link_up_failure_count = 0;
+	pcie->link_stats.link_up_failure_count_reported = 0;
+	pcie->link_stats.link_recovery_failure_count = 0;
+	pcie->link_stats.link_recovery_failure_count_reported = 0;
+	pcie->link_stats.pll_lock_time_avg = 0;
+	pcie->link_stats.link_up_time_avg = 0;
+}
+
+static void link_stats_log_pll_lock(struct exynos_pcie *pcie, u32 pll_lock_time)
+{
+	pcie->link_stats.pll_lock_time_avg =
+	    DIV_ROUND_CLOSEST(pcie->link_stats.pll_lock_time_avg *
+			      (100 - NEW_DATA_AVERAGE_WEIGHT) +
+			      pll_lock_time * NEW_DATA_AVERAGE_WEIGHT, 100);
+}
+
+static void link_stats_log_link_up(struct exynos_pcie *pcie, u32 link_up_time)
+{
+	pcie->link_stats.link_up_time_avg =
+	    DIV_ROUND_CLOSEST(pcie->link_stats.link_up_time_avg *
+			      (100 - NEW_DATA_AVERAGE_WEIGHT) +
+			      link_up_time * NEW_DATA_AVERAGE_WEIGHT, 100);
+}
+
+static int check_exynos_pcie_reg_status(struct exynos_pcie *exynos_pcie,
+					void __iomem *base, int offset,
+					u32 mask, u32 mask_value, int *cnt)
+{
+	int i;
+	bool status = false;
+	u32 value;
+	ktime_t timestamp = ktime_get();
+
+	for (i = 0; i < 10000; i++) {
+		udelay(1);
+		value = readl(base + offset);
+
+		status = (value & mask) == mask_value;
+		if (status)
+			break;
+
+	}
+	*cnt = i;
+
+	timestamp = ktime_sub(ktime_get(), timestamp);
+	dev_info(exynos_pcie->pci->dev, "elapsed time: %lld uS\n", ktime_to_us(timestamp));
+
+	if (status) {
+		dev_info(exynos_pcie->pci->dev, "status %#x passed cnt=%d\n", offset, i);
+	}
+
+	return status;
+}
+
+static ssize_t link_down_irqs_show(struct device *dev, struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+	u32 value = pcie->link_stats.link_down_irq_count;
+
+	/* report delta since last write */
+	value -= pcie->link_stats.link_down_irq_count_reported;
+
+	return sysfs_emit(buf, "%d\n", value);
+}
+
+static ssize_t link_down_irqs_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int ret;
+	u32 value, delta;
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	/* Writing a value marks those as reported */
+	ret = kstrtouint(buf, 10, &value);
+	if (ret < 0)
+		return ret;
+
+	delta = pcie->link_stats.link_down_irq_count -
+		pcie->link_stats.link_down_irq_count_reported;
+
+	if (value > delta) {
+		dev_info(dev, "Value needs to be <= %d\n", delta);
+		return -EINVAL;
+	}
+
+	pcie->link_stats.link_down_irq_count_reported += value;
+
+	return count;
+}
+
+static ssize_t complete_timeout_irqs_show(struct device *dev,
+					  struct device_attribute *attr,
+					  char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+	u32 value = pcie->link_stats.cmpl_timeout_irq_count;
+
+	/* report delta since last sysfs write */
+	value -= pcie->link_stats.cmpl_timeout_irq_count_reported;
+
+	return sysfs_emit(buf, "%d\n", value);
+}
+
+static ssize_t complete_timeout_irqs_store(struct device *dev,
+					   struct device_attribute *attr,
+					   const char *buf, size_t count)
+{
+	int ret;
+	u32 value, delta;
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	/* Writing a value marks those as reported */
+	ret = kstrtouint(buf, 10, &value);
+	if (ret < 0)
+		return ret;
+
+	delta = pcie->link_stats.cmpl_timeout_irq_count -
+		pcie->link_stats.cmpl_timeout_irq_count_reported;
+
+	if (value > delta) {
+		dev_info(dev, "Value needs to be <= %d\n", delta);
+		return -EINVAL;
+	}
+
+	pcie->link_stats.cmpl_timeout_irq_count_reported += value;
+
+	return count;
+}
+
+
+static ssize_t link_up_failures_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+	u32 value = pcie->link_stats.link_up_failure_count;
+
+	/* report delta since last write */
+	value -= pcie->link_stats.link_up_failure_count_reported;
+
+	return sysfs_emit(buf, "%d\n", value);
+}
+
+static ssize_t link_up_failures_store(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	int ret;
+	u32 value, delta;
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	/* Writing a value marks those as reported */
+	ret = kstrtouint(buf, 10, &value);
+	if (ret < 0)
+		return ret;
+
+	delta = pcie->link_stats.link_up_failure_count -
+		pcie->link_stats.link_up_failure_count_reported;
+
+	if (value > delta) {
+		dev_info(dev, "Value needs to be <= %d\n", delta);
+		return -EINVAL;
+	}
+
+	pcie->link_stats.link_up_failure_count_reported += value;
+
+	return count;
+}
+
+static ssize_t link_recovery_failures_show(struct device *dev,
+					   struct device_attribute *attr,
+					   char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+	u32 value = pcie->link_stats.link_recovery_failure_count;
+
+	/* report delta since last write */
+	value -= pcie->link_stats.link_recovery_failure_count_reported;
+
+	return sysfs_emit(buf, "%d\n", value);
+}
+
+static ssize_t link_recovery_failures_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	int ret;
+	u32 value, delta;
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	/* Writing a value marks those as reported */
+	ret = kstrtouint(buf, 10, &value);
+	if (ret < 0)
+		return ret;
+
+	delta = pcie->link_stats.link_recovery_failure_count -
+		pcie->link_stats.link_recovery_failure_count_reported;
+
+	if (value > delta) {
+		dev_info(dev, "Value needs to be <= %d\n", delta);
+		return -EINVAL;
+	}
+
+	pcie->link_stats.link_recovery_failure_count_reported += value;
+
+	return count;
+}
+
+static ssize_t pll_lock_average_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	return sysfs_emit(buf, "%d\n", pcie->link_stats.pll_lock_time_avg);
+}
+
+static ssize_t link_up_average_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	struct exynos_pcie *pcie = dev_get_drvdata(dev);
+
+	return sysfs_emit(buf, "%d\n", pcie->link_stats.link_up_time_avg);
+}
+
+static DEVICE_ATTR_RW(link_down_irqs);
+static DEVICE_ATTR_RW(complete_timeout_irqs);
+static DEVICE_ATTR_RW(link_up_failures);
+static DEVICE_ATTR_RW(link_recovery_failures);
+static DEVICE_ATTR_RO(pll_lock_average);
+static DEVICE_ATTR_RO(link_up_average);
+
+static struct attribute *link_stats_attrs[] = {
+	&dev_attr_link_down_irqs.attr,
+	&dev_attr_complete_timeout_irqs.attr,
+	&dev_attr_link_up_failures.attr,
+	&dev_attr_link_recovery_failures.attr,
+	&dev_attr_pll_lock_average.attr,
+	&dev_attr_link_up_average.attr,
+	NULL,
+};
+
+static const struct attribute_group link_stats_group = {
+	.attrs = link_stats_attrs,
+	.name = "link_stats",
+};
+
+static inline int create_pcie_sys_file(struct device *dev)
+{
+	struct device_node *np = dev->of_node;
+	struct pci_dev *pdev = to_pci_dev_from_dev(dev);
+	int ret;
+	int num_lane;
+
+	ret = device_create_file(dev, &dev_attr_pcie_rc_test);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for test(%d)\n", ret);
+
+		return ret;
+	}
+
+	ret = of_property_read_u32(np, "num-lanes", &num_lane);
+	if (ret)
+		num_lane = 0;
+
+	ret = device_create_file(dev, &dev_attr_eom1);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for eom(%d)\n", ret);
+		return ret;
+	}
+
+	if (num_lane > 0) {
+		ret = device_create_file(dev, &dev_attr_eom2);
+		if (ret) {
+			dev_err(dev, "couldn't create device file for eom(%d)\n", ret);
+			return ret;
+		}
+	}
+
+	ret = device_create_file(dev, &dev_attr_l12_state);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for l12_state(%d)\n", ret);
+		return ret;
+	}
+
+	ret = device_create_file(dev, &dev_attr_link_speed);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for link_speed(%d)\n", ret);
+		return ret;
+	}
+
+	ret = device_create_file(dev, &dev_attr_link_state);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for linkst(%d)\n", ret);
+		return ret;
+	}
+
+	ret = device_create_file(dev, &dev_attr_power_stats);
+	if (ret) {
+		dev_err(dev, "couldn't create device file for power_stats(%d)\n", ret);
+		return ret;
+	}
+
+	ret = sysfs_create_group(&pdev->dev.kobj, &link_stats_group);
+	if (ret) {
+		dev_err(dev, "couldn't create sysfs group for link_stats(%d)\n", ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+static inline void remove_pcie_sys_file(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev_from_dev(dev);
+
+	device_remove_file(dev, &dev_attr_pcie_rc_test);
+	device_remove_file(dev, &dev_attr_l12_state);
+	device_remove_file(dev, &dev_attr_link_speed);
+	device_remove_file(dev, &dev_attr_link_state);
+	device_remove_file(dev, &dev_attr_power_stats);
+	sysfs_remove_group(&pdev->dev.kobj, &link_stats_group);
+}
+
+static int exynos_pcie_rc_clock_enable(struct dw_pcie_rp *pp, int enable)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct exynos_pcie_clks	*clks = &exynos_pcie->clks;
+	int i;
+	int ret = 0;
+
+	if (enable) {
+		for (i = 0; i < exynos_pcie->pcie_clk_num; i++)
+			ret = clk_prepare_enable(clks->pcie_clks[i]);
+	} else {
+		for (i = 0; i < exynos_pcie->pcie_clk_num; i++)
+			clk_disable_unprepare(clks->pcie_clks[i]);
+	}
+
+	return ret;
+}
+
+static int exynos_pcie_rc_phy_clock_enable(struct dw_pcie_rp *pp, int enable)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct exynos_pcie_clks	*clks = &exynos_pcie->clks;
+	int i;
+	int ret = 0;
+
+	if (enable) {
+		for (i = 0; i < exynos_pcie->phy_clk_num; i++)
+			ret = clk_prepare_enable(clks->phy_clks[i]);
+	} else {
+		for (i = 0; i < exynos_pcie->phy_clk_num; i++)
+			clk_disable_unprepare(clks->phy_clks[i]);
+	}
+
+	return ret;
+}
+
+void exynos_pcie_rc_print_link_history(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct device *dev = pci->dev;
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	u32 history;
+	int i;
+
+	for (i = 31; i >= 0; i--) {
+		history = exynos_elbi_read(exynos_pcie,
+					   PCIE_HISTORY_REG(i));
+
+		dev_info(dev, "LTSSM: 0x%02x, L1sub: 0x%x, D state: 0x%x\n",
+				LTSSM_STATE(history),
+				L1SUB_STATE(history),
+				PM_DSTATE(history));
+	}
+}
+
+static int exynos_pcie_rc_rd_own_conf(struct dw_pcie_rp *pp, int where, int size, u32 *val)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int is_linked = 0;
+	int ret = 0;
+	u32 __maybe_unused reg_val;
+	unsigned long flags;
+
+	if (exynos_pcie->phy_control == PCIE_PHY_ISOLATION) {
+		*val = 0xffffffff;
+		return PCIBIOS_DEVICE_NOT_FOUND;
+	}
+
+	spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+
+	if (exynos_pcie->state == STATE_LINK_UP)
+		is_linked = 1;
+
+	if (is_linked == 0) {
+		exynos_pcie_rc_clock_enable(pp, PCIE_ENABLE_CLOCK);
+		exynos_pcie_rc_phy_clock_enable(pp, PCIE_ENABLE_CLOCK);
+
+		if (exynos_pcie->phy_ops.phy_check_rx_elecidle)
+			exynos_pcie->phy_ops.phy_check_rx_elecidle(exynos_pcie->phy_pcs_base,
+								   IGNORE_ELECIDLE,
+								   exynos_pcie->ch_num);
+	}
+
+	ret = dw_pcie_read(exynos_pcie->rc_dbi_base + (where), size, val);
+
+	if (is_linked == 0) {
+		if (exynos_pcie->phy_ops.phy_check_rx_elecidle)
+			exynos_pcie->phy_ops.phy_check_rx_elecidle(exynos_pcie->phy_pcs_base,
+								   ENABLE_ELECIDLE,
+								   exynos_pcie->ch_num);
+
+		exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+		exynos_pcie_rc_clock_enable(pp, PCIE_DISABLE_CLOCK);
+	}
+	spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+
+	return ret;
+}
+
+static int exynos_pcie_rc_wr_own_conf(struct dw_pcie_rp *pp, int where, int size, u32 val)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int is_linked = 0;
+	int ret = 0;
+	u32 __maybe_unused reg_val;
+	unsigned long flags;
+
+	if (exynos_pcie->phy_control == PCIE_PHY_ISOLATION)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+
+	if (exynos_pcie->state == STATE_LINK_UP)
+		is_linked = 1;
+
+	if (is_linked == 0) {
+		exynos_pcie_rc_clock_enable(pp, PCIE_ENABLE_CLOCK);
+		exynos_pcie_rc_phy_clock_enable(pp, PCIE_ENABLE_CLOCK);
+
+		if (exynos_pcie->phy_ops.phy_check_rx_elecidle)
+			exynos_pcie->phy_ops.phy_check_rx_elecidle(exynos_pcie->phy_pcs_base,
+								   IGNORE_ELECIDLE,
+								   exynos_pcie->ch_num);
+	}
+
+	/* If secure ATU then make SMC call, since only TFA has write access */
+	if (exynos_pcie->use_secure_atu && where == SECURE_ATU_ENABLE)
+		ret = exynos_smc(SMC_SECURE_ATU_SETUP, 0, 0, 0);
+	else
+		ret = dw_pcie_write(exynos_pcie->rc_dbi_base + (where), size, val);
+
+	if (is_linked == 0) {
+		if (exynos_pcie->phy_ops.phy_check_rx_elecidle)
+			exynos_pcie->phy_ops.phy_check_rx_elecidle(exynos_pcie->phy_pcs_base,
+								   ENABLE_ELECIDLE,
+								   exynos_pcie->ch_num);
+
+		exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+		exynos_pcie_rc_clock_enable(pp, PCIE_DISABLE_CLOCK);
+	}
+
+	spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+	return ret;
+}
+
+static void exynos_pcie_rc_prog_viewport_cfg0(struct dw_pcie_rp *pp, u32 busdev)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	/* ATU needs to be written just once during establish link */
+
+	if (exynos_pcie->atu_ok)
+		return;
+
+	exynos_pcie->atu_ok = 1;
+
+	if (exynos_pcie->use_secure_atu) {
+		exynos_pcie_rc_wr_own_conf(pp, SECURE_ATU_ENABLE, 0, 0);
+		return;
+	}
+
+	/* Program viewport 0 : OUTBOUND : CFG0 */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_BASE_OUTBOUND0, 4, pp->cfg0_base);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_BASE_OUTBOUND0, 4, (pp->cfg0_base >> 32));
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LIMIT_OUTBOUND0, 4,
+				   pp->cfg0_base + pp->cfg0_size - 1);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_TARGET_OUTBOUND0, 4, busdev);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_TARGET_OUTBOUND0, 4, 0);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR1_OUTBOUND0, 4, PCIE_ATU_TYPE_CFG0);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR2_OUTBOUND0, 4, EXYNOS_PCIE_ATU_ENABLE);
+}
+
+static void exynos_pcie_rc_prog_viewport_mem_outbound(struct dw_pcie_rp *pp)
+{
+	struct resource_entry *entry =
+		resource_list_first_type(&pp->bridge->windows, IORESOURCE_MEM);
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	/* Secure ATU is already setup by prog_viewport_cfg0, so skip it here */
+	if (exynos_pcie->use_secure_atu)
+		return;
+
+	/* Program viewport 0 : OUTBOUND : MEM */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR1_OUTBOUND1, 4, EXYNOS_PCIE_ATU_TYPE_MEM);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_BASE_OUTBOUND1, 4, entry->res->start);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_BASE_OUTBOUND1, 4, (entry->res->start >> 32));
+	/* exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LIMIT_OUTBOUND1, 4,
+	 *			      entry->res->start +
+	 *			      resource_size(entry->res) - 1);
+	 */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LIMIT_OUTBOUND1, 4, entry->res->start + SZ_2M - 1);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_TARGET_OUTBOUND1, 4,
+				   entry->res->start - entry->offset);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_TARGET_OUTBOUND1, 4,
+				   upper_32_bits(entry->res->start -
+						 entry->offset));
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR2_OUTBOUND1, 4, EXYNOS_PCIE_ATU_ENABLE);
+}
+
+int exynos_pcie_rc_set_bar(int ch_num, u32 bar_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct pci_dev *ep_pci_dev;
+	u32 val;
+
+	pr_info("%s: +++\n", __func__);
+
+	if (exynos_pcie->state == STATE_LINK_UP) {
+		ep_pci_dev = exynos_pcie_get_pci_dev(pp);
+	} else {
+		pr_info("%s: PCIe link is not up\n", __func__);
+
+		return -EPIPE;
+	}
+
+	/* EP BAR setup */
+	ep_pci_dev->resource[bar_num].start =
+		exynos_pcie->btl_target_addr + exynos_pcie->btl_offset;
+	ep_pci_dev->resource[bar_num].end =
+		exynos_pcie->btl_target_addr + exynos_pcie->btl_offset + exynos_pcie->btl_size;
+	ep_pci_dev->resource[bar_num].flags = 0x82000000;
+	pci_assign_resource(ep_pci_dev, bar_num);
+
+	pci_read_config_dword(ep_pci_dev, PCI_BASE_ADDRESS_0 + (bar_num * 0x4), &val);
+	pr_info("%s: Check EP BAR[%d] = 0x%x\n", __func__, bar_num, val);
+
+	pr_info("%s: ---\n", __func__);
+	return 0;
+}
+
+int exynos_pcie_rc_set_outbound_atu(int ch_num, u32 target_addr, u32 offset, u32 size)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct resource_entry *entry =
+		resource_list_first_type(&pp->bridge->windows, IORESOURCE_MEM);
+	u32 val;
+	int ret;
+
+	pr_info("%s: +++\n", __func__);
+
+	exynos_pcie->btl_target_addr = target_addr;
+	exynos_pcie->btl_offset = offset;
+	exynos_pcie->btl_size = size;
+
+	pr_info("%s: target_addr = 0x%x, offset = 0x%x, size = 0x%x\n", __func__,
+		exynos_pcie->btl_target_addr,
+		exynos_pcie->btl_offset,
+		exynos_pcie->btl_size);
+
+	/* Only for BTL */
+	/* 0x1420_0000 ~ (size -1) */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR1_OUTBOUND2, 4, EXYNOS_PCIE_ATU_TYPE_MEM);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_BASE_OUTBOUND2, 4, entry->res->start + SZ_2M);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_BASE_OUTBOUND2, 4,
+				   ((entry->res->start + SZ_2M) >> 32));
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LIMIT_OUTBOUND2, 4,
+				   entry->res->start + SZ_2M + exynos_pcie->btl_size - 1);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_LOWER_TARGET_OUTBOUND2, 4,
+				   exynos_pcie->btl_target_addr + exynos_pcie->btl_offset);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_UPPER_TARGET_OUTBOUND2, 4, 0);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_ATU_CR2_OUTBOUND2, 4, EXYNOS_PCIE_ATU_ENABLE);
+
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_CR1_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_CR1_OUTBOUND2(0x400) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_LOWER_BASE_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_LOWER_BASE_OUTBOUND2(0x408) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_UPPER_BASE_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_UPPER_BASE_OUTBOUND2(0x40C) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_LIMIT_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_LIMIT_OUTBOUND2(0x410) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_LOWER_TARGET_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_LOWER_TARGET_OUTBOUND2(0x414) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_UPPER_TARGET_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_UPPER_TARGET_OUTBOUND2(0x418) = 0x%x\n", __func__, val);
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_ATU_CR2_OUTBOUND2, 4, &val);
+	pr_info("%s:  PCIE_ATU_CR2_OUTBOUND2(0x404) = 0x%x\n", __func__, val);
+
+	ret = exynos_pcie_rc_set_bar(ch_num, 2);
+
+	pr_info("%s: ---\n", __func__);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_set_outbound_atu);
+
+static int exynos_pcie_rc_rd_other_conf(struct dw_pcie_rp *pp, struct pci_bus *bus, u32 devfn,
+					int where, int size, u32 *val)
+{
+	u32 busdev, cfg_size;
+	u64 cpu_addr;
+	void __iomem *va_cfg_base;
+
+	busdev = EXYNOS_PCIE_ATU_BUS(bus->number) | EXYNOS_PCIE_ATU_DEV(PCI_SLOT(devfn)) |
+		 EXYNOS_PCIE_ATU_FUNC(PCI_FUNC(devfn));
+
+	cpu_addr = pp->cfg0_base;
+	cfg_size = pp->cfg0_size;
+	va_cfg_base = pp->va_cfg0_base;
+	/* setup ATU for cfg/mem outbound */
+	exynos_pcie_rc_prog_viewport_cfg0(pp, busdev);
+
+	return dw_pcie_read(va_cfg_base + where, size, val);
+}
+
+static int exynos_pcie_rc_wr_other_conf(struct dw_pcie_rp *pp, struct pci_bus *bus, u32 devfn,
+					int where, int size, u32 val)
+{
+	u32 busdev, cfg_size;
+	u64 cpu_addr;
+	void __iomem *va_cfg_base;
+
+	busdev = EXYNOS_PCIE_ATU_BUS(bus->number) | EXYNOS_PCIE_ATU_DEV(PCI_SLOT(devfn)) |
+		 EXYNOS_PCIE_ATU_FUNC(PCI_FUNC(devfn));
+
+	cpu_addr = pp->cfg0_base;
+	cfg_size = pp->cfg0_size;
+	va_cfg_base = pp->va_cfg0_base;
+	/* setup ATU for cfg/mem outbound */
+	exynos_pcie_rc_prog_viewport_cfg0(pp, busdev);
+
+	return dw_pcie_write(va_cfg_base + where, size, val);
+}
+
+static int exynos_pcie_rc_rd_other_conf_new(struct pci_bus *bus,
+					    unsigned int devfn,
+					    int where, int size, u32 *val)
+{
+	struct dw_pcie_rp *pp = bus->sysdata;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int ret = 0;
+
+	if (exynos_pcie->state == STATE_LINK_UP)
+		ret = exynos_pcie_rc_rd_other_conf(pp, bus, devfn, where, size, val);
+	return ret;
+}
+
+static int exynos_pcie_rc_wr_other_conf_new(struct pci_bus *bus,
+					    unsigned int devfn,
+					    int where, int size, u32 val)
+{
+	struct dw_pcie_rp *pp = bus->sysdata;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int ret = 0;
+
+	if (exynos_pcie->state == STATE_LINK_UP)
+		ret = exynos_pcie_rc_wr_other_conf(pp, bus, devfn, where, size, val);
+	return ret;
+}
+
+static int exynos_pcie_rc_rd_own_conf_new(struct pci_bus *bus,
+					  unsigned int devfn,
+					  int where, int size, u32 *val)
+{
+	struct dw_pcie_rp *pp = bus->sysdata;
+
+	if (PCI_SLOT(devfn) > 0) {
+		*val = ~0;
+		return PCIBIOS_DEVICE_NOT_FOUND;
+	}
+
+	return exynos_pcie_rc_rd_own_conf(pp, where, size, val);
+}
+
+static int exynos_pcie_rc_wr_own_conf_new(struct pci_bus *bus,
+					  unsigned int devfn,
+					  int where, int size, u32 val)
+{
+	struct dw_pcie_rp *pp = bus->sysdata;
+
+	if (PCI_SLOT(devfn) > 0)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	return exynos_pcie_rc_wr_own_conf(pp, where, size, val);
+}
+
+static struct pci_ops exynos_pcie_rc_child_ops = {
+	.read = exynos_pcie_rc_rd_other_conf_new,
+	.write = exynos_pcie_rc_wr_other_conf_new
+};
+
+static struct pci_ops exynos_pcie_rc_root_ops = {
+	.read = exynos_pcie_rc_rd_own_conf_new,
+	.write = exynos_pcie_rc_wr_own_conf_new
+};
+
+u32 exynos_pcie_rc_read_dbi(struct dw_pcie *pci, void __iomem *base, u32 reg, size_t size)
+{
+	struct dw_pcie_rp *pp = &pci->pp;
+	u32 val;
+
+	exynos_pcie_rc_rd_own_conf(pp, reg, size, &val);
+
+	return val;
+}
+
+void exynos_pcie_rc_write_dbi(struct dw_pcie *pci, void __iomem *base, u32 reg, size_t size,
+			      u32 val)
+{
+	struct dw_pcie_rp *pp = &pci->pp;
+
+	exynos_pcie_rc_wr_own_conf(pp, reg, size, val);
+}
+
+static int exynos_pcie_rc_link_up(struct dw_pcie *pci)
+{
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	u32 val;
+
+	if (exynos_pcie->state != STATE_LINK_UP)
+		return 0;
+
+	val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP) & PCIE_ELBI_LTSSM_STATE_MASK;
+	if (val >= S_RCVRY_LOCK && val <= S_L1_IDLE)
+		return 1;
+
+	return 0;
+}
+
+static const struct dw_pcie_ops dw_pcie_ops = {
+	.read_dbi = exynos_pcie_rc_read_dbi,
+	.write_dbi = exynos_pcie_rc_write_dbi,
+	.link_up = exynos_pcie_rc_link_up,
+};
+
+static void exynos_pcie_rc_set_iocc(struct dw_pcie_rp *pp, int enable)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int val;
+	u32 sysreg_sharability;
+
+	if (!exynos_pcie->use_cache_coherency) {
+		dev_err(pci->dev, "IOCC: use_cache_coherency = false\n");
+
+		return;
+	}
+
+	if (exynos_pcie->ip_ver != EXYNOS_IP_VER_OF_WHI) {
+		dev_info(pci->dev, "IOCC: not supported SoC(ip_ver=0x%x)\n", exynos_pcie->ip_ver);
+
+		return;
+	}
+
+	/* set SYSREG SHAABILITY sfr offset of HSI1(GEN4A_0) or HSI2(GEN4A_1) */
+	if (exynos_pcie->ch_num == 0)		/* HSI1 */
+		sysreg_sharability = PCIE_SYSREG_HSI1_SHARABILITY_CTRL;
+	else if (exynos_pcie->ch_num == 1) {	/* HSI2 */
+		sysreg_sharability = PCIE_SYSREG_HSI2_SHARABILITY_CTRL;
+	} else {
+		dev_err(pci->dev, "IOCC: not supported: wrong ch_num\n");
+
+		return;
+	}
+
+	if (enable) {
+		dev_info(pci->dev, "enable cache coherency.\n");
+
+		/* set PCIe Axcache[1] = 1 */
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_COHERENCY_CONTROL_3_OFF, 4, 0x10101010);
+
+		/* set PCIe Shareability */
+		val = exynos_sysreg_read(exynos_pcie, sysreg_sharability);
+		val &= ~(PCIE_SYSREG_HSIX_SHARABLE_MASK);
+		val |= PCIE_SYSREG_HSIX_SHARABLE_ENABLE;
+		exynos_sysreg_write(exynos_pcie, val, sysreg_sharability);
+	} else {
+		dev_info(pci->dev, "disable cache coherency.\n");
+
+		/* clear PCIe Axcache[1] = 1 */
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_COHERENCY_CONTROL_3_OFF, 4, 0x0);
+
+		/* clear PCIe Shareability */
+		val = exynos_sysreg_read(exynos_pcie, sysreg_sharability);
+		val &= ~(PCIE_SYSREG_HSIX_SHARABLE_MASK);
+		exynos_sysreg_write(exynos_pcie, val, sysreg_sharability);
+	}
+
+#if IS_ENABLED(CONFIG_EXYNOS_PCIE_IOMMU)
+	pcie_sysmmu_set_use_iocc(pcie_ch_to_hsi(exynos_pcie->ch_num));
+#endif
+
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_COHERENCY_CONTROL_3_OFF, 4, &val);
+	dev_info(pci->dev, "PCIe Axcache[1] = 0x%x\n", val);
+
+	dev_info(pci->dev, "PCIe Shareability(offset: 0x%x) = 0x%x\n", sysreg_sharability,
+		exynos_sysreg_read(exynos_pcie, sysreg_sharability));
+}
+
+static int exynos_pcie_rc_parse_dt(struct device *dev, struct exynos_pcie *exynos_pcie)
+{
+	struct device_node *np = dev->of_node;
+	struct device_node *syscon_np;
+	struct resource res;
+	const char *use_cache_coherency;
+	const char *use_sicd;
+	const char *use_sysmmu;
+	const char *use_ia;
+	const char *use_l1ss;
+	const char *use_secure_atu;
+	const char *use_nclkoff_en;
+	const char *use_pcieon_sleep;
+	const char *use_phy_isol_con;
+
+	if (of_property_read_u32(np, "ip-ver", &exynos_pcie->ip_ver)) {
+		dev_err(dev, "Failed to parse the number of ip-ver\n");
+
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "pcie-clk-num", &exynos_pcie->pcie_clk_num)) {
+		dev_err(dev, "Failed to parse the number of pcie clock\n");
+
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "phy-clk-num", &exynos_pcie->phy_clk_num)) {
+		dev_err(dev, "Failed to parse the number of phy clock\n");
+
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "pmu-offset", &exynos_pcie->pmu_offset)) {
+		dev_err(dev, "Failed to parse the number of pmu-offset\n");
+
+		return -EINVAL;
+	}
+
+	if (of_property_read_u32(np, "ep-device-type", &exynos_pcie->ep_device_type)) {
+		dev_err(dev, "EP device type is NOT defined, device type is 'EP_NO_DEVICE(0)'\n");
+		exynos_pcie->ep_device_type = EP_NO_DEVICE;
+	}
+
+	if (of_property_read_u32(np, "max-link-speed", &exynos_pcie->max_link_speed)) {
+		dev_err(dev, "MAX Link Speed is NOT defined...(GEN1)\n");
+		/* Default Link Speet is GEN1 */
+		exynos_pcie->max_link_speed = LINK_SPEED_GEN1;
+	}
+
+	if (of_property_read_u32(np, "chip-ver", &exynos_pcie->chip_ver)) {
+		dev_err(dev, "Failed to parse the number of chip-ver, default '0'\n");
+		exynos_pcie->chip_ver = 0;
+	}
+
+	if (of_property_read_u32(np, "num-lanes", &exynos_pcie->num_lanes)) {
+		dev_err(dev, "Failed to parse the # of lanes, default '1'\n");
+		exynos_pcie->num_lanes = 1;
+	} else {
+		dev_info(dev, "parse the number of lanes: %d\n", exynos_pcie->num_lanes);
+	}
+
+	if (!of_property_read_string(np, "use-cache-coherency", &use_cache_coherency)) {
+		if (!strcmp(use_cache_coherency, "true")) {
+			dev_info(dev, "Cache Coherency unit is ENABLED.\n");
+			exynos_pcie->use_cache_coherency = true;
+		} else if (!strcmp(use_cache_coherency, "false")) {
+			exynos_pcie->use_cache_coherency = false;
+		} else {
+			dev_err(dev, "Invalid use-cache-coherency value(Set to default->false)\n");
+			exynos_pcie->use_cache_coherency = false;
+		}
+	} else {
+		exynos_pcie->use_cache_coherency = false;
+	}
+
+	if (!of_property_read_string(np, "use-sicd", &use_sicd)) {
+		if (!strcmp(use_sicd, "true")) {
+			dev_info(dev, "## PCIe use SICD\n");
+			exynos_pcie->use_sicd = true;
+		} else if (!strcmp(use_sicd, "false")) {
+			dev_info(dev, "## PCIe don't use SICD\n");
+			exynos_pcie->use_sicd = false;
+		} else {
+			dev_err(dev, "Invalid use-sicd value(set to default->false)\n");
+			exynos_pcie->use_sicd = false;
+		}
+	} else {
+		exynos_pcie->use_sicd = false;
+	}
+
+	if (!of_property_read_string(np, "use-pcieon-sleep", &use_pcieon_sleep)) {
+		if (!strcmp(use_pcieon_sleep, "true")) {
+			dev_info(dev, "## PCIe use PCIE ON Sleep\n");
+			exynos_pcie->use_pcieon_sleep = true;
+		} else if (!strcmp(use_pcieon_sleep, "false")) {
+			dev_info(dev, "## PCIe don't use PCIE ON Sleep\n");
+			exynos_pcie->use_pcieon_sleep = false;
+		} else {
+			dev_err(dev, "Invalid use-pcieon-sleep value(set to default->false)\n");
+			exynos_pcie->use_pcieon_sleep = false;
+		}
+	} else {
+		exynos_pcie->use_pcieon_sleep = false;
+	}
+
+	if (!of_property_read_string(np, "use-sysmmu", &use_sysmmu)) {
+		if (!strcmp(use_sysmmu, "true")) {
+			dev_info(dev, "PCIe SysMMU is ENABLED.\n");
+			exynos_pcie->use_sysmmu = true;
+		} else if (!strcmp(use_sysmmu, "false")) {
+			dev_info(dev, "PCIe SysMMU is DISABLED.\n");
+			exynos_pcie->use_sysmmu = false;
+		} else {
+			dev_err(dev, "Invalid use-sysmmu value(set to default->false)\n");
+			exynos_pcie->use_sysmmu = false;
+		}
+	} else {
+		exynos_pcie->use_sysmmu = false;
+	}
+
+	if (!of_property_read_string(np, "use-ia", &use_ia)) {
+		if (!strcmp(use_ia, "true")) {
+			dev_info(dev, "PCIe I/A is ENABLED.\n");
+			exynos_pcie->use_ia = true;
+		} else if (!strcmp(use_ia, "false")) {
+			dev_info(dev, "PCIe I/A is DISABLED.\n");
+			exynos_pcie->use_ia = false;
+		} else {
+			dev_err(dev, "Invalid use-ia value(set to default->false)\n");
+			exynos_pcie->use_ia = false;
+		}
+	} else {
+		exynos_pcie->use_ia = false;
+	}
+
+	if (!of_property_read_string(np, "use-l1ss", &use_l1ss)) {
+		if (!strcmp(use_l1ss, "true")) {
+			dev_info(dev, "PCIe L1SS(L1.2) is ENABLED.\n");
+			exynos_pcie->use_l1ss = true;
+		} else if (!strcmp(use_l1ss, "false")) {
+			dev_info(dev, "PCIe L1SS(L1.2) is DISABLED.\n");
+			exynos_pcie->use_l1ss = false;
+		} else {
+			dev_err(dev, "Invalid use-l1ss value(default=false)\n");
+			exynos_pcie->use_l1ss = false;
+		}
+	} else {
+		exynos_pcie->use_l1ss = false;
+	}
+
+	exynos_pcie->use_secure_atu = false;
+	if (!of_property_read_string(np, "use-secure-atu", &use_secure_atu)) {
+		if (!strcmp(use_secure_atu, "true")) {
+			dev_info(dev, "PCIe Secure ATU is ENABLED.\n");
+			exynos_pcie->use_secure_atu = true;
+		} else if (!strcmp(use_secure_atu, "false"))
+			dev_info(dev, "PCIe Secure ATU is DISABLED.\n");
+		else
+			dev_err(dev, "Invalid use-secure-atu value(default=false)\n");
+	}
+
+	if (!of_property_read_string(np, "use-nclkoff-en", &use_nclkoff_en)) {
+		if (!strcmp(use_nclkoff_en, "true")) {
+			dev_info(dev, "PCIe NCLKOFF is ENABLED.\n");
+			exynos_pcie->use_nclkoff_en = true;
+		} else if (!strcmp(use_nclkoff_en, "false")) {
+			dev_info(dev, "PCIe NCLKOFF is DISABLED.\n");
+			exynos_pcie->use_nclkoff_en = false;
+		} else {
+			dev_err(dev, "Invalid use-nclkoff_en value(set to default -> false)\n");
+			exynos_pcie->use_nclkoff_en = false;
+		}
+	} else {
+		exynos_pcie->use_nclkoff_en = false;
+	}
+
+	if (!of_property_read_string(np, "use-phy-isol-en", &use_phy_isol_con)) {
+		if (!strcmp(use_phy_isol_con, "true")) {
+			dev_info(dev, "PCIe DYNAMIC PHY ISOLATION is Enabled.\n");
+			exynos_pcie->use_phy_isol_con = true;
+		} else if (!strcmp(use_phy_isol_con, "false")) {
+			dev_info(dev, "PCIe DYNAMIC PHY ISOLATION is Disabled.\n");
+			exynos_pcie->use_phy_isol_con = false;
+		} else {
+			dev_err(dev, "Invalid use-phy-isol-en value(set to default -> false)\n");
+			exynos_pcie->use_phy_isol_con = false;
+		}
+	} else {
+		exynos_pcie->use_phy_isol_con = false;
+	}
+
+	if (!exynos_pcie->use_phy_isol_con)
+		exynos_pcie->phy_control = PCIE_PHY_BYPASS;
+
+#if 0//IS_ENABLED(CONFIG_PM_DEVFREQ)
+	if (of_property_read_u32(np, "pcie-pm-qos-int", &exynos_pcie->int_min_lock))
+		exynos_pcie->int_min_lock = 0;
+
+	if (exynos_pcie->int_min_lock)
+		exynos_pm_qos_add_request(&exynos_pcie_int_qos[exynos_pcie->ch_num],
+					  PM_QOS_DEVICE_THROUGHPUT, 0);
+
+	dev_info(dev, "%s: pcie int_min_lock = %d\n", __func__, exynos_pcie->int_min_lock);
+#endif
+	exynos_pcie->pmureg = syscon_regmap_lookup_by_phandle(np, "samsung,syscon-phandle");
+	if (IS_ERR(exynos_pcie->pmureg)) {
+		dev_err(dev, "syscon regmap lookup failed.\n");
+		return PTR_ERR(exynos_pcie->pmureg);
+	}
+
+	syscon_np = of_parse_phandle(np, "samsung,syscon-phandle", 0);
+	if (!syscon_np) {
+		dev_err(dev, "syscon device node not found\n");
+		return -EINVAL;
+	}
+
+	if (of_address_to_resource(syscon_np, 0, &res)) {
+		dev_err(dev, "failed to get syscon base address\n");
+		return -ENOMEM;
+	}
+
+	exynos_pcie->pmu_alive_pa = res.start;
+
+	exynos_pcie->sysreg = syscon_regmap_lookup_by_phandle(np, "samsung,sysreg-phandle");
+	/* Check definitions to access SYSREG in DT*/
+	if (IS_ERR(exynos_pcie->sysreg) && IS_ERR(exynos_pcie->sysreg_base)) {
+		dev_err(dev, "SYSREG is not defined.\n");
+		return PTR_ERR(exynos_pcie->sysreg);
+	}
+
+	/* SSD & WIFI power control */
+	exynos_pcie->wlan_gpio = of_get_named_gpio(np, "pcie,wlan-gpio", 0);
+	if (exynos_pcie->wlan_gpio < 0) {
+		dev_err(dev, "wlan gpio is not defined -> don't use wifi through pcie#%d\n",
+			exynos_pcie->ch_num);
+	} else {
+		gpio_direction_output(exynos_pcie->wlan_gpio, 0);
+	}
+
+	exynos_pcie->ssd_gpio = of_get_named_gpio(np, "pcie,ssd-gpio", 0);
+	if (exynos_pcie->ssd_gpio < 0) {
+		dev_err(dev, "ssd gpio is not defined -> don't use ssd through pcie#%d\n",
+			exynos_pcie->ch_num);
+	} else {
+		gpio_direction_output(exynos_pcie->ssd_gpio, 0);
+	}
+
+	return 0;
+}
+
+static int exynos_pcie_rc_get_pin_state(struct platform_device *pdev,
+					struct exynos_pcie *exynos_pcie)
+{
+	struct device *dev = &pdev->dev;
+	struct device_node *np = dev->of_node;
+	int ret;
+
+	exynos_pcie->perst_gpio = of_get_named_gpio(np, "gpios", 0);
+	if (exynos_pcie->perst_gpio < 0) {
+		dev_err(&pdev->dev, "cannot get perst_gpio\n");
+	} else {
+		ret = devm_gpio_request_one(dev, exynos_pcie->perst_gpio,
+					    GPIOF_OUT_INIT_LOW, dev_name(dev));
+		if (ret)
+			return -EINVAL;
+	}
+	/* Get pin state */
+	exynos_pcie->pcie_pinctrl = devm_pinctrl_get(&pdev->dev);
+	if (IS_ERR(exynos_pcie->pcie_pinctrl)) {
+		dev_err(&pdev->dev, "Can't get pcie pinctrl!!!\n");
+
+		return -EINVAL;
+	}
+	exynos_pcie->pin_state[PCIE_PIN_ACTIVE] =
+		pinctrl_lookup_state(exynos_pcie->pcie_pinctrl, "active");
+	if (IS_ERR(exynos_pcie->pin_state[PCIE_PIN_ACTIVE])) {
+		dev_err(&pdev->dev, "Can't set pcie clkerq to output high!\n");
+
+		return -EINVAL;
+	}
+	exynos_pcie->pin_state[PCIE_PIN_IDLE] =
+		pinctrl_lookup_state(exynos_pcie->pcie_pinctrl, "idle");
+	if (IS_ERR(exynos_pcie->pin_state[PCIE_PIN_IDLE]))
+		dev_err(&pdev->dev, "No idle pin state(but it's OK)!!\n");
+	else
+		pinctrl_select_state(exynos_pcie->pcie_pinctrl,
+				     exynos_pcie->pin_state[PCIE_PIN_IDLE]);
+
+	return 0;
+}
+
+static int exynos_pcie_rc_clock_get(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct device *dev = pci->dev;
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct exynos_pcie_clks	*clks = &exynos_pcie->clks;
+	int i, total_clk_num, phy_count;
+
+	/*
+	 * CAUTION - PCIe and phy clock have to define in order.
+	 * You must define related PCIe clock first in DT.
+	 */
+	total_clk_num = exynos_pcie->pcie_clk_num + exynos_pcie->phy_clk_num;
+
+	for (i = 0; i < total_clk_num; i++) {
+		if (i < exynos_pcie->pcie_clk_num) {
+			clks->pcie_clks[i] = of_clk_get(dev->of_node, i);
+			if (IS_ERR(clks->pcie_clks[i])) {
+				dev_err(dev, "Failed to get pcie clock\n");
+
+				return -ENODEV;
+			}
+		} else {
+			phy_count = i - exynos_pcie->pcie_clk_num;
+			clks->phy_clks[phy_count] = of_clk_get(dev->of_node, i);
+			if (IS_ERR(clks->phy_clks[i])) {
+				dev_err(dev, "Failed to get pcie clock\n");
+
+				return -ENODEV;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int exynos_pcie_rc_nclkoff_ctrl(struct platform_device *pdev,
+				       struct exynos_pcie *exynos_pcie)
+{
+#ifdef NEED_NCLKOFF	/* NEED_NCLKOFF is always 0(not defined) in gs101 */
+	struct device *dev = &pdev->dev;
+	u32 val;
+
+	dev_info(dev, "control NCLK OFF to prevent DBI asseccing when PCIE off\n");
+
+	/* need to check base address & offset of each channel's sysreg */
+	val = readl(exynos_pcie->sysreg_base + 0x4);
+	dev_info(dev, "orig HSI1_PCIE_GEN4_0_BUS_CTRL: 0x%x\n", val);
+	val &= ~PCIE_SUB_CTRL_SLV_EN;
+	val &= ~PCIE_SLV_BUS_NCLK_OFF;
+	val &= ~PCIE_DBI_BUS_NCLK_OFF;
+	writel(val, exynos_pcie->sysreg_base);
+	dev_info(dev, "aft HSI1_PCIE_GEN4_0_BUS_CTRL: 0x%x\n", val);
+#endif
+	return 0;
+}
+
+static int exynos_pcie_rc_get_resource(struct platform_device *pdev,
+				       struct exynos_pcie *exynos_pcie)
+{
+	struct resource *temp_rsc;
+	int ret;
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "elbi");
+	exynos_pcie->elbi_base_physical_addr = temp_rsc->start;
+	exynos_pcie->elbi_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (IS_ERR(exynos_pcie->elbi_base)) {
+		ret = PTR_ERR(exynos_pcie->elbi_base);
+
+		return ret;
+	}
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "udbg");
+	exynos_pcie->udbg_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (temp_rsc && IS_ERR(exynos_pcie->udbg_base)) {
+		ret = PTR_ERR(exynos_pcie->udbg_base);
+		return ret;
+	}
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "phy");
+	exynos_pcie->phy_base_physical_addr = temp_rsc->start;
+	exynos_pcie->phy_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (IS_ERR(exynos_pcie->phy_base)) {
+		ret = PTR_ERR(exynos_pcie->phy_base);
+
+		return ret;
+	}
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "sysreg");
+	exynos_pcie->sysreg_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (IS_ERR(exynos_pcie->sysreg_base)) {
+		ret = PTR_ERR(exynos_pcie->sysreg_base);
+
+		return ret;
+	}
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "dbi");
+	exynos_pcie->rc_dbi_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (IS_ERR(exynos_pcie->rc_dbi_base)) {
+		ret = PTR_ERR(exynos_pcie->rc_dbi_base);
+
+		return ret;
+	}
+
+	temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "pcs");
+	exynos_pcie->phy_pcs_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+	if (IS_ERR(exynos_pcie->phy_pcs_base)) {
+		ret = PTR_ERR(exynos_pcie->phy_pcs_base);
+
+		return ret;
+	}
+
+	if (exynos_pcie->use_ia) {
+		temp_rsc = platform_get_resource_byname(pdev, IORESOURCE_MEM, "ia");
+		exynos_pcie->ia_base_physical_addr = temp_rsc->start;
+		exynos_pcie->ia_base = devm_ioremap_resource(&pdev->dev, temp_rsc);
+		if (IS_ERR(exynos_pcie->ia_base)) {
+			ret = PTR_ERR(exynos_pcie->ia_base);
+
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+static void exynos_pcie_rc_enable_interrupts(struct dw_pcie_rp *pp, int enable)
+{
+	u32 val_irq0, val_irq1, val_irq2;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	dev_info(pci->dev, "## %s PCIe INTERRUPT ##\n", enable ? "ENABLE" : "DISABLE");
+
+	if (enable) {
+		/* IRQ0: to enable INTX interrupt */
+		val_irq0 = exynos_elbi_read(exynos_pcie, PCIE_IRQ0_EN);
+		val_irq0 |= (IRQ_INTA_ENABLE | IRQ_INTB_ENABLE |
+			     IRQ_INTC_ENABLE | IRQ_INTD_ENABLE);
+		exynos_elbi_write(exynos_pcie, val_irq0, PCIE_IRQ0_EN);
+
+		/* IRQ1: to enable LINKDOWN interrupt */
+		val_irq1 = exynos_elbi_read(exynos_pcie, PCIE_IRQ1_EN);
+		val_irq1 |= IRQ_LINK_DOWN_ENABLE;
+		exynos_elbi_write(exynos_pcie, val_irq1, PCIE_IRQ1_EN);
+
+		/* IRQ2: to enable Completion_Timeout interrupt */
+		val_irq2 = exynos_elbi_read(exynos_pcie, PCIE_IRQ2_EN);
+		val_irq2 |= IRQ_RADM_CPL_TIMEOUT_ENABLE;
+		exynos_elbi_write(exynos_pcie, val_irq2, PCIE_IRQ2_EN);
+
+		dev_info(pci->dev, "enabled irqs:IRQ0_EN = 0x%x / IRQ1_EN = 0x%x / IRQ2_EN = 0x%x\n",
+			val_irq0, val_irq1, val_irq2);
+	} else {
+		exynos_elbi_write(exynos_pcie, 0, PCIE_IRQ0_EN);
+		exynos_elbi_write(exynos_pcie, 0, PCIE_IRQ1_EN);
+		exynos_elbi_write(exynos_pcie, 0, PCIE_IRQ2_EN);
+	}
+}
+
+static void __maybe_unused exynos_pcie_notify_callback(struct dw_pcie_rp *pp, int event)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	if (exynos_pcie->event_reg && exynos_pcie->event_reg->callback &&
+	    (exynos_pcie->event_reg->events & event)) {
+		struct exynos_pcie_notify *notify = &exynos_pcie->event_reg->notify;
+
+		notify->event = event;
+		notify->user = exynos_pcie->event_reg->user;
+		dev_info(pci->dev, "Callback for the event : %d\n", event);
+		exynos_pcie->event_reg->callback(notify);
+	} else {
+		dev_info(pci->dev, "Client driver does not have registration of the event : %d\n",
+			 event);
+		dev_info(pci->dev, "Force PCIe poweroff --> poweron\n");
+		exynos_pcie_rc_poweroff(exynos_pcie->ch_num);
+		exynos_pcie_rc_poweron(exynos_pcie->ch_num);
+	}
+}
+
+void exynos_pcie_rc_register_dump(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	u32 i, val_0, val_4, val_8, val_c;
+
+	pr_err("%s: +++\n", __func__);
+	/* ---------------------- */
+	/* Link Reg : 0x0 ~ 0x47C */
+	/* ---------------------- */
+	pr_err("[Print SUB_CTRL region]\n");
+	pr_err("offset:	0x0	0x4	0x8	0xC\n");
+	for (i = 0; i < 0x480; i += 0x10) {
+		pr_err("ELBI 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_elbi_read(exynos_pcie, i + 0x0),
+				exynos_elbi_read(exynos_pcie, i + 0x4),
+				exynos_elbi_read(exynos_pcie, i + 0x8),
+				exynos_elbi_read(exynos_pcie, i + 0xC));
+	}
+	pr_err("\n");
+
+	/* ---------------------- */
+	/* PHY Reg : 0x0 ~ 0x19C */
+	/* ---------------------- */
+	pr_err("[Print PHY region]\n");
+	pr_err("offset:	0x0	0x4	0x8	0xC\n");
+	for (i = 0; i < 0x200; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+	/* common */
+	pr_err("PHY 0x03F0:    0x%08x\n", exynos_phy_read(exynos_pcie, 0x3F0));
+
+	/* lane0 */
+	for (i = 0xE00; i < 0xED0; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+	pr_err("PHY 0x0FC0:    0x%08x\n", exynos_phy_read(exynos_pcie, 0xFC0));
+
+	/* lane1 */
+	for (i = (0xE00 + 0x800); i < ( 0xED0 + 0x800); i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+	pr_err("PHY 0x17C0 : 0x%08x\n",
+		exynos_phy_read(exynos_pcie, 0xFC0 + 0x800));
+
+	pr_err("PHY 0x760 : %#08x, 0x764 : %#08x\n",
+				exynos_phy_read(exynos_pcie, 0x760),
+				exynos_phy_read(exynos_pcie, 0x764));
+
+	/* pll control signal/state monitor */
+	for (i=0x760; i<0x780; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+
+	/* pll control/retry setting check */
+	for (i=0x7E0; i<0x800; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+
+	/* OC configuration settings (override, monitor, code, control) */
+	for (i=0xA70; i<0xBD0; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+
+	/* OC and rate change state, override, monitor, control */
+	for (i=0xD90; i<0xDF0; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+
+	/* lane OC configuration check */
+	for (i=0xFB0; i<0xFC0; i += 0x10) {
+		pr_err("PHY 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_read(exynos_pcie, i + 0x0),
+				exynos_phy_read(exynos_pcie, i + 0x4),
+				exynos_phy_read(exynos_pcie, i + 0x8),
+				exynos_phy_read(exynos_pcie, i + 0xC));
+	}
+	pr_err("\n");
+
+	/* ---------------------- */
+	/* PHY PCS : 0x0 ~ 0x19C */
+	/* ---------------------- */
+	pr_err("[Print PHY_PCS region]\n");
+	pr_err("offset:	0x0 	0x4	0x8	0xC\n");
+	for (i = 0; i < 0x200; i += 0x10) {
+		pr_err("PCS 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i,
+				exynos_phy_pcs_read(exynos_pcie, i + 0x0),
+				exynos_phy_pcs_read(exynos_pcie, i + 0x4),
+				exynos_phy_pcs_read(exynos_pcie, i + 0x8),
+				exynos_phy_pcs_read(exynos_pcie, i + 0xC));
+	}
+	pr_err("\n");
+
+	/* ---------------------- */
+	/* DBI : 0x0 ~ 0x8FC */
+	/* ---------------------- */
+	pr_err("[Print DBI region]\n");
+	pr_err("offset:	0x0	0x4	0x8	0xC\n");
+	for (i = 0; i < 0x900; i += 0x10) {
+		exynos_pcie_rc_rd_own_conf(pp, i + 0x0, 4, &val_0);
+		exynos_pcie_rc_rd_own_conf(pp, i + 0x4, 4, &val_4);
+		exynos_pcie_rc_rd_own_conf(pp, i + 0x8, 4, &val_8);
+		exynos_pcie_rc_rd_own_conf(pp, i + 0xC, 4, &val_c);
+		pr_err("DBI 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+				i, val_0, val_4, val_8, val_c);
+	}
+	pr_err("\n");
+
+	/* ---------------------- */
+	/* UDBG: 0xA000 ~ 0xAFFF */
+	/* ---------------------- */
+	if (!IS_ERR(exynos_pcie->udbg_base)) {
+		exynos_udbg_write(exynos_pcie, 0x13, 0xC604); /* change udbg read mode */
+		pr_err("[Print UDBG region]\n");
+		pr_err("offset: 0x0     0x4     0x8     0xC\n");
+		for (i = 0xA000; i < 0xB000; i += 0x10) {
+			pr_err("UDBG 0x%04x:    0x%08x    0x%08x    0x%08x    0x%08x\n",
+					i,
+					exynos_udbg_read(exynos_pcie, i + 0x0),
+					exynos_udbg_read(exynos_pcie, i + 0x4),
+					exynos_udbg_read(exynos_pcie, i + 0x8),
+					exynos_udbg_read(exynos_pcie, i + 0xC));
+		}
+		pr_err("\n");
+	}
+	pr_err("%s: ---\n", __func__);
+
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_register_dump);
+
+void exynos_pcie_rc_dump_link_down_status(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+
+	/* if (exynos_pcie->state == STATE_LINK_UP) { */
+		dev_info(pci->dev, "LTSSM: 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP));
+		dev_info(pci->dev, "LTSSM_H: 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_CXPL_DEBUG_INFO_H));
+		dev_info(pci->dev, "DMA_MONITOR1: 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_DMA_MONITOR1));
+		dev_info(pci->dev, "DMA_MONITOR2: 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_DMA_MONITOR2));
+		dev_info(pci->dev, "DMA_MONITOR3: 0x%08x\n",
+			 exynos_elbi_read(exynos_pcie, PCIE_DMA_MONITOR3));
+	/* } else { */
+		dev_info(pci->dev, "PCIE link state is %d\n", exynos_pcie->state);
+	/* } */
+}
+
+void exynos_pcie_rc_dump_all_status(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+	exynos_pcie_rc_print_link_history(pp);
+	exynos_pcie_rc_dump_link_down_status(exynos_pcie->ch_num);
+	exynos_pcie_rc_register_dump(exynos_pcie->ch_num);
+	spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_dump_all_status);
+
+void exynos_pcie_rc_dislink_work(struct work_struct *work)
+{
+	struct exynos_pcie *exynos_pcie = container_of(work, struct exynos_pcie, dislink_work.work);
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct device *dev = pci->dev;
+	unsigned long flags;
+
+	if (exynos_pcie->state == STATE_LINK_DOWN)
+		return;
+
+	if (exynos_pcie->ep_device_type != EP_SAMSUNG_MODEM) {
+		spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+		exynos_pcie_rc_print_link_history(pp);
+		exynos_pcie_rc_dump_link_down_status(exynos_pcie->ch_num);
+		exynos_pcie_rc_register_dump(exynos_pcie->ch_num);
+		spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+	}
+
+	exynos_pcie->linkdown_cnt++;
+	dev_info(dev, "link down and recovery cnt: %d\n", exynos_pcie->linkdown_cnt);
+	if (exynos_pcie->use_pcieon_sleep) {
+		dev_info(dev, "%s, pcie_is_linkup 0\n", __func__);
+		pcie_is_linkup = 0;
+	}
+
+	exynos_pcie_notify_callback(pp, EXYNOS_PCIE_EVENT_LINKDOWN);
+}
+
+void exynos_pcie_rc_cpl_timeout_work(struct work_struct *work)
+{
+	struct exynos_pcie *exynos_pcie =
+		container_of(work, struct exynos_pcie, cpl_timeout_work.work);
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct device *dev = pci->dev;
+	unsigned long flags;
+
+	if (exynos_pcie->state == STATE_LINK_DOWN)
+		return;
+
+	if (exynos_pcie->ep_device_type != EP_SAMSUNG_MODEM) {
+		spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+		exynos_pcie_rc_print_link_history(pp);
+		exynos_pcie_rc_dump_link_down_status(exynos_pcie->ch_num);
+		exynos_pcie_rc_register_dump(exynos_pcie->ch_num);
+		spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+	}
+
+	if (exynos_pcie->use_pcieon_sleep) {
+		dev_info(dev, "[%s] pcie_is_linkup = 0\n", __func__);
+		pcie_is_linkup = 0;
+	}
+
+	dev_info(dev, "call PCIE_CPL_TIMEOUT callback\n");
+	exynos_pcie_notify_callback(pp, EXYNOS_PCIE_EVENT_CPL_TIMEOUT);
+}
+
+static void exynos_pcie_rc_use_ia(struct exynos_pcie *exynos_pcie)
+{
+	if (!exynos_pcie->use_ia) {
+		pr_info("[%s] Not support I/A(use_ia = false)\n", __func__);
+
+		return;
+	}
+
+	/* PCIE_IA_IRQ Selection */
+	/* 1. Enable Link up Interrupt for PCIE_IA */
+	exynos_elbi_write(exynos_pcie, 0x400, 0x388);
+
+	/* BASE_ADDR */
+	/* Base_addr of WHI
+	 * 1) [elbi_base]
+	 *	ch0> 0x1192_0000, ch1> 0x1452_0000
+	 * 2) [phy_base]
+	 *	ch0> 0x1195_0000, ch1> 0x1455_0000
+	 * 3) [ia_base]
+	 *	ch0> 0x1190_0000, ch1> 0x1450_0000
+	 */
+	/* 1. GEN4_HSIx sub_con */
+	exynos_ia_write(exynos_pcie, exynos_pcie->elbi_base_physical_addr, 0x30);
+	/* 2. GEN4_HSIx pma */
+	exynos_ia_write(exynos_pcie, exynos_pcie->phy_base_physical_addr, 0x34);
+	/* 3. PCIE_IA_GEN4A&B base */
+	exynos_ia_write(exynos_pcie, exynos_pcie->ia_base_physical_addr, 0x38);
+
+	/* Loop CTRL */
+	/* 1. LOOP Interval, 0x3000(12288) x bus_clk_period */
+	exynos_ia_write(exynos_pcie, 0x00023000, 0x40);
+
+	/* EVENT: L1.2 EXIT Interrupt happens */
+	/* SQ0) UIR_1 : DATA MASK */
+	/* 1. UIR_1: DATA MASK_REG */
+	exynos_ia_write(exynos_pcie, 0x50000004, 0x100);
+	/* 2. ENABLE bit[15:0] */
+	exynos_ia_write(exynos_pcie, 0x00000100, 0x104);
+
+	/* SQ1) BRT */
+	/* 1. READ CDR_DONE REG */
+	exynos_ia_write(exynos_pcie, 0x20420008, 0x108);
+	/* 2. CHECK CDR_DONE */
+	exynos_ia_write(exynos_pcie, 0x00000100, 0x10C);
+
+	/* SQ2) UIR_1 : DATA MASK */
+	/* 1. UIR_1: DATA MASK_REG */
+	exynos_ia_write(exynos_pcie, 0x50000004, 0x110);
+	/* 2. ENABLE bit[15:0] */
+	exynos_ia_write(exynos_pcie, 0x00000400, 0x114);
+
+	/* SQ3) LOOP */
+	/* 1. CDR_EN 0x18 */
+	exynos_ia_write(exynos_pcie, 0x10000008, 0x118);
+	/* 2. LOOP until L1.2 exit IRQ clear,
+	 * but loop timeout because of pending still
+	 */
+	exynos_ia_write(exynos_pcie, 0x00000000, 0x11C);
+
+	/* SQ4) WRITE */
+	/* 1. CDR_EN 0x00 */
+	exynos_ia_write(exynos_pcie, 0x40020008, 0x120);
+	/* 2. Write0-Clear LOOP_CNT_OVER_INTR_STATUS */
+	exynos_ia_write(exynos_pcie, 0x00000100, 0x124);
+
+	/* SQ5) UIR_1 : DATA MASK */
+	/* 1. AFC toggle */
+	exynos_ia_write(exynos_pcie, 0x50000004, 0x128);
+	/* 2. ENABLE bit[15:0] */
+	exynos_ia_write(exynos_pcie, 0x000000F0, 0x12C);
+
+	/* SQ6) BRT */
+	/* 1. READ CDR_DONE REG */
+	exynos_ia_write(exynos_pcie, 0x20D10FC0, 0x130);
+	/* 2. CHECK CDR_DONE */
+	exynos_ia_write(exynos_pcie, 0x000000F0, 0x134);
+
+	/* SQ7) WRITE */
+	/* 1. CDR_EN 0x20 */
+	exynos_ia_write(exynos_pcie, 0x40010A48, 0x138);
+	exynos_ia_write(exynos_pcie, 0x00000020, 0x13C);
+
+	/* SQ8) WRITE */
+	/* 1. CDR_EN 0x30 */
+	exynos_ia_write(exynos_pcie, 0x40010A48, 0x140);
+	exynos_ia_write(exynos_pcie, 0x00000030, 0x144);
+
+	/* SQ9) WRITE */
+	/* 1. CDR_EN 0x00 */
+	exynos_ia_write(exynos_pcie, 0x40010A48, 0x148);
+	exynos_ia_write(exynos_pcie, 0x00000000, 0x14C);
+
+	/* SQ10) WRITE */
+	/* 1. AFC toggle */
+	exynos_ia_write(exynos_pcie, 0x40010BF4, 0x150);
+	exynos_ia_write(exynos_pcie, 0x00000005, 0x154);
+
+	/* SQ11) WRITE */
+	/* 1. AFC toggle */
+	exynos_ia_write(exynos_pcie, 0x40010BF4, 0x158);
+	exynos_ia_write(exynos_pcie, 0x00000004, 0x15C);
+
+	/* SQ12) BRF */
+	/* 1. READ CDR_DONE REG */
+	exynos_ia_write(exynos_pcie, 0x30010FC0, 0x160);
+	/* 2. CHECK CDR_DONE  */
+	exynos_ia_write(exynos_pcie, 0x000000F0, 0x164);
+
+	/* SQ13) WRITE */
+	/* 1. AFC toggle */
+	exynos_ia_write(exynos_pcie, 0x40010BF4, 0x168);
+	exynos_ia_write(exynos_pcie, 0x00000005, 0x16C);
+
+	/* SQ14) WRITE */
+	exynos_ia_write(exynos_pcie, 0x40000008, 0x170);
+	/* 1. Write1-Clear IRQ  */
+	exynos_ia_write(exynos_pcie, 0x00000400, 0x174);
+
+	/* SQ15) WRITE */
+	/* 1. RETURN to IDLE */
+	exynos_ia_write(exynos_pcie, 0x80000000, 0x178);
+	exynos_ia_write(exynos_pcie, 0x00000000, 0x17C);
+
+	/* PCIE_IA_EN */
+	exynos_ia_write(exynos_pcie, 0x00000001, 0x000);
+}
+
+void exynos_pcie_rc_assert_phy_reset(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ret;
+
+	ret = exynos_pcie_rc_phy_clock_enable(pp, PCIE_ENABLE_CLOCK);
+	dev_info(dev, "phy clk enable, ret value = %d\n", ret);
+	if (exynos_pcie->phy_ops.phy_config)
+		exynos_pcie->phy_ops.phy_config(exynos_pcie, exynos_pcie->ch_num);
+
+	/* Added for CDR Lock */
+	exynos_pcie_rc_use_ia(exynos_pcie);
+}
+
+void exynos_pcie_rc_resumed_phydown(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct device *dev = pci->dev;
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int ret;
+
+	ret = exynos_pcie_rc_clock_enable(pp, PCIE_ENABLE_CLOCK);
+	dev_info(dev, "pcie clk enable, ret value = %d\n", ret);
+
+	exynos_pcie_rc_enable_interrupts(pp, 0);
+	exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_BYPASS);
+
+	if (exynos_pcie->phy_ops.phy_all_pwrdn)
+		exynos_pcie->phy_ops.phy_all_pwrdn(exynos_pcie, exynos_pcie->ch_num);
+
+	exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+	exynos_pcie_rc_clock_enable(pp, PCIE_DISABLE_CLOCK);
+}
+
+static void exynos_pcie_setup_rc(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	u32 pcie_cap_off = PCIE_CAP_OFFSET;
+	u32 pm_cap_off = PM_CAP_OFFSET;
+	u32 val;
+
+	/* enable writing to DBI read-only registers */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MISC_CONTROL, 4, DBI_RO_WR_EN);
+
+	if (exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+		/* Disable BAR and Exapansion ROM BAR */
+		exynos_pcie_rc_wr_own_conf(pp, 0x100010, 4, 0);
+		exynos_pcie_rc_wr_own_conf(pp, 0x100030, 4, 0);
+	}
+
+	/* change vendor ID and device ID for PCIe */
+	exynos_pcie_rc_wr_own_conf(pp, PCI_VENDOR_ID, 2, PCI_VENDOR_ID_SAMSUNG);
+	exynos_pcie_rc_wr_own_conf(pp, PCI_DEVICE_ID, 2,
+				   PCI_DEVICE_ID_EXYNOS + exynos_pcie->ch_num);
+
+	/* set max link width & speed : Gen3, Lane1 */
+	exynos_pcie_rc_rd_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCAP, 4, &val);
+	val &= ~(PCI_EXP_LNKCAP_L1EL | PCI_EXP_LNKCAP_SLS);
+	val |= PCI_EXP_LNKCAP_L1EL_64USEC;
+	val |= PCI_EXP_LNKCTL2_TLS_8_0GB;
+
+	exynos_pcie_rc_wr_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCAP, 4, val);
+
+	/* set auxiliary clock frequency: 26MHz */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_AUX_CLK_FREQ_OFF, 4, PCIE_AUX_CLK_FREQ_26MHZ);
+
+	/* set duration of L1.2 & L1.2.Entry */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_L1_SUBSTATES_OFF, 4, PCIE_L1_SUB_VAL);
+
+	/* clear power management control and status register */
+	exynos_pcie_rc_wr_own_conf(pp, pm_cap_off + PCI_PM_CTRL, 4, 0x0);
+
+	/* set target speed from DT */
+	exynos_pcie_rc_rd_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCTL2, 4, &val);
+	val &= ~PCI_EXP_LNKCTL2_TLS;
+	val |= exynos_pcie->max_link_speed;
+	exynos_pcie_rc_wr_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCTL2, 4, val);
+
+	/* TBD : */
+	/* initiate link retraining */
+	/* exynos_pcie_rc_rd_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCTL, 4, &val);
+	 * val |= PCI_EXP_LNKCTL_RL;
+	 * exynos_pcie_rc_wr_own_conf(pp, pcie_cap_off + PCI_EXP_LNKCTL, 4, val);
+	 */
+
+	/* completion timeout setting values*/
+	/* Range/ Encoding/ Spec Minimum/ Spec Maximum/ PCIe controller Minimum/
+	 *						 PCIe controller Maximum
+	 *Default 0000b 50 us 50 ms 28 ms 44 ms (M-PCIe: 45)
+	 *	A 0001b 50 us 100 us 65 us 99 us (M-PCIe: 100)
+	 *	A 0010b 1 ms 10 ms 4.1 ms 6.2 ms (M-PCIe: 6.4)
+	 *	B 0101b 16 ms 55 ms 28 ms 44 ms (M-PCIe: 45)
+	 *	B 0110b 65 ms 210 ms 86 ms 131 ms (M-PCIe: 133)
+	 *	C 1001b 260 ms 900 ms 260 ms 390 ms (M-PCIe: 398)
+	 *	C 1010b 1s 3.5 s 1.8 s 2.8 s (M-PCIe: 2.8)
+	 *	D 1101b 4s 13 s 5.4 s 8.2 s (M-PCIe: 8.4)
+	 *	D 1110b 17s 64 s 38 s 58 s (M-PCIe: 59)
+	 */
+	exynos_pcie_rc_rd_own_conf(pp, pcie_cap_off + PCI_EXP_DEVCTL2, 4, &val);
+	/* pr_info("%s:read:device_ctrl_status2(0x98)=0x%x\n", __func__, val); */
+	val &= ~(PCIE_CAP_CPL_TIMEOUT_VAL_MASK);
+	val |= PCIE_CAP_CPL_TIMEOUT_VAL_44MS_DEFALT;
+	exynos_pcie_rc_wr_own_conf(pp, pcie_cap_off + PCI_EXP_DEVCTL2, 4, val);
+}
+
+static int exynos_pcie_rc_init(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	/* Setup RC to avoid initialization faile in PCIe stack */
+	if (exynos_pcie->use_phy_isol_con)
+		pp->bridge->ops = &exynos_pcie_rc_root_ops;
+	pp->bridge->child_ops = &exynos_pcie_rc_child_ops;
+	dw_pcie_setup_rc(pp);
+
+	return 0;
+}
+
+static struct dw_pcie_host_ops exynos_pcie_rc_ops = {
+	.init = exynos_pcie_rc_init,
+};
+
+void exynos_pcie_msi_post_process(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	int ctrl, num_ctrls;
+
+	if (exynos_pcie->separated_msi)
+		return;
+
+	num_ctrls = pp->num_vectors / MAX_MSI_IRQS_PER_CTRL;
+	for (ctrl = 0; ctrl < num_ctrls; ctrl++) {
+		/* clear MSI register because of edge */
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_MASK +
+				(ctrl * MSI_REG_CTRL_BLOCK_SIZE), 4, 0xffffffff);
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_MASK +
+				(ctrl * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x0);
+	}
+}
+
+void exynos_pcie_rc_force_linkdown_work(int ch_num) {
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct device *dev = pci->dev;
+
+	dev_info(dev, "start force Link down S/W recovery\n");
+	if (!exynos_pcie->cpl_timeout_recovery) {
+		if (!exynos_pcie->sudden_linkdown) {
+			exynos_pcie->sudden_linkdown = 1;
+			exynos_pcie->state = STATE_LINK_DOWN_TRY;
+			exynos_pcie_notify_callback(pp, EXYNOS_PCIE_EVENT_LINKDOWN);
+		}
+	}
+
+}
+EXPORT_SYMBOL(exynos_pcie_rc_force_linkdown_work);
+
+static irqreturn_t exynos_pcie_rc_irq_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	u32 val_irq0, val_irq1, val_irq2;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+
+	/* handle IRQ0 interrupt */
+	val_irq0 = exynos_elbi_read(exynos_pcie, PCIE_IRQ0);
+	exynos_elbi_write(exynos_pcie, val_irq0, PCIE_IRQ0);
+
+	/* handle IRQ1 interrupt */
+	val_irq1 = exynos_elbi_read(exynos_pcie, PCIE_IRQ1);
+	exynos_elbi_write(exynos_pcie, val_irq1, PCIE_IRQ1);
+
+	/* handle IRQ2 interrupt */
+	val_irq2 = exynos_elbi_read(exynos_pcie, PCIE_IRQ2);
+	exynos_elbi_write(exynos_pcie, val_irq2, PCIE_IRQ2);
+
+	/* only support after EXYNOS9820 EVT 1.1 */
+	if (val_irq1 & IRQ_LINK_DOWN_ASSERT) {
+		dev_info(dev, "(irq0 = 0x%x, irq1 = 0x%x, irq2 = 0x%x)\n",
+			 val_irq0, val_irq1, val_irq2);
+		exynos_pcie->link_stats.link_down_irq_count++;
+
+		if (!exynos_pcie->cpl_timeout_recovery) {
+			if (!exynos_pcie->sudden_linkdown) {
+				exynos_pcie->sudden_linkdown = 1;
+				exynos_pcie->state = STATE_LINK_DOWN_TRY;
+				queue_work(exynos_pcie->pcie_wq, &exynos_pcie->dislink_work.work);
+			}
+		}
+	}
+
+	if (val_irq2 & IRQ_RADM_CPL_TIMEOUT_ASSERT) {
+		dev_info(dev, "(irq0 = 0x%x, irq1 = 0x%x, irq2 = 0x%x)\n",
+				val_irq0, val_irq1, val_irq2);
+		exynos_pcie->link_stats.cmpl_timeout_irq_count++;
+
+		val_irq2 = exynos_elbi_read(exynos_pcie, PCIE_IRQ2);
+		dev_info(dev, "check irq22 pending clear: irq2_state = 0x%x\n", val_irq2);
+
+		if (exynos_pcie->sudden_linkdown) {
+			if (exynos_pcie->cpl_timeout_recovery) {
+				exynos_pcie->cpl_timeout_recovery = 1;
+				exynos_pcie->state = STATE_LINK_DOWN_TRY;
+				queue_work(exynos_pcie->pcie_wq,
+					   &exynos_pcie->cpl_timeout_work.work);
+			}
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+static void exynos_pcie_rc_send_pme_turn_off(struct exynos_pcie *exynos_pcie)
+{
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct device *dev = pci->dev;
+	int count = 0;
+	u32 val;
+
+	/* L1.2 enable check */
+	dev_info(dev, "Current PM state(PCS + 0x188) : 0x%x\n",
+		readl(exynos_pcie->phy_pcs_base + 0x188));
+	dev_info(dev, "DBI Link Control Register: 0x%x\n", readl(exynos_pcie->rc_dbi_base + 0x80));
+
+	val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP) & PCIE_ELBI_LTSSM_STATE_MASK;
+	dev_info(dev, "%s: link state:%x\n", __func__, val);
+	if (!(val >= S_RCVRY_LOCK && val <= S_L1_IDLE)) {
+		dev_info(dev, "%s, pcie link is not up\n", __func__);
+
+		return;
+	}
+
+	exynos_elbi_write(exynos_pcie, 0x1, PCIE_APP_REQ_EXIT_L1);
+	val = exynos_elbi_read(exynos_pcie, PCIE_APP_REQ_EXIT_L1_MODE);
+	val &= ~APP_REQ_EXIT_L1_MODE;
+	val |= L1_REQ_NAK_CONTROL_MASTER;
+	exynos_elbi_write(exynos_pcie, val, PCIE_APP_REQ_EXIT_L1_MODE);
+
+	exynos_elbi_write(exynos_pcie, 0x1, XMIT_PME_TURNOFF);
+
+	while (count < MAX_L2_TIMEOUT) {
+		if ((exynos_elbi_read(exynos_pcie, PCIE_IRQ0) & IRQ_RADM_PM_TO_ACK)) {
+			dev_info(dev, "ack message is ok\n");
+			udelay(10);
+
+			break;
+		}
+
+		udelay(10);
+		count++;
+	}
+
+	exynos_elbi_write(exynos_pcie, 0x0, XMIT_PME_TURNOFF);
+
+	count = 0;
+	do {
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		if (val == S_L2_IDLE) {
+			dev_info(dev, "received Enter_L23_READY DLLP packet\n");
+			break;
+		}
+		udelay(10);
+		count++;
+	} while (count < MAX_L2_TIMEOUT);
+}
+
+static int exynos_pcie_rc_establish_link(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	void __iomem *phy_base_regs = exynos_pcie->phy_base;
+	struct device *dev = pci->dev;
+	u32 val, busdev;
+	int count = 0, try_cnt = 0;
+	unsigned int save_before_state = 0xff;
+	bool pll_lock, cdr_lock, oc_done;
+	int lock_cnt;
+retry:
+	/* to call eyxnos_pcie_rc_pcie_phy_config() in cal.c file */
+	exynos_pcie_rc_assert_phy_reset(pp);
+
+	/* check pll lock */
+	pll_lock = check_exynos_pcie_reg_status(exynos_pcie, phy_base_regs,
+						0x03F0, BIT(3), BIT(3), &lock_cnt);
+
+	/* check cdr lock */
+	cdr_lock = check_exynos_pcie_reg_status(exynos_pcie, phy_base_regs,
+						0x0E0C, BIT(2), BIT(2), &lock_cnt);
+	if (cdr_lock)
+		link_stats_log_pll_lock(exynos_pcie, lock_cnt);
+
+	/* check offset calibration */
+	oc_done = check_exynos_pcie_reg_status(exynos_pcie, phy_base_regs,
+					       0x0DE8, 0xf, 0xf, &lock_cnt);
+	if (!oc_done)
+		dev_err(dev, "OC Fail: PLL_LOCK:%d, CDR_LOCK:%d, OC:%d\n",
+			pll_lock, cdr_lock, oc_done);
+
+	/* Soft Power RST */
+	val = exynos_elbi_read(exynos_pcie, PCIE_SOFT_RESET);
+	val &= ~SOFT_PWR_RESET;
+	exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+	/* old: udelay(20); */
+	mdelay(1);
+	val |= SOFT_PWR_RESET;
+	exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+
+	val = exynos_phy_read(exynos_pcie, 0x0E18) & (1 << 7);
+	/* Device Type (Sub Controller: DEVICE_TYPE offset: 0x80  */
+	exynos_elbi_write(exynos_pcie, 0x04, 0x80);
+
+	/* NON-sticky RST */
+	val = exynos_elbi_read(exynos_pcie, PCIE_SOFT_RESET);
+	val |= SOFT_NON_STICKY_RESET;
+	exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+	usleep_range(10, 12);
+	val &= ~SOFT_NON_STICKY_RESET;
+	exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+	mdelay(1);
+	val |= SOFT_NON_STICKY_RESET;
+	exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+
+	/* EQ Off */
+	exynos_pcie_rc_wr_own_conf(pp, 0x890, 4, 0x12000);
+
+	/* set #PERST high */
+	gpio_set_value(exynos_pcie->perst_gpio, 1);
+
+	dev_info(dev, "%s: Set PERST to HIGH, gpio val = %d\n",
+		__func__, gpio_get_value(exynos_pcie->perst_gpio));
+	if (exynos_pcie->ep_device_type == EP_BCM_WIFI) {
+		usleep_range(20000, 22000);
+	} else {
+		usleep_range(18000, 20000);
+	}
+
+	val = exynos_elbi_read(exynos_pcie, PCIE_APP_REQ_EXIT_L1_MODE);
+	val |= APP_REQ_EXIT_L1_MODE;
+	val |= L1_REQ_NAK_CONTROL_MASTER;
+	exynos_elbi_write(exynos_pcie, val, PCIE_APP_REQ_EXIT_L1_MODE);
+	exynos_elbi_write(exynos_pcie, PCIE_LINKDOWN_RST_MANUAL, PCIE_LINKDOWN_RST_CTRL_SEL);
+
+	/* Q-Channel support */
+	val = exynos_elbi_read(exynos_pcie, PCIE_QCH_SEL);
+	if (exynos_pcie->ip_ver >= 0x889500) {
+		val &= ~(CLOCK_GATING_PMU_MASK | CLOCK_GATING_APB_MASK | CLOCK_GATING_AXI_MASK);
+	} else {
+		val &= ~CLOCK_GATING_MASK;
+		val |= CLOCK_NOT_GATING;
+	}
+	exynos_elbi_write(exynos_pcie, val, PCIE_QCH_SEL);
+
+	/* NAK enable when AXI pending */
+	exynos_elbi_write(exynos_pcie, NACK_ENABLE, PCIE_MSTR_PEND_SEL_NAK);
+	dev_info(dev, "%s: NACK option enable: 0x%x\n", __func__,
+		exynos_elbi_read(exynos_pcie, PCIE_MSTR_PEND_SEL_NAK));
+
+	/* DBI L1 exit disable(use aux_clk in L1.2) */
+	exynos_elbi_write(exynos_pcie, DBI_L1_EXIT_DISABLE, PCIE_DBI_L1_EXIT_DISABLE);
+	dev_info(dev, "%s: DBI L1 exit disable option enable: 0x%x\n", __func__,
+		exynos_elbi_read(exynos_pcie, PCIE_DBI_L1_EXIT_DISABLE));
+
+	/* setup root complex */
+	dw_pcie_setup_rc(pp);
+	exynos_pcie_setup_rc(pp);
+
+	if (exynos_pcie->use_cache_coherency)
+		exynos_pcie_rc_set_iocc(pp, 1);
+
+	save_before_state = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP);
+	/* DBG: sleep_range(48000, 50000); */
+
+	/* assert LTSSM enable */
+	exynos_elbi_write(exynos_pcie, PCIE_ELBI_LTSSM_ENABLE, PCIE_APP_LTSSM_ENABLE);
+	count = 0;
+	while (count < MAX_TIMEOUT) {
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		if (val == S_L0)
+			break;
+		count++;
+		usleep_range(10, 12);
+	}
+
+	if (count >= MAX_TIMEOUT) {
+		try_cnt++;
+
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		exynos_pcie->link_stats.link_up_failure_count++;
+
+		restore_pma_regs(exynos_pcie);
+
+		if (try_cnt < 10) {
+			gpio_set_value(exynos_pcie->perst_gpio, 0);
+			dev_info(dev, "%s: Set PERST to LOW, gpio val = %d\n", __func__,
+				gpio_get_value(exynos_pcie->perst_gpio));
+			/* LTSSM disable */
+			exynos_elbi_write(exynos_pcie, PCIE_ELBI_LTSSM_DISABLE,
+					  PCIE_APP_LTSSM_ENABLE);
+			exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+
+			goto retry;
+		} else {
+			//exynos_pcie_host_v1_print_link_history(pp);
+			exynos_pcie_rc_print_link_history(pp);
+			exynos_pcie_rc_dump_link_down_status(exynos_pcie->ch_num);
+			exynos_pcie_rc_register_dump(exynos_pcie->ch_num);
+
+			exynos_pcie->link_stats.link_recovery_failure_count++;
+
+			if (exynos_pcie->ip_ver >= 0x889000 &&
+			    exynos_pcie->ep_device_type == EP_BCM_WIFI) {
+				return -EPIPE;
+			}
+
+			return -EPIPE;
+		}
+	} else {
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		link_stats_log_link_up(exynos_pcie, count);
+
+		save_pma_regs(exynos_pcie);
+
+		dev_info(dev, "(phy+0xC08=0x%x)(phy+0x1408=0x%x)(phy+0xC6C=0x%x)(phy+0x146C=0x%x)\n",
+			exynos_phy_read(exynos_pcie, 0xC08),
+			exynos_phy_read(exynos_pcie, 0x1408),
+			exynos_phy_read(exynos_pcie, 0xC6C),
+			exynos_phy_read(exynos_pcie, 0x146C));
+		/* need delay for link speed change from GEN1 to Max(ex GEN3) */
+		usleep_range(2800, 3000); /* 3 ms - OK */
+
+		exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_CTRL_STAT, 4, &val);
+		val = (val >> 16) & 0xf;
+		dev_info(dev, "Current Link Speed is GEN%d (MAX GEN%d)\n",
+			val, exynos_pcie->max_link_speed);
+
+		/* check link training result(speed) */
+		if (exynos_pcie->ip_ver >= 0x982000 && val < exynos_pcie->max_link_speed) {
+			try_cnt++;
+			dev_info(dev, "%s: Link is up. But not max speed, try count: %d\n",
+				__func__, try_cnt);
+			if (try_cnt < 10) {
+				gpio_set_value(exynos_pcie->perst_gpio, 0);
+				dev_info(dev, "Set PERST LOW, gpio val = %d\n",
+					gpio_get_value(exynos_pcie->perst_gpio));
+				/* LTSSM disable */
+				exynos_elbi_write(exynos_pcie, PCIE_ELBI_LTSSM_DISABLE,
+						  PCIE_APP_LTSSM_ENABLE);
+				exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+
+				goto retry;
+			} else {
+				dev_info(dev, "Current Link Speed is GEN%d (MAX GEN%d)\n",
+					val, exynos_pcie->max_link_speed);
+			}
+		}
+
+		/* check L0 state one more time after link recovery */
+		count = 0;
+		dev_info(dev, "check L0 state after link recovery\n");
+		while (count < MAX_TIMEOUT) {
+			val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+			      & PCIE_ELBI_LTSSM_STATE_MASK;
+			if (val >= S_L0 && val <= S_L1_IDLE)
+				break;
+			count++;
+			usleep_range(10, 12);
+		}
+
+		val = exynos_elbi_read(exynos_pcie, PCIE_IRQ0);
+		exynos_elbi_write(exynos_pcie, val, PCIE_IRQ0);
+		val = exynos_elbi_read(exynos_pcie, PCIE_IRQ1);
+		exynos_elbi_write(exynos_pcie, val, PCIE_IRQ1);
+		val = exynos_elbi_read(exynos_pcie, PCIE_IRQ2);
+		exynos_elbi_write(exynos_pcie, val, PCIE_IRQ2);
+
+		/* enable IRQ */
+		exynos_pcie_rc_enable_interrupts(pp, 1);
+
+		/* setup ATU for cfg/mem outbound */
+		busdev = EXYNOS_PCIE_ATU_BUS(1) | EXYNOS_PCIE_ATU_DEV(0) | EXYNOS_PCIE_ATU_FUNC(0);
+		exynos_pcie_rc_prog_viewport_cfg0(pp, busdev);
+		exynos_pcie_rc_prog_viewport_mem_outbound(pp);
+	}
+
+	return 0;
+}
+
+int exynos_pcie_rc_poweron(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci;
+	struct dw_pcie_rp *pp;
+	struct device *dev;
+	u32 val, vendor_id, device_id;
+	int ret;
+	unsigned long flags;
+
+	if (!exynos_pcie) {
+		pr_err("%s: ch#%d PCIe device is not loaded\n", __func__, ch_num);
+
+		return -ENODEV;
+	}
+
+	mutex_lock(&exynos_pcie->power_onoff_lock);
+
+	pci = exynos_pcie->pci;
+	pp = &pci->pp;
+	dev = pci->dev;
+
+	dev_info(dev, "start poweron, state: %d\n", exynos_pcie->state);
+	if (exynos_pcie->state == STATE_LINK_DOWN) {
+		if (exynos_pcie->use_phy_isol_con)
+			exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_BYPASS);
+
+		if (exynos_pcie->use_pcieon_sleep) {
+			dev_info(dev, "%s, pcie_is_linkup 1\n", __func__);
+			pcie_is_linkup = 1;
+		}
+		ret = exynos_pcie_rc_clock_enable(pp, PCIE_ENABLE_CLOCK);
+		dev_info(dev, "pcie clk enable, ret value = %d\n", ret);
+
+#if 0//IS_ENABLED(CONFIG_CPU_IDLE)
+		if (exynos_pcie->use_sicd) {
+			dev_info(dev, "ip idle status: %d, index: %d\n",
+				PCIE_IS_ACTIVE, exynos_pcie->idle_ip_index);
+			exynos_update_ip_idle_status(exynos_pcie->idle_ip_index, PCIE_IS_ACTIVE);
+		}
+#endif
+#if 0//IS_ENABLED(CONFIG_PM_DEVFREQ)
+		if (exynos_pcie->int_min_lock) {
+			exynos_pm_qos_update_request(&exynos_pcie_int_qos[ch_num],
+						     exynos_pcie->int_min_lock);
+			dev_info(dev, "%s: pcie int_min_lock = %d\n",
+				__func__, exynos_pcie->int_min_lock);
+		}
+#endif
+		/* Enable SysMMU */
+		if (exynos_pcie->use_sysmmu)
+			pcie_sysmmu_enable(pcie_ch_to_hsi(ch_num));
+
+		pinctrl_select_state(exynos_pcie->pcie_pinctrl,
+				     exynos_pcie->pin_state[PCIE_PIN_ACTIVE]);
+
+		/* phy all power down clear */
+		if (exynos_pcie->phy_ops.phy_all_pwrdn_clear)
+			exynos_pcie->phy_ops.phy_all_pwrdn_clear(exynos_pcie, exynos_pcie->ch_num);
+
+		/* make sure force_pclk_en disabled before link-up start */
+		/* this setting will be done during phy_config
+		 * writel(0x0C, exynos_pcie->phy_pcs_base + 0x0180);
+		 */
+
+		spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+		exynos_pcie->state = STATE_LINK_UP_TRY;
+		spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+
+		//enable_irq(pp->irq);
+
+		if (exynos_pcie_rc_establish_link(pp)) {
+			goto poweron_fail;
+		}
+
+		exynos_pcie->sudden_linkdown = 0;
+		exynos_pcie->cpl_timeout_recovery = 0;
+
+		val = exynos_elbi_read(exynos_pcie, PCIE_STATE_HISTORY_CHECK);
+		val &= ~(HISTORY_BUFFER_CONDITION_SEL);
+		exynos_elbi_write(exynos_pcie, val, PCIE_STATE_HISTORY_CHECK);
+
+		exynos_elbi_write(exynos_pcie, 0xffffffff, PCIE_STATE_POWER_S);
+		exynos_elbi_write(exynos_pcie, 0xffffffff, PCIE_STATE_POWER_M);
+
+		val = exynos_elbi_read(exynos_pcie, PCIE_STATE_HISTORY_CHECK);
+		val |= HISTORY_BUFFER_ENABLE;
+		exynos_elbi_write(exynos_pcie, val, PCIE_STATE_HISTORY_CHECK);
+
+		spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+		exynos_pcie->state = STATE_LINK_UP;
+		spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+
+		power_stats_update_up(exynos_pcie);
+
+		dev_info(dev, "[%s] exynos_pcie->probe_ok : %d\n", __func__, exynos_pcie->probe_ok);
+		if (!exynos_pcie->probe_ok) {
+			exynos_pcie_rc_rd_own_conf(pp, PCI_VENDOR_ID, 4, &val);
+			vendor_id = val & ID_MASK;
+			device_id = (val >> 16) & ID_MASK;
+
+			exynos_pcie->pci_dev = pci_get_device(vendor_id, device_id, NULL);
+			if (!exynos_pcie->pci_dev) {
+				goto poweron_fail;
+			}
+			dev_info(dev, "(%s):ep_pci_device:vendor/device id = 0x%x\n", __func__, val);
+			pci_rescan_bus(exynos_pcie->pci_dev->bus);
+
+			if (pci_save_state(exynos_pcie->pci_dev)) {
+				dev_err(dev, "Failed to save pcie state\n");
+
+				goto poweron_fail;
+			}
+			exynos_pcie->pci_saved_configs =
+				pci_store_saved_state(exynos_pcie->pci_dev);
+
+			exynos_pcie->ep_pci_dev = exynos_pcie_get_pci_dev(&pci->pp);
+
+#if IS_ENABLED(CONFIG_GS_S2MPU) || IS_ENABLED(CONFIG_EXYNOS_PCIE_IOMMU)
+			if (exynos_pcie->s2mpu || exynos_pcie->use_sysmmu) {
+				if (exynos_pcie->ep_device_type == EP_BCM_WIFI) {
+					set_dma_ops(&exynos_pcie->ep_pci_dev->dev, &pcie_dma_ops);
+					dev_info(dev, "Wifi DMA operations are changed\n");
+					memcpy(&fake_dma_dev, exynos_pcie->pci->dev,
+					       sizeof(fake_dma_dev));
+				}
+			}
+#endif
+
+			exynos_pcie->probe_ok = 1;
+		} else if (exynos_pcie->probe_ok) {
+			if (pci_load_saved_state(exynos_pcie->pci_dev,
+						 exynos_pcie->pci_saved_configs)) {
+				goto poweron_fail;
+			}
+			pci_restore_state(exynos_pcie->pci_dev);
+		}
+	}
+
+	dev_info(dev, "end poweron, state: %d\n", exynos_pcie->state);
+	mutex_unlock(&exynos_pcie->power_onoff_lock);
+
+	return 0;
+
+poweron_fail:
+	exynos_pcie->state = STATE_LINK_UP;
+	mutex_unlock(&exynos_pcie->power_onoff_lock);
+	exynos_pcie_rc_poweroff(exynos_pcie->ch_num);
+
+	return -EPIPE;
+}
+
+void exynos_pcie_rc_poweroff(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci;
+	struct dw_pcie_rp *pp;
+	struct device *dev;
+	unsigned long flags, flags1;
+	u32 val;
+
+	if (!exynos_pcie) {
+		pr_err("%s: ch#%d PCIe device is not loaded\n", __func__, ch_num);
+		return;
+	}
+
+	mutex_lock(&exynos_pcie->power_onoff_lock);
+
+	pci = exynos_pcie->pci;
+	pp = &pci->pp;
+	dev = pci->dev;
+
+	dev_info(dev, "start poweroff, state: %d\n", exynos_pcie->state);
+
+	if (exynos_pcie->state == STATE_LINK_UP ||
+	    exynos_pcie->state == STATE_LINK_DOWN_TRY) {
+		spin_lock_irqsave(&exynos_pcie->reg_lock, flags1);
+		exynos_pcie->state = STATE_LINK_DOWN_TRY;
+		spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags1);
+
+		disable_irq(pp->irq);
+
+		/* disable LINKDOWN irq */
+		if (exynos_pcie->ip_ver == 0x982000) {
+			/* only support EVT1.1 */
+			val = exynos_elbi_read(exynos_pcie, PCIE_IRQ1_EN);
+			val &= ~IRQ_LINKDOWN_ENABLE_EVT1_1;
+			exynos_elbi_write(exynos_pcie, val, PCIE_IRQ1_EN);
+		} else {
+			val = exynos_elbi_read(exynos_pcie, PCIE_IRQ1_EN);
+			val &= ~IRQ_LINK_DOWN_ENABLE;
+			exynos_elbi_write(exynos_pcie, val, PCIE_IRQ1_EN);
+		}
+
+		spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+		exynos_pcie->state = STATE_LINK_DOWN;
+		exynos_pcie_rc_send_pme_turn_off(exynos_pcie);
+		power_stats_update_down(exynos_pcie);
+
+		/* Disable SysMMU */
+		if (exynos_pcie->use_sysmmu)
+			pcie_sysmmu_disable(pcie_ch_to_hsi(ch_num));
+
+		/* Disable history buffer */
+		val = exynos_elbi_read(exynos_pcie, PCIE_STATE_HISTORY_CHECK);
+		val &= ~HISTORY_BUFFER_ENABLE;
+		exynos_elbi_write(exynos_pcie, val, PCIE_STATE_HISTORY_CHECK);
+
+		gpio_set_value(exynos_pcie->perst_gpio, 0);
+		dev_info(dev, "%s: Set PERST to LOW, gpio val = %d\n",
+			__func__, gpio_get_value(exynos_pcie->perst_gpio));
+
+		/* LTSSM disable */
+		exynos_elbi_write(exynos_pcie, PCIE_ELBI_LTSSM_DISABLE, PCIE_APP_LTSSM_ENABLE);
+
+		/* force SOFT_PWR_RESET */
+		val = exynos_elbi_read(exynos_pcie, PCIE_SOFT_RESET);
+		val &= ~SOFT_PWR_RESET;
+		exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+		udelay(20);
+		val |= SOFT_PWR_RESET;
+		exynos_elbi_write(exynos_pcie, val, PCIE_SOFT_RESET);
+
+		/* phy all power down */
+		if (exynos_pcie->phy_ops.phy_all_pwrdn)
+			exynos_pcie->phy_ops.phy_all_pwrdn(exynos_pcie, exynos_pcie->ch_num);
+
+		spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+
+		exynos_pcie_rc_phy_clock_enable(pp, PCIE_DISABLE_CLOCK);
+		exynos_pcie_rc_clock_enable(pp, PCIE_DISABLE_CLOCK);
+		exynos_pcie->atu_ok = 0;
+
+		if (!IS_ERR(exynos_pcie->pin_state[PCIE_PIN_IDLE]))
+			pinctrl_select_state(exynos_pcie->pcie_pinctrl,
+					     exynos_pcie->pin_state[PCIE_PIN_IDLE]);
+
+#if 0//IS_ENABLED(CONFIG_PM_DEVFREQ)
+		if (exynos_pcie->int_min_lock) {
+			exynos_pm_qos_update_request(&exynos_pcie_int_qos[ch_num], 0);
+			dev_info(dev, "%s: pcie int_min_lock = %d\n",
+				__func__, exynos_pcie->int_min_lock);
+		}
+#endif
+#if 0//IS_ENABLED(CONFIG_CPU_IDLE)
+		if (exynos_pcie->use_sicd) {
+			dev_info(dev, "%s, ip idle status: %d, idle_ip_index: %d\n",
+				__func__, PCIE_IS_IDLE, exynos_pcie->idle_ip_index);
+			exynos_update_ip_idle_status(exynos_pcie->idle_ip_index, PCIE_IS_IDLE);
+		}
+#endif
+		if (exynos_pcie->use_phy_isol_con)
+			exynos_pcie_phy_isolation(exynos_pcie,
+						  PCIE_PHY_ISOLATION);
+	}
+
+	if (exynos_pcie->use_pcieon_sleep) {
+		dev_info(dev, "%s, pcie_is_linkup 0\n", __func__);
+		pcie_is_linkup = 0;
+	}
+
+	dev_info(dev, "end poweroff, state: %d\n", exynos_pcie->state);
+
+	mutex_unlock(&exynos_pcie->power_onoff_lock);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_poweroff);
+
+void exynos_pcie_pm_suspend(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	unsigned long flags;
+
+	if (exynos_pcie->state == STATE_LINK_DOWN)
+		return;
+
+	spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+	exynos_pcie->state = STATE_LINK_DOWN_TRY;
+	spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+
+	exynos_pcie_rc_poweroff(ch_num);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_pm_suspend);
+
+int exynos_pcie_pm_resume(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	return exynos_pcie_rc_poweron(ch_num);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_pm_resume);
+
+bool exynos_pcie_rc_get_sudden_linkdown_state(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	return exynos_pcie->sudden_linkdown;
+}
+EXPORT_SYMBOL(exynos_pcie_rc_get_sudden_linkdown_state);
+
+void exynos_pcie_rc_set_sudden_linkdown_state(int ch_num, bool recovery)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	pr_err("[%s] set sudden_linkdown_state to recovery_on\n", __func__);
+	exynos_pcie->sudden_linkdown = recovery;
+}
+EXPORT_SYMBOL(exynos_pcie_rc_set_sudden_linkdown_state);
+
+bool exynos_pcie_rc_get_cpl_timeout_state(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	return exynos_pcie->cpl_timeout_recovery;
+}
+EXPORT_SYMBOL(exynos_pcie_rc_get_cpl_timeout_state);
+
+void exynos_pcie_rc_set_cpl_timeout_state(int ch_num, bool recovery)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+
+	pr_err("set cpl_timeout_recovery to %d for ch_num:%d\n", recovery, ch_num);
+	exynos_pcie->cpl_timeout_recovery = recovery;
+}
+EXPORT_SYMBOL(exynos_pcie_rc_set_cpl_timeout_state);
+
+/* get EP pci_dev structure of BUS */
+static struct pci_dev *exynos_pcie_get_pci_dev(struct dw_pcie_rp *pp)
+{
+	int domain_num;
+	struct pci_bus *ep_pci_bus;
+	struct pci_dev *ep_pci_dev;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	u32 val;
+
+	if (exynos_pcie->ep_pci_dev)
+		return exynos_pcie->ep_pci_dev;
+
+	/* Get EP vendor/device ID to get pci_dev structure */
+	domain_num = exynos_pcie->pci_dev->bus->domain_nr;
+	ep_pci_bus = pci_find_bus(domain_num, 1);
+
+	exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, PCI_VENDOR_ID, 4, &val);
+
+	ep_pci_dev = pci_get_device(val & ID_MASK, (val >> 16) & ID_MASK, NULL);
+
+	return ep_pci_dev;
+}
+
+int exynos_pcie_l1_exit(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	u32 count = 0, ret = 0, val = 0;
+	unsigned long flags;
+
+	if (exynos_pcie->ep_device_type != EP_BCM_WIFI) {
+		pr_err("%s: EP is not EP_BCM_WIFI (not support l1_exit)\n", __func__);
+
+		return -EPIPE;
+	}
+
+	spin_lock_irqsave(&exynos_pcie->pcie_l1_exit_lock, flags);
+
+	if (exynos_pcie->l1ss_ctrl_id_state == 0) {
+		/* Set s/w L1 exit mode */
+		/* 1. Set app_req_exit_l1 Signal */
+		exynos_elbi_write(exynos_pcie, 0x1, PCIE_APP_REQ_EXIT_L1);
+		/* 2. exit_l1_control_mode[0:0] - 0x0: SW mode (0x1: HW mode) */
+		val = exynos_elbi_read(exynos_pcie, PCIE_APP_REQ_EXIT_L1_MODE);
+		val &= ~APP_REQ_EXIT_L1_MODE;
+		exynos_elbi_write(exynos_pcie, val, PCIE_APP_REQ_EXIT_L1_MODE);
+
+		/* Max timeout = 3ms (300 * 10us) */
+		while (count < MAX_L1_EXIT_TIMEOUT) {
+			val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+			      & PCIE_ELBI_LTSSM_STATE_MASK;
+			if (val == S_L0)
+				break;
+			count++;
+			udelay(10);
+		}
+
+		if (count >= MAX_L1_EXIT_TIMEOUT) {
+			pr_err("%s: cannot change to L0(LTSSM = 0x%x, cnt = %d)\n",
+			       __func__, val, count);
+			ret = -EPIPE;
+		}
+
+		/* Set h/w L1 exit mode */
+		/* 1. exit_l1_control_mode[0:0] - 0x1: HW mode (0x0: SW mode) */
+		val = exynos_elbi_read(exynos_pcie, PCIE_APP_REQ_EXIT_L1_MODE);
+		val |= APP_REQ_EXIT_L1_MODE;
+		exynos_elbi_write(exynos_pcie, val, PCIE_APP_REQ_EXIT_L1_MODE);
+		/* 2. Reset app_req_exit_l1 Signal */
+		exynos_elbi_write(exynos_pcie, 0x0, PCIE_APP_REQ_EXIT_L1);
+	}
+
+	spin_unlock_irqrestore(&exynos_pcie->pcie_l1_exit_lock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_l1_exit);
+
+static int exynos_pcie_rc_set_l1ss(int enable, struct dw_pcie_rp *pp, int id)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	u32 val;
+	unsigned long flags;
+	int domain_num;
+	struct pci_bus *ep_pci_bus;
+	u32 exp_cap_off = PCIE_CAP_OFFSET;
+
+	/* This function is only working with the devices which support L1SS */
+	if (exynos_pcie->ep_device_type != EP_SAMSUNG_MODEM &&
+	    exynos_pcie->ep_device_type != EP_BCM_WIFI &&
+	    exynos_pcie->ep_device_type != EP_QC_WIFI) {
+		dev_err(dev, "Can't set L1SS!!! (EP: L1SS not supported)\n");
+
+		return -EINVAL;
+	}
+
+	dev_info(dev, "%s:L1SS_START: l1ss_ctrl_id_state = 0x%x\n",
+		__func__, exynos_pcie->l1ss_ctrl_id_state);
+	dev_info(dev, "%s:\tid = 0x%x, enable=%d, exynos_pcie=%pK\n",
+		__func__, id, enable, exynos_pcie);
+
+	if (exynos_pcie->state != STATE_LINK_UP || exynos_pcie->atu_ok == 0) {
+		spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+
+		if (enable)
+			exynos_pcie->l1ss_ctrl_id_state &= ~(id);
+		else
+			exynos_pcie->l1ss_ctrl_id_state |= id;
+
+		spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+
+		dev_info(dev, "%s:state = 0x%x, id = 0x%x: not needed(This will be set later)\n",
+			__func__, exynos_pcie->l1ss_ctrl_id_state, id);
+
+		return -1;
+	} else {
+		exynos_pcie->ep_l1ss_cap_off =
+			pci_find_ext_capability(exynos_pcie->ep_pci_dev, PCI_EXT_CAP_ID_L1SS);
+		exynos_pcie->ep_link_ctrl_off = exynos_pcie->ep_pci_dev->pcie_cap + PCI_EXP_LNKCTL;
+		exynos_pcie->ep_l1ss_ctrl1_off = exynos_pcie->ep_l1ss_cap_off + PCI_L1SS_CTL1;
+		exynos_pcie->ep_l1ss_ctrl2_off = exynos_pcie->ep_l1ss_cap_off + PCI_L1SS_CTL2;
+	}
+
+	/* get the domain_num & ep_pci_bus of EP device */
+	domain_num = exynos_pcie->pci_dev->bus->domain_nr;
+	ep_pci_bus = pci_find_bus(domain_num, 1);
+	dev_info(dev, "%s:[DBG] domain_num = %d, ep_pci_bus = %pK\n",
+		__func__, domain_num, ep_pci_bus);
+
+	spin_lock_irqsave(&exynos_pcie->conf_lock, flags);
+	if (enable) {	/* enable == 1 */
+		exynos_pcie->l1ss_ctrl_id_state &= ~(id);
+
+		if (exynos_pcie->l1ss_ctrl_id_state == 0) {
+			/* [RC & EP] enable L1SS & ASPM */
+			if (exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+				dev_info(dev, "%s: #1 enalbe CP L1.2\n", __func__);
+
+				/* 1) [RC] enable L1SS */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				/* Actual TCOMMON is 42usec (val = 0x2a << 8) */
+				val |= PORT_LINK_TCOMMON_32US
+					| PORT_LINK_L1SS_ENABLE;
+				dev_info(dev, "CPen:1RC:L1SS_CTRL(0x19C) = 0x%x\n", val);
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+
+				/* [RC] set TPOWERON */
+				/* Set TPOWERON value for RC: 90->200 usec */
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL2, 4,
+							   PORT_LINK_TPOWERON_200US);
+
+				/* exynos_pcie_rc_wr_own_conf(pp,
+				 *	PCIE_L1_SUBSTATES_OFF, 4,
+				 *	PCIE_L1_SUB_VAL);
+				 */
+
+				/* [RC] set LTR_EN */
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_DEVCTL2,
+							   4, PCI_EXP_DEVCTL2_LTR_EN);
+
+				/* [EP] set LTR_EN (reg_addr = 0x98) */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exp_cap_off + PCI_EXP_DEVCTL2, 4,
+							     &val);
+				val |= PCI_EXP_DEVCTL2_LTR_EN;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exp_cap_off + PCI_EXP_DEVCTL2, 4, val);
+
+				/* [EP] set TPOWERON */
+				/* Set TPOWERON value for EP: 90->200 usec */
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_l1ss_ctrl2_off, 4,
+							     PORT_LINK_TPOWERON_200US);
+
+				/* [EP] set Entrance latency */
+				/* Set L1.2 Enterance Latency for EP: 64 usec */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     PCIE_ACK_F_ASPM_CONTROL, 4, &val);
+				val &= ~PCIE_L1_ENTERANCE_LATENCY;
+				val |= PCIE_L1_ENTERANCE_LATENCY_64us;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     PCIE_ACK_F_ASPM_CONTROL, 4, val);
+
+				/* 2) [EP] enable L1SS */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_l1ss_ctrl1_off, 4, &val);
+				val |= PORT_LINK_L1SS_ENABLE;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_l1ss_ctrl1_off, 4, val);
+				dev_info(dev, "CPen:2EP:L1SS_CTRL(0x19C)=0x%x\n", val);
+
+				/* 3) [RC] enable ASPM */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off +
+							   PCI_EXP_LNKCTL, 4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				val |= PCI_EXP_LNKCTL_CCC | PCI_EXP_LNKCTL_ASPM_L1;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off +
+							   PCI_EXP_LNKCTL, 4, val);
+				dev_info(dev, "CPen:3RC:ASPM(0x70+16)=0x%x\n", val);
+
+				/* 4) [EP] enable ASPM */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_link_ctrl_off, 4, &val);
+				val |= PCI_EXP_LNKCTL_CCC | PCI_EXP_LNKCTL_CLKREQ_EN |
+					PCI_EXP_LNKCTL_ASPM_L1;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_link_ctrl_off, 4, val);
+				dev_info(dev, "CPen:4EP:ASPM(0x80)=0x%x\n", val);
+			} else if (exynos_pcie->ep_device_type == EP_BCM_WIFI) {
+				dev_info(dev, "%s: #2 enable WIFI L1.2\n", __func__);
+
+				/* enable sequence:
+				 * 1. PCIPM RC
+				 * 2. PCIPM EP
+				 * 3. ASPM RC
+				 * 4. ASPM EP
+				 */
+
+				/* 1. to enable PCIPM RC */
+				/* [RC:set value] TPowerOn(130 usec) */
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL2, 4,
+							   PORT_LINK_TPOWERON_130US);
+
+				/* [RC:set value] TPowerOff(3 us), T_L1.2(10 us), T_PCLKACK(2 us) */
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_L1_SUBSTATES_OFF, 4,
+							   PCIE_L1_SUB_VAL);
+
+				/* [RC:set enable bit] LTR Mechanism Enable */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_DEVCTL2,
+							   4, &val);
+				val |= PCI_EXP_DEVCTL2_LTR_EN;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_DEVCTL2,
+							   4, val);
+
+				/* [RC:set value] LTR_L1.2_Threshold(160 us) and TCommon(42 us)
+				 *	Actual TCommon is 42usec (val = (0xa | 0x20) << 8)
+				 * [RC:enable] L1SS_ENABLE(0xf)
+				 */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				val |= PORT_LINK_L12_LTR_THRESHOLD | PORT_LINK_TCOMMON_32US |
+				       PORT_LINK_L1SS_ENABLE;
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+				dev_info(dev, "WIFIen:1RC:L1SS_CTRL(0x19C)=0x%x\n", val);
+
+				/* 2. to enable PCIPM EP */
+				/* [EP:set value] TPowerOn(130 usec) */
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_CONTROL2,
+							     4, PORT_LINK_TPOWERON_130US);
+
+				/* [EP:set value] LTR Latency(max snoop(3 ms)/no-snoop(3 ms))
+				 * Reported_LTR value in LTR message
+				 */
+				val = MAX_NO_SNOOP_LAT_SCALE_MS | MAX_NO_SNOOP_LAT_VALUE_3 |
+					      MAX_SNOOP_LAT_SCALE_MS | MAX_SNOOP_LAT_VALUE_3;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     WIFI_L1SS_LTR_LATENCY, 4, val);
+
+				/* [EP:set enable bit] LTR Mechanism Enable */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     WIFI_PCI_EXP_DEVCTL2, 4, &val);
+				val |= PCI_EXP_DEVCTL2_LTR_EN;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     WIFI_PCI_EXP_DEVCTL2, 4, val);
+
+				/* [EP:set values] LTR_L1.2_Threshold(160 us) and TCommon(10 us)
+				 * [EP:enable] WIFI_PM_ENALKBE(0xf)
+				 */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_CONTROL,
+							     4, &val);
+				val |= PORT_LINK_L12_LTR_THRESHOLD | WIFI_COMMON_RESTORE_TIME |
+				       WIFI_ALL_PM_ENABEL;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_CONTROL,
+							     4, val);
+				dev_info(dev, "WIFIen:2EP:L1SS_CTRL(0x248)=0x%x\n", val);
+
+				/* 3. to enable ASPM RC */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				/* PCI_EXP_LNKCTL_CCC: Common Clock Configuration */
+				val |= PCI_EXP_LNKCTL_CCC | PCI_EXP_LNKCTL_ASPM_L1;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, val);
+				dev_info(dev, "WIFIen:3RC:ASPM(0x70+16)=0x%x\n", val);
+
+				/* 4. to enable ASPM EP */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_LINKCTRL,
+							     4, &val);
+				val &= ~(WIFI_ASPM_CONTROL_MASK);
+				val |= WIFI_CLK_REQ_EN | WIFI_USE_SAME_REF_CLK |
+				       WIFI_ASPM_L1_ENTRY_EN;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_LINKCTRL,
+							     4, val);
+				dev_info(dev, "WIFIen:4EP:ASPM(0xBC)=0x%x\n", val);
+			} else if (exynos_pcie->ep_device_type == EP_QC_WIFI) {
+				dev_info(dev, "%s: #2 enable WIFI L1.2\n", __func__);
+
+				/* enable sequence:
+				 * 1. PCIPM RC
+				 * 2. PCIPM EP
+				 * 3. ASPM RC
+				 * 4. ASPM EP
+				 */
+
+				/* 1. to enable PCIPM RC */
+				/* [RC:set value] TPowerOn(10 usec) */
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL2, 4,
+							   PORT_LINK_TPOWERON_10US);
+
+				/* [RC:set enable bit] LTR Mechanism Enable */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_DEVCTL2,
+							   4, &val);
+				val |= PCI_EXP_DEVCTL2_LTR_EN;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_DEVCTL2,
+							   4, val);
+
+				/* [RC:set value] LTR_L1.2_Threshold(150(0x96) us)
+				 * and TCommon is 42usec (val = (0xa | 0x20) << 8)
+				 * [RC:enable] L1SS_ENABLE(0xf)
+				 */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				val |= WIFI_QC_L12_LTR_THRESHOLD | PORT_LINK_TCOMMON_32US |
+				       PORT_LINK_L1SS_ENABLE;
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+				dev_info(dev, "%s: WIFIen:1RC:L1SS_CTRL(0x19C)=0x%x\n",
+						__func__, val);
+
+				/* 2. to enable PCIPM EP */
+				/* [EP:set value] TPowerOn(10 usec) */
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+						exynos_pcie->ep_l1ss_ctrl2_off,
+						4, PORT_LINK_TPOWERON_10US);
+				dev_info(dev, "%s: WIFIen:2EP:L1SS_CTRL2(0x%x)=0x%x\n",
+						__func__, exynos_pcie->ep_l1ss_ctrl2_off,
+						PORT_LINK_TPOWERON_10US);
+
+				/* [EP:set enable bit] LTR Mechanism Enable */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exp_cap_off+PCI_EXP_DEVCTL2, 4, &val);
+				val |= PCI_EXP_DEVCTL2_LTR_EN;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exp_cap_off+PCI_EXP_DEVCTL2, 4, val);
+				dev_info(dev, "%s: WIFIen:2EP:EXP_DEVCTL2(0x%x)=0x%x\n",
+						__func__, exp_cap_off+PCI_EXP_DEVCTL2, val);
+
+				/* [EP:set values] LTR_L1.2_Threshold(150 us) and TCommon(0 us)
+				 * [EP:enable] WIFI_PM_ENALKBE(0xf)
+				 */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+						exynos_pcie->ep_l1ss_ctrl1_off, 4, &val);
+				val |= WIFI_QC_L12_LTR_THRESHOLD | PORT_LINK_L1SS_ENABLE;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+						exynos_pcie->ep_l1ss_ctrl1_off, 4, val);
+				dev_info(dev, "%s: WIFIen:2EP:L1SS_CTRL(0x%x)=0x%x\n",
+						__func__, exynos_pcie->ep_l1ss_ctrl1_off, val);
+
+				/* 3. to enable ASPM RC */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				/* PCI_EXP_LNKCTL_CCC: Common Clock Configuration */
+				val |= PCI_EXP_LNKCTL_CCC | PCI_EXP_LNKCTL_ASPM_L1;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, val);
+				dev_info(dev, "%s: WIFIen:3RC:ASPM(0x70+16)=0x%x\n", __func__, val);
+
+				/* 4. to enable ASPM EP */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+						exp_cap_off+PCI_EXP_LNKCTL, 4, &val);
+				val |= PCI_EXP_LNKCTL_CCC | PCI_EXP_LNKCTL_CLKREQ_EN |
+					PCI_EXP_LNKCTL_ASPM_L1;
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+						exp_cap_off+PCI_EXP_LNKCTL, 4, val);
+				dev_info(dev, "%s: WIFIen:4EP:ASPM(0x%x)=0x%x\n",
+						__func__,exp_cap_off+PCI_EXP_LNKCTL, val);
+			} else {
+				dev_err(dev, "[ERR] EP: L1SS not supported\n");
+			}
+		}
+	} else {	/* enable == 0 */
+		if (exynos_pcie->l1ss_ctrl_id_state) {
+			exynos_pcie->l1ss_ctrl_id_state |= id;
+		} else {
+			exynos_pcie->l1ss_ctrl_id_state |= id;
+
+			if (exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+				dev_info(dev, "%s: #3 disable CP L1.2\n", __func__);
+
+				/* 1) [EP] disable ASPM */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_link_ctrl_off, 4, &val);
+				val &= ~(PCI_EXP_LNKCTL_ASPMC);
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_link_ctrl_off, 4, val);
+				dev_info(dev, "CPdis:1EP:ASPM(0x80)=0x%x\n", val);
+
+				/* 2) [RC] disable ASPM */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off +
+							   PCI_EXP_LNKCTL, 4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off +
+							   PCI_EXP_LNKCTL, 4, val);
+				dev_info(dev, "CPdis:2RC:ASPM(0x70+16)=0x%x\n", val);
+
+				/* 3) [EP] disable L1SS */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_l1ss_ctrl1_off, 4, &val);
+				val &= ~(PORT_LINK_L1SS_ENABLE);
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+							     exynos_pcie->ep_l1ss_ctrl1_off, 4, val);
+				dev_info(dev, "CPdis:3EP:L1SS_CTRL(0x19C)=0x%x\n", val);
+
+				/* 4) [RC] disable L1SS */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				val &= ~(PORT_LINK_L1SS_ENABLE);
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+				dev_info(dev, "CPdis:4RC:L1SS_CTRL(0x19C)=0x%x\n", val);
+			} else if (exynos_pcie->ep_device_type == EP_BCM_WIFI) {
+				dev_info(dev, "%s: #4 disable WIFI L1.2\n", __func__);
+
+				/* disable sequence:
+				 * 1. ASPM EP
+				 * 2. ASPM RC
+				 * 3. PCIPM EP
+				 * 4. PCIPM RC
+				 */
+				/* 1) [EP] disable ASPM */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_LINKCTRL,
+							     4, &val);
+				val &= ~(WIFI_ASPM_CONTROL_MASK);
+				/* val |= WIFI_CLK_REQ_EN | WIFI_USE_SAME_REF_CLK; */
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_LINKCTRL,
+							     4, val);
+				dev_info(pci->dev, "WIFIdis:1EP:ASPM(0xBC)=0x%x\n", val);
+
+				/* 2) [RC] disable ASPM */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, val);
+				dev_info(pci->dev, "WIFIdis:2RC:ASPM(0x70+16)=0x%x\n", val);
+
+				/* 3) [EP] disable L1SS */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_CONTROL,
+							     4, &val);
+				val &= ~(WIFI_ALL_PM_ENABEL);
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, WIFI_L1SS_CONTROL,
+							     4, val);
+				dev_info(pci->dev, "WIFIdis:3EP:L1SS_CTRL(0x248)=0x%x\n", val);
+
+				/* 4) [RC] disable L1SS */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				val &= ~(PORT_LINK_L1SS_ENABLE);
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+				dev_info(pci->dev, "WIFIdis:4RC:L1SS_CTRL(0x19C)=0x%x\n", val);
+			} else if (exynos_pcie->ep_device_type == EP_QC_WIFI) {
+				dev_info(dev, "%s: #4 disable WIFI L1.2\n", __func__);
+
+				/* disable sequence:
+				 * 1. ASPM EP
+				 * 2. ASPM RC
+				 * 3. PCIPM EP
+				 * 4. PCIPM RC
+				 */
+				/* 1) [EP] disable ASPM */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+						exp_cap_off+PCI_EXP_LNKCTL, 4, &val);
+				val &= ~(PCI_EXP_LNKCTL_ASPMC);
+				/* val |= WIFI_CLK_REQ_EN | WIFI_USE_SAME_REF_CLK; */
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+						exp_cap_off+PCI_EXP_LNKCTL, 4, val);
+				dev_info(pci->dev, "%s: WIFIdis:1EP:ASPM(0x%x)=0x%x\n",
+						__func__,exp_cap_off+PCI_EXP_LNKCTL,val);
+
+				/* 2) [RC] disable ASPM */
+				exynos_pcie_rc_rd_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, &val);
+				val &= ~PCI_EXP_LNKCTL_ASPMC;
+				exynos_pcie_rc_wr_own_conf(pp, exp_cap_off + PCI_EXP_LNKCTL,
+							   4, val);
+				dev_info(pci->dev, "%s: WIFIdis:2RC:ASPM(0x70+16)=0x%x\n",
+						__func__, val);
+
+				/* 3) [EP] disable L1SS */
+				exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0,
+						exynos_pcie->ep_l1ss_ctrl1_off, 4, &val);
+				val &= ~(PORT_LINK_L1SS_ENABLE);
+				exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0,
+						exynos_pcie->ep_l1ss_ctrl1_off, 4, val);
+				dev_info(pci->dev, "%s: WIFIdis:3EP:L1SS_CTRL(0x%x)=0x%x\n",
+						__func__, exynos_pcie->ep_l1ss_ctrl1_off,val);
+
+				/* 4) [RC] disable L1SS */
+				exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, &val);
+				val &= ~(PORT_LINK_L1SS_ENABLE);
+				exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL, 4, val);
+				dev_info(pci->dev, "%s: WIFIdis:4RC:L1SS_CTRL(0x19C)=0x%x\n",
+						__func__, val);
+			} else {
+				dev_err(dev, "[ERR] EP: L1SS not supported\n");
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&exynos_pcie->conf_lock, flags);
+
+	dev_info(dev, "%s:L1SS_END(l1ss_ctrl_id_state=0x%x, id=0x%x, enable=%d)\n",
+		__func__, exynos_pcie->l1ss_ctrl_id_state, id, enable);
+
+	return 0;
+}
+
+int exynos_pcie_rc_l1ss_ctrl(int enable, int id, int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+
+	if (!exynos_pcie->use_l1ss) {
+		pr_err("%s: 'use_l1ss' is false in DT(not support L1.2)\n", __func__);
+
+		return -EINVAL;
+	}
+
+	if (pp)
+		return	exynos_pcie_rc_set_l1ss(enable, pp, id);
+	else
+		return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_l1ss_ctrl);
+
+/* to support CP driver */
+int exynos_pcie_poweron(int ch_num, int spd)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+
+	dev_info(pci->dev, "%s requested with link speed GEN%d\n", __func__, spd);
+	exynos_pcie->max_link_speed = spd;
+
+	return exynos_pcie_rc_poweron(ch_num);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_poweron);
+
+void exynos_pcie_poweroff(int ch_num)
+{
+	return exynos_pcie_rc_poweroff(ch_num);
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_poweroff);
+
+/* PCIe link status check function */
+int exynos_pcie_rc_chk_link_status(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci;
+	struct device *dev;
+	unsigned long flags;
+
+	u32 val;
+	int link_status;
+
+	if (!exynos_pcie) {
+		pr_err("%s: ch#%d PCIe device is not loaded\n", __func__, ch_num);
+
+		return -ENODEV;
+	}
+	pci = exynos_pcie->pci;
+	dev = pci->dev;
+
+	if (exynos_pcie->state == STATE_LINK_DOWN)
+		return 0;
+
+	if (exynos_pcie->cpl_timeout_recovery) {
+		spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+		exynos_pcie->state = STATE_LINK_DOWN;
+		spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+		return 0;
+	}
+
+	if (exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		if (val >= S_RCVRY_LOCK && val <= S_L1_IDLE) {
+			link_status = 1;
+		} else {
+			dev_err(dev, "Check unexpected state - H/W:0x%x, S/W:%d\n",
+				val, exynos_pcie->state);
+
+			exynos_pcie_rc_print_link_history(&pci->pp);
+
+			spin_lock_irqsave(&exynos_pcie->reg_lock, flags);
+			exynos_pcie->state = STATE_LINK_DOWN;
+			spin_unlock_irqrestore(&exynos_pcie->reg_lock, flags);
+			link_status = 0;
+		}
+
+		return link_status;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_chk_link_status);
+
+/* LINK SPEED: check and change */
+int exynos_pcie_rc_check_link_speed(int ch_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	u32 current_speed = 0;
+
+	if (exynos_pcie->state != STATE_LINK_UP) {
+		dev_err(pci->dev, "Link is not up\n");
+
+		return -EINVAL;
+	}
+
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_CTRL_STAT, 4, &current_speed);
+	current_speed = current_speed >> 16;
+	current_speed &= PCIE_LINK_CTRL_LINK_SPEED_MASK;
+	dev_info(pci->dev, "(%s) Current link speed(0x80): GEN%d\n", __func__, (int)current_speed);
+
+	return current_speed;
+}
+
+int exynos_pcie_rc_change_link_speed(int ch_num, int target_speed)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	struct pci_bus *ep_pci_bus;
+	int i;
+	u32 val, current_speed, new_speed;
+
+	if (exynos_pcie->state != STATE_LINK_UP) {
+		dev_err(pci->dev, "Link is not up\n");
+		return -EINVAL;
+	}
+
+	if (target_speed > 3 || target_speed < 1) {
+		dev_err(pci->dev, "Invalid target speed: Unable to change\n");
+
+		return -EINVAL;
+	}
+
+	current_speed = exynos_pcie_rc_check_link_speed(ch_num);
+	if (current_speed == target_speed) {
+		dev_err(pci->dev, "Already GEN%d(current), target: GEN%d\n",
+			current_speed, target_speed);
+
+		return -EINVAL;
+	}
+
+	/* make sure that the link state is L0 by accessing ep config register
+	 * such as 'PCI_VENDOR_ID'.
+	 */
+	ep_pci_bus = pci_find_bus(exynos_pcie->pci_dev->bus->domain_nr, 1);
+	exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, PCI_VENDOR_ID, 4, &val);
+
+	/* modify registers to change link speed:
+	 * 1. PCIe_LINK_CTRL2_STAT2(offset: 0xA0)
+	 */
+	exynos_pcie_rc_rd_other_conf(pp, ep_pci_bus, 0, PCIE_LINK_CTRL2_STAT2, 4, &val);
+	val = val & PCIE_LINK_CTRL2_TARGET_LINK_SPEED_MASK;
+	val = val | 0x3;
+	exynos_pcie_rc_wr_other_conf(pp, ep_pci_bus, 0, PCIE_LINK_CTRL2_STAT2, 4, val);
+
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_CTRL2_STAT2, 4, &val);
+	val = val & PCIE_LINK_CTRL2_TARGET_LINK_SPEED_MASK;
+	val = val | target_speed;
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_CTRL2_STAT2, 4, val);
+
+	/* modify registers to change link speed:
+	 * 2. PCIE_LINK_WIDTH_SPEED_CONTROL(offset: 0x80C)
+	 */
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_WIDTH_SPEED_CONTROL, 4, &val);
+	val = val & DIRECT_SPEED_CHANGE_ENABLE_MASK;
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_WIDTH_SPEED_CONTROL, 4, val);
+
+	exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_WIDTH_SPEED_CONTROL, 4, &val);
+	val = val | DIRECT_SPEED_CHANGE_ENABLE;
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_WIDTH_SPEED_CONTROL, 4, val);
+
+	for (i = 0; i < MAX_TIMEOUT_SPEEDCHANGE; i++) {
+		val = exynos_elbi_read(exynos_pcie, PCIE_ELBI_RDLH_LINKUP)
+		      & PCIE_ELBI_LTSSM_STATE_MASK;
+		if (val >= S_L0 && val <= S_L1_IDLE)
+			break;
+		usleep_range(80, 100);
+	}
+
+	for (i = 0; i < MAX_TIMEOUT_SPEEDCHANGE; i++) {
+		exynos_pcie_rc_rd_own_conf(pp, PCIE_LINK_CTRL_STAT, 4, &new_speed);
+		new_speed = new_speed >> 16;
+		new_speed &= PCIE_LINK_CTRL_LINK_SPEED_MASK;
+		if (new_speed == target_speed)
+			break;
+		usleep_range(80, 100);
+	}
+
+	if (new_speed != target_speed) {
+		dev_err(pci->dev, "Fail: Unable to change to GEN%d\n", target_speed);
+
+		return -EINVAL;
+	}
+
+	dev_info(pci->dev, "Link Speed Changed: from GEN%d to GEN%d\n", current_speed, new_speed);
+
+	return 0;
+}
+
+int exynos_pcie_register_event(struct exynos_pcie_register_event *reg)
+{
+	int ret = 0;
+	struct dw_pcie_rp *pp;
+	struct exynos_pcie *exynos_pcie;
+	struct dw_pcie *pci;
+
+	if (!reg) {
+		pr_err("PCIe: Event registration is NULL\n");
+
+		return -ENODEV;
+	}
+	if (!reg->user) {
+		pr_err("PCIe: User of event registration is NULL\n");
+
+		return -ENODEV;
+	}
+	pp = PCIE_BUS_PRIV_DATA(((struct pci_dev *)reg->user));
+	pci = to_dw_pcie_from_pp(pp);
+	exynos_pcie = to_exynos_pcie(pci);
+
+	if (pp) {
+		exynos_pcie->event_reg = reg;
+		dev_info(pci->dev, "Event 0x%x is registered for RC %d\n",
+			 reg->events, exynos_pcie->ch_num);
+	} else {
+		pr_err("PCIe: did not find RC for pci endpoint device\n");
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_register_event);
+
+int exynos_pcie_deregister_event(struct exynos_pcie_register_event *reg)
+{
+	int ret = 0;
+	struct dw_pcie_rp *pp;
+	struct exynos_pcie *exynos_pcie;
+	struct dw_pcie *pci;
+
+	if (!reg) {
+		pr_err("PCIe: Event deregistration is NULL\n");
+
+		return -ENODEV;
+	}
+	if (!reg->user) {
+		pr_err("PCIe: User of event deregistration is NULL\n");
+
+		return -ENODEV;
+	}
+
+	pp = PCIE_BUS_PRIV_DATA(((struct pci_dev *)reg->user));
+	pci = to_dw_pcie_from_pp(pp);
+	exynos_pcie = to_exynos_pcie(pci);
+
+	if (pp) {
+		exynos_pcie->event_reg = NULL;
+		dev_info(pci->dev, "Event is deregistered for RC %d\n", exynos_pcie->ch_num);
+	} else {
+		pr_err("PCIe: did not find RC for pci endpoint device\n");
+		ret = -ENODEV;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_deregister_event);
+
+int exynos_pcie_rc_set_affinity(int ch_num, int affinity)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci;
+	struct dw_pcie_rp *pp;
+
+	if (!exynos_pcie) {
+		pr_err("%s: ch#%d PCIe device is not loaded\n", __func__, ch_num);
+
+		return -ENODEV;
+	}
+
+	pci = exynos_pcie->pci;
+	pp = &pci->pp;
+
+	irq_set_affinity_hint(pp->irq, cpumask_of(affinity));
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(exynos_pcie_rc_set_affinity);
+
+int exynos_pcie_rc_set_enable_wake(struct irq_data *data, unsigned int enable)
+{
+	int ret = 0;
+	struct dw_pcie_rp *pp = data->parent_data->domain->host_data;
+
+	pr_debug("%s: enable = %d\n", __func__, enable);
+
+	if (pp == NULL) {
+		pr_err("Warning: exynos_pcie_rc_set_enable_wake: not exist pp\n");
+		return -EINVAL;
+	}
+
+	if (enable)
+		ret = enable_irq_wake(pp->irq);
+	else
+		ret = disable_irq_wake(pp->irq);
+
+	return ret;
+}
+
+#if 0//IS_ENABLED(CONFIG_CPU_IDLE)
+static void __maybe_unused exynos_pcie_rc_set_tpoweron(struct dw_pcie_rp *pp, int max)
+{
+	void __iomem *ep_dbi_base = pp->va_cfg0_base;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	u32 val;
+
+	if (exynos_pcie->state != STATE_LINK_UP)
+		return;
+
+	/* Disable ASPM */
+	val = readl(ep_dbi_base + WIFI_L1SS_LINKCTRL);
+	val &= ~(WIFI_ASPM_CONTROL_MASK);
+	writel(val, ep_dbi_base + WIFI_L1SS_LINKCTRL);
+
+	val = readl(ep_dbi_base + WIFI_L1SS_CONTROL);
+	writel(val & ~(WIFI_ALL_PM_ENABEL), ep_dbi_base + WIFI_L1SS_CONTROL);
+
+	if (max) {
+		writel(PORT_LINK_TPOWERON_3100US, ep_dbi_base + WIFI_L1SS_CONTROL2);
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL2, 4,
+					   PORT_LINK_TPOWERON_3100US);
+	} else {
+		writel(PORT_LINK_TPOWERON_130US, ep_dbi_base + WIFI_L1SS_CONTROL2);
+		exynos_pcie_rc_wr_own_conf(pp, PCIE_LINK_L1SS_CONTROL2, 4,
+					   PORT_LINK_TPOWERON_130US);
+	}
+
+	/* Enable L1ss */
+	val = readl(ep_dbi_base + WIFI_L1SS_LINKCTRL);
+	val |= WIFI_ASPM_L1_ENTRY_EN;
+	writel(val, ep_dbi_base + WIFI_L1SS_LINKCTRL);
+
+	val = readl(ep_dbi_base + WIFI_L1SS_CONTROL);
+	val |= WIFI_ALL_PM_ENABEL;
+	writel(val, ep_dbi_base + WIFI_L1SS_CONTROL);
+}
+#endif
+
+static int exynos_pcie_msi_set_affinity(struct irq_data *irq_data, const struct cpumask *mask,
+					bool force)
+{
+	struct dw_pcie_rp *pp;
+	struct irq_data *idata = irq_data->parent_data;
+	struct dw_pcie *pci;
+	struct exynos_pcie *exynos_pcie;
+
+	if (!idata)
+		return -ENODEV;
+
+	/* set affinity for PCIe IRQ */
+	pp = (struct dw_pcie_rp *) idata->chip_data;
+	if (!pp)
+		return -ENODEV;
+
+	pci = to_dw_pcie_from_pp(pp);
+	if (!pci)
+		return -ENODEV;
+
+	exynos_pcie = to_exynos_pcie(pci);
+	if (!exynos_pcie)
+		return -ENODEV;
+
+	/* modem driver sets msi irq affinity */
+	if (exynos_pcie->ch_num == 0)
+		return 0;
+
+	idata = irq_get_irq_data(pp->irq);
+	if (!idata || !idata->chip)
+		return -ENODEV;
+
+	if (idata->chip->irq_set_affinity)
+		idata->chip->irq_set_affinity(idata, mask, force);
+
+	return 0;
+}
+static const char *sep_irq_name[PCIE_MAX_SEPA_IRQ_NUM] = {
+	"exynos-pcie-msi0", "exynos-pcie-msi1", "exynos-pcie-msi2",
+	"exynos-pcie-msi3", "exynos-pcie-msi4" };
+
+static irqreturn_t exynos_pcie_msi0_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ch_num = exynos_pcie->ch_num;
+	struct separated_msi_vector *msi_vec = &sep_msi_vec[ch_num][0];
+
+	if (!msi_vec->is_used) {
+		dev_err(dev, "Unexpected separated MSI0 interrupt!");
+		return IRQ_HANDLED;
+	}
+
+	dw_handle_msi_irq(pp);
+
+	if (!msi_vec->flags) {
+		/* To set as handle_level_irq, get virq, mapped_irq, irq_data. */
+		struct irq_data *irq_data;
+		int i, virq, mapped_irq, changed_irq = 0;
+
+		virq = irq_find_mapping(pp->irq_domain, 0);
+		mapped_irq = pp->irq_domain->mapcount;
+
+		dev_dbg(dev, "Start virq = %d, Total mapped irq = %d\n",
+				virq, mapped_irq);
+
+		for (i = 0; i < PCIE_DOMAIN_MAX_IRQ; i++) {
+			irq_data = irq_domain_get_irq_data(pp->irq_domain, virq);
+			if (irq_data == NULL) {
+				virq++;
+				continue;
+			}
+
+			dev_dbg(dev, "Change flow interrupt for virq(%d)\n", virq);
+			irq_domain_set_info(pp->irq_domain, virq, irq_data->hwirq,
+					pp->msi_irq_chip,
+					pp, handle_level_irq,
+					NULL, NULL);
+
+			virq++;
+			changed_irq++;
+
+			if (changed_irq == mapped_irq)
+				break;
+		}
+		msi_vec->flags = 1;
+	}
+	exynos_pcie_msi_post_process(pp);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t exynos_pcie_msi1_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ch_num = exynos_pcie->ch_num;
+	struct separated_msi_vector *msi_vec = &sep_msi_vec[ch_num][1];
+	int vec_num = 1;
+
+	/* Clear MSI interrupt */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_STATUS +
+			(vec_num * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x1);
+
+	if (!msi_vec->is_used) {
+		dev_err(dev, "Unexpected separated MSI1 interrupt!");
+		return IRQ_HANDLED;
+	}
+
+	if (msi_vec->msi_irq_handler != NULL)
+		msi_vec->msi_irq_handler(irq, msi_vec->context);
+
+	exynos_pcie_msi_post_process(pp);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t exynos_pcie_msi2_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ch_num = exynos_pcie->ch_num;
+	struct separated_msi_vector *msi_vec = &sep_msi_vec[ch_num][2];
+	int vec_num = 2;
+
+	/* Clear MSI interrupt */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_STATUS +
+			(vec_num * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x1);
+
+	if (!msi_vec->is_used) {
+		dev_err(dev, "Unexpected separated MSI2 interrupt!");
+		return IRQ_HANDLED;
+	}
+
+	if (msi_vec->msi_irq_handler != NULL)
+		msi_vec->msi_irq_handler(irq, msi_vec->context);
+
+	exynos_pcie_msi_post_process(pp);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t exynos_pcie_msi3_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ch_num = exynos_pcie->ch_num;
+	struct separated_msi_vector *msi_vec = &sep_msi_vec[ch_num][3];
+	int vec_num = 3;
+
+	/* Clear MSI interrupt */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_STATUS +
+			(vec_num * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x1);
+
+	if (!msi_vec->is_used) {
+		dev_err(dev, "Unexpected separated MSI3 interrupt!");
+		return IRQ_HANDLED;
+	}
+
+	if (msi_vec->msi_irq_handler != NULL)
+		msi_vec->msi_irq_handler(irq, msi_vec->context);
+
+	exynos_pcie_msi_post_process(pp);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t exynos_pcie_msi4_handler(int irq, void *arg)
+{
+	struct dw_pcie_rp *pp = arg;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct device *dev = pci->dev;
+	int ch_num = exynos_pcie->ch_num;
+	struct separated_msi_vector *msi_vec = &sep_msi_vec[ch_num][4];
+	int vec_num = 4;
+
+	/* Clear MSI interrupt */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_STATUS +
+			(vec_num * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x1);
+
+	if (!msi_vec->is_used) {
+		dev_err(dev, "Unexpected separated MSI4 interrupt!");
+		return IRQ_HANDLED;
+	}
+
+	if (msi_vec->msi_irq_handler != NULL)
+		msi_vec->msi_irq_handler(irq, msi_vec->context);
+
+	exynos_pcie_msi_post_process(pp);
+
+	return IRQ_HANDLED;
+}
+
+irqreturn_t (*msi_handler[PCIE_MAX_SEPA_IRQ_NUM])(int , void *) = {
+	exynos_pcie_msi0_handler, exynos_pcie_msi1_handler,  exynos_pcie_msi2_handler,
+	exynos_pcie_msi3_handler, exynos_pcie_msi4_handler };
+
+int register_separated_msi_vector(int ch_num, irq_handler_t handler, void *context,
+		int *irq_num)
+{
+	struct exynos_pcie *exynos_pcie = &g_pcie_rc[ch_num];
+	struct dw_pcie *pci = exynos_pcie->pci;
+	struct dw_pcie_rp *pp = &pci->pp;
+	int i, ret;
+
+	for (i = 1; i < PCIE_MAX_SEPA_IRQ_NUM; i++) {
+		if (!sep_msi_vec[ch_num][i].is_used)
+			break;
+	}
+
+	if (i == PCIE_MAX_SEPA_IRQ_NUM) {
+		dev_info(pci->dev, "PCIe Ch%d : There is no empty MSI vector!\n", ch_num);
+		return -EBUSY;
+	}
+
+	pr_info("PCIe Ch%d MSI%d vector is registered!\n", ch_num, i);
+	sep_msi_vec[ch_num][i].is_used = true;
+	sep_msi_vec[ch_num][i].context = context;
+	sep_msi_vec[ch_num][i].msi_irq_handler = handler;
+	*irq_num = sep_msi_vec[ch_num][i].irq;
+
+	/* Enable MSI interrupt for separated MSI. */
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_ENABLE +
+			(i * MSI_REG_CTRL_BLOCK_SIZE), 4, 0x1);
+	exynos_pcie_rc_wr_own_conf(pp, PCIE_MSI_INTR0_MASK +
+			(i * MSI_REG_CTRL_BLOCK_SIZE), 4, ~(0x1));
+
+	ret = devm_request_irq(pci->dev, sep_msi_vec[ch_num][i].irq,
+			msi_handler[i],
+			IRQF_SHARED | IRQF_TRIGGER_HIGH,
+			sep_irq_name[i], pp);
+	if (ret) {
+		pr_err("failed to request MSI%d irq\n", i);
+
+		return ret;
+	}
+
+	return i * PCIE_MSI_MAX_VEC_NUM;
+}
+EXPORT_SYMBOL_GPL(register_separated_msi_vector);
+
+
+static int exynos_pcie_rc_add_port(struct platform_device *pdev, struct dw_pcie_rp *pp, int ch_num)
+{
+	struct irq_domain *msi_domain;
+	struct msi_domain_info *msi_domain_info;
+	int ret, i, sep_irq;
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	pp->irq = platform_get_irq(pdev, 0);
+	if (!pp->irq) {
+		dev_err(&pdev->dev, "failed to get irq\n");
+		return -ENODEV;
+	}
+	ret = devm_request_irq(&pdev->dev, pp->irq, exynos_pcie_rc_irq_handler,
+			       IRQF_SHARED | IRQF_TRIGGER_HIGH, "exynos-pcie", pp);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to request irq\n");
+
+		return ret;
+	}
+	exynos_pcie_setup_rc(pp);
+
+	if (exynos_pcie->ep_device_type == EP_QC_WIFI) {
+		/*
+		 * Set DMA mask to 32-bit because
+		 * devices only work with 32-bit MSI.
+		 */
+		ret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(32));
+		if (ret) {
+			dev_err(&pdev->dev, "Failed to set DMA mask to 32-bit.");
+		}
+	}
+	pp->msi_irq[0] = -ENODEV;
+
+	ret = dw_pcie_host_init(pp);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to dw pcie host init\n");
+
+		return ret;
+	}
+
+	if (pp->msi_domain) {
+		msi_domain = pp->msi_domain;
+		msi_domain_info = (struct msi_domain_info *)msi_domain->host_data;
+		msi_domain_info->chip->irq_set_affinity = exynos_pcie_msi_set_affinity;
+		msi_domain_info->chip->irq_set_wake = exynos_pcie_rc_set_enable_wake;
+		if (exynos_pcie->ep_device_type == EP_QC_WIFI ||
+				exynos_pcie->ep_device_type == EP_SAMSUNG_MODEM) {
+			msi_domain_info->chip->irq_mask = pci_msi_mask_irq;
+			msi_domain_info->chip->irq_unmask = pci_msi_unmask_irq;
+		}
+	}
+
+	return 0;
+}
+
+static void exynos_pcie_rc_pcie_ops_init(struct dw_pcie_rp *pp)
+{
+	struct dw_pcie *pci = to_dw_pcie_from_pp(pp);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+	struct exynos_pcie_ops *pcie_ops = &exynos_pcie->exynos_pcie_ops;
+	struct device *dev = pci->dev;
+
+	dev_info(dev, "Initialize PCIe function.\n");
+
+	pcie_ops->poweron = exynos_pcie_rc_poweron;
+	pcie_ops->poweroff = exynos_pcie_rc_poweroff;
+	pcie_ops->rd_own_conf = exynos_pcie_rc_rd_own_conf;
+	pcie_ops->wr_own_conf = exynos_pcie_rc_wr_own_conf;
+	pcie_ops->rd_other_conf = exynos_pcie_rc_rd_other_conf;
+	pcie_ops->wr_other_conf = exynos_pcie_rc_wr_other_conf;
+}
+
+static int exynos_pcie_rc_make_reg_tb(struct device *dev, struct exynos_pcie *exynos_pcie)
+{
+	unsigned int pos, val, id;
+	int i;
+
+	/* initialize the reg table */
+	for (i = 0; i < 48; i++) {
+		exynos_pcie->pci_cap[i] = 0;
+		exynos_pcie->pci_ext_cap[i] = 0;
+	}
+
+	pos = 0xFF & readl(exynos_pcie->rc_dbi_base + PCI_CAPABILITY_LIST);
+
+	while (pos) {
+		val = readl(exynos_pcie->rc_dbi_base + pos);
+		id = val & CAP_ID_MASK;
+		exynos_pcie->pci_cap[id] = pos;
+		pos = (readl(exynos_pcie->rc_dbi_base + pos) & CAP_NEXT_OFFSET_MASK) >> 8;
+		dev_info(dev, "Next Cap pointer : 0x%x\n", pos);
+	}
+
+	pos = PCI_CFG_SPACE_SIZE;
+
+	while (pos) {
+		val = readl(exynos_pcie->rc_dbi_base + pos);
+		if (val == 0) {
+			dev_info(dev, "we have no ext capabilities!\n");
+
+			break;
+		}
+		id = PCI_EXT_CAP_ID(val);
+		exynos_pcie->pci_ext_cap[id] = pos;
+		pos = PCI_EXT_CAP_NEXT(val);
+		dev_info(dev, "Next ext Cap pointer : 0x%x\n", pos);
+	}
+
+	for (i = 0; i < 48; i++) {
+		if (exynos_pcie->pci_cap[i])
+			dev_info(dev, "PCIe cap [0x%x][%s]: 0x%x\n",
+				 i, CAP_ID_NAME(i), exynos_pcie->pci_cap[i]);
+	}
+	for (i = 0; i < 48; i++) {
+		if (exynos_pcie->pci_ext_cap[i])
+			dev_info(dev, "PCIe ext cap [0x%x][%s]: 0x%x\n",
+				 i, EXT_CAP_ID_NAME(i), exynos_pcie->pci_ext_cap[i]);
+	}
+
+	return 0;
+}
+
+u32 pcie_linkup_stat(void)
+{
+	pr_info("[%s] pcie_is_linkup : %d\n", __func__, pcie_is_linkup);
+
+	return pcie_is_linkup;
+}
+EXPORT_SYMBOL_GPL(pcie_linkup_stat);
+
+#if IS_ENABLED(CONFIG_GS_S2MPU)
+static int setup_s2mpu_mem(struct device *dev, struct exynos_pcie *exynos_pcie)
+{
+	struct device_node *np;
+	struct resource res;
+	struct phys_mem *pm;
+	phys_addr_t addr;
+	int ret;
+
+	/* Parse the memory nodes in the device tree to determine which areas
+	 * the s2mpu should protect.
+	 */
+
+	INIT_LIST_HEAD(&exynos_pcie->phys_mem_list);
+
+	for_each_node_by_type(np, "memory") {
+		ret = of_address_to_resource(np, 0, &res);
+		if (ret)
+			continue;
+
+		if (list_empty(&exynos_pcie->phys_mem_list)) {
+			pm = devm_kzalloc(dev, sizeof(*pm), GFP_KERNEL);
+			if (!pm)
+				return -ENOMEM;
+
+			pm->start = res.start;
+			pm->size = resource_size(&res);
+			list_add(&pm->list, &exynos_pcie->phys_mem_list);
+		} else {
+			/* To simplify the code, assume that memory regions
+			 * in the device tree are sorted in the descending
+			 * order. If this is not the case, abort.
+			 */
+			pm = list_last_entry(&exynos_pcie->phys_mem_list,
+					     struct phys_mem, list);
+			if (res.end > pm->start) {
+				dev_err(dev, "s2mpu memory sort invalid end=%pa start=%pa\n",
+					&res.end, &pm->start);
+				dev_err(dev, "This driver expects all DRAM ranges i.e. device tree nodes with device_type=\"memory\" to be defined in descending order in the device tree. Please change your device tree accordingly.\n");
+				return -EINVAL;
+			}
+
+			/* If two memory regions are consecutive, merge them. */
+			if (pm->start - res.end == 1) {
+				pm->start = res.start;
+				pm->size += resource_size(&res);
+			} else {
+				pm = devm_kzalloc(dev, sizeof(*pm), GFP_KERNEL);
+				if (!pm)
+					return -ENOMEM;
+
+				pm->start = res.start;
+				pm->size = resource_size(&res);
+				list_add_tail(&pm->list,
+					      &exynos_pcie->phys_mem_list);
+			}
+		}
+	}
+
+	list_for_each_entry(pm, &exynos_pcie->phys_mem_list, list) {
+		pm->refcnt_array = devm_kzalloc(dev, pm->size / SZ_4K,
+						GFP_KERNEL);
+		if (!pm->refcnt_array)
+			return -ENOMEM;
+
+		/* Optimize s2mpu operation by setting up 1G page tables */
+		addr = pm->start;
+		while (addr <  pm->start + pm->size) {
+			ret = s2mpu_close(exynos_pcie->s2mpu, addr, ALIGN_SIZE,
+					  DMA_BIDIRECTIONAL);
+			if (ret) {
+				dev_err(dev,
+					"probe s2mpu_close failed addr = 0x%pa\n",
+					&addr);
+			}
+			addr += SZ_1G;
+		}
+	}
+
+	return ret;
+}
+#endif
+
+static void exynos_d3_sleep_hook(void *unused, struct pci_dev *dev,
+				 unsigned int *delay_ms)
+{
+	if (delay_ms) {
+		usleep_range(*delay_ms * 1000, *delay_ms * 1000);
+		*delay_ms = 0;
+	}
+}
+
+static int exynos_pcie_rc_probe(struct platform_device *pdev)
+{
+	struct exynos_pcie *exynos_pcie;
+	struct dw_pcie *pci;
+	struct dw_pcie_rp *pp;
+	struct device_node *np = pdev->dev.of_node;
+	int ret = 0;
+	int ch_num;
+#if IS_ENABLED(CONFIG_GS_S2MPU)
+	struct device_node *s2mpu_dn;
+#endif
+
+	dev_info(&pdev->dev, "## PCIe RC PROBE start\n");
+
+	if (create_pcie_sys_file(&pdev->dev))
+		dev_err(&pdev->dev, "Failed to create pcie sys file\n");
+
+	if (of_property_read_u32(np, "ch-num", &ch_num)) {
+		dev_err(&pdev->dev, "Failed to parse the channel number\n");
+		return -EINVAL;
+	}
+/*
+	if (!is_vhook_registered) {
+		ret = register_trace_android_rvh_pci_d3_sleep(exynos_d3_sleep_hook,
+							      NULL);
+		if (ret) {
+			dev_err(&pdev->dev, "PCI sleep hook failed\n");
+			return ret;
+		}
+		is_vhook_registered = true;
+	}
+*/
+	dev_info(&pdev->dev, "## PCIe ch %d ##\n", ch_num);
+
+	pci = devm_kzalloc(&pdev->dev, sizeof(*pci), GFP_KERNEL);
+	if (!pci) {
+		/* dev_err(&pdev->dev, "dw_pcie allocation is failed\n"); */
+
+		return -ENOMEM;
+	}
+
+	exynos_pcie = &g_pcie_rc[ch_num];
+	exynos_pcie->pci = pci;
+
+	pci->dev = &pdev->dev;
+	pci->ops = &dw_pcie_ops;
+
+	pp = &pci->pp;
+	pp->ops = &exynos_pcie_rc_ops;
+
+	/* Reserve memory for pma_regs that may need to be restored on
+	 * pcie link up failure.
+	 */
+	exynos_pcie->pma_regs = devm_kcalloc(&pdev->dev, NUM_PMA_REGS,
+					     sizeof(u32), GFP_KERNEL);
+	if (!exynos_pcie->pma_regs)
+		return -ENOMEM;
+
+	spin_lock_init(&exynos_pcie->pcie_l1_exit_lock);
+	spin_lock_init(&exynos_pcie->conf_lock);
+	spin_lock_init(&exynos_pcie->power_stats_lock);
+	spin_lock_init(&exynos_pcie->reg_lock);
+	spin_lock_init(&exynos_pcie->s2mpu_refcnt_lock);
+
+	mutex_init(&exynos_pcie->power_onoff_lock);
+
+	exynos_pcie->ch_num = ch_num;
+	exynos_pcie->l1ss_enable = 1;
+	exynos_pcie->state = STATE_LINK_DOWN;
+
+	exynos_pcie->linkdown_cnt = 0;
+	exynos_pcie->sudden_linkdown = 0;
+	exynos_pcie->cpl_timeout_recovery = 0;
+	exynos_pcie->l1ss_ctrl_id_state = 0;
+	exynos_pcie->atu_ok = 0;
+
+	exynos_pcie->app_req_exit_l1 = PCIE_APP_REQ_EXIT_L1;
+	exynos_pcie->app_req_exit_l1_mode = PCIE_APP_REQ_EXIT_L1_MODE;
+	exynos_pcie->linkup_offset = PCIE_ELBI_RDLH_LINKUP;
+
+	pm_runtime_enable(&pdev->dev);
+	pm_runtime_get_sync(&pdev->dev);
+	device_enable_async_suspend(&pdev->dev);
+
+	dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(36));
+	platform_set_drvdata(pdev, exynos_pcie);
+	power_stats_init(exynos_pcie);
+	link_stats_init(exynos_pcie);
+
+#if IS_ENABLED(CONFIG_GS_S2MPU)
+	s2mpu_dn = of_parse_phandle(np, "s2mpu", 0);
+	if (s2mpu_dn) {
+		memcpy(&fake_dma_dev, &pdev->dev, sizeof(fake_dma_dev));
+		fake_dma_dev.dma_ops = NULL;
+
+		exynos_pcie->s2mpu = s2mpu_fwnode_to_info(&s2mpu_dn->fwnode);
+		if (!exynos_pcie->s2mpu) {
+			dev_err(&pdev->dev, "Failed to get S2MPU\n");
+			return -EPROBE_DEFER;
+		}
+
+		ret = setup_s2mpu_mem(&pdev->dev, exynos_pcie);
+		if (ret) {
+			dev_err(&pdev->dev, "failed to bind to s2mpu\n");
+			goto probe_fail;
+		}
+
+		dev_info(&pdev->dev, "successfully bound to S2MPU\n");
+	}
+#endif
+
+	/* parsing pcie dts data for exynos */
+	ret = exynos_pcie_rc_parse_dt(&pdev->dev, exynos_pcie);
+	if (ret)
+		goto probe_fail;
+
+	ret = exynos_pcie_rc_get_pin_state(pdev, exynos_pcie);
+	if (ret)
+		goto probe_fail;
+
+	ret = exynos_pcie_rc_clock_get(pp);
+	if (ret)
+		goto probe_fail;
+
+	ret = exynos_pcie_rc_get_resource(pdev, exynos_pcie);
+	if (ret)
+		goto probe_fail;
+	pci->dbi_base = exynos_pcie->rc_dbi_base;
+
+	/* NOTE: TDB */
+	/* Mapping PHY functions */
+	exynos_pcie_rc_phy_init(pp);
+
+	exynos_pcie_rc_pcie_ops_init(pp);
+
+	exynos_pcie_rc_resumed_phydown(pp);
+
+	if (exynos_pcie->use_nclkoff_en)
+		exynos_pcie_rc_nclkoff_ctrl(pdev, exynos_pcie);
+
+	ret = exynos_pcie_rc_make_reg_tb(&pdev->dev, exynos_pcie);
+	if (ret)
+		goto probe_fail;
+
+	ret = exynos_pcie_rc_add_port(pdev, pp, ch_num);
+	if (ret)
+		goto probe_fail;
+
+	if (exynos_pcie->use_cache_coherency)
+		exynos_pcie_rc_set_iocc(pp, 1);
+
+	//disable_irq(pp->irq);
+
+#if 0//IS_ENABLED(CONFIG_CPU_IDLE)
+	exynos_pcie->idle_ip_index =
+			exynos_get_idle_ip_index(dev_name(&pdev->dev));
+	if (exynos_pcie->idle_ip_index < 0)
+		dev_err(&pdev->dev, "Cant get idle_ip_dex!!!\n");
+	else
+		dev_err(&pdev->dev, "PCIE idle ip index : %d\n", exynos_pcie->idle_ip_index);
+
+	exynos_update_ip_idle_status(exynos_pcie->idle_ip_index, PCIE_IS_IDLE);
+	dev_info(&pdev->dev, "%s, ip idle status : %d, idle_ip_index: %d\n",
+		 __func__, PCIE_IS_IDLE, exynos_pcie->idle_ip_index);
+#endif
+
+	exynos_pcie->pcie_wq = create_freezable_workqueue("pcie_wq");
+	if (IS_ERR(exynos_pcie->pcie_wq)) {
+		dev_err(&pdev->dev, "couldn't create workqueue\n");
+		ret = EBUSY;
+
+		goto probe_fail;
+	}
+
+	INIT_DELAYED_WORK(&exynos_pcie->dislink_work, exynos_pcie_rc_dislink_work);
+	INIT_DELAYED_WORK(&exynos_pcie->cpl_timeout_work, exynos_pcie_rc_cpl_timeout_work);
+
+/*
+	if (exynos_pcie->use_pcieon_sleep) {
+		dev_info(&pdev->dev, "## register pcie connection function\n");
+		register_pcie_is_connect(pcie_linkup_stat);
+	}*/
+
+	platform_set_drvdata(pdev, exynos_pcie);
+
+probe_fail:
+	if (exynos_pcie->use_phy_isol_con)
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_ISOLATION);
+
+	if (ret) {
+		dev_err(&pdev->dev, "## %s: PCIe probe failed\n", __func__);
+	} else {
+		dev_info(&pdev->dev, "## %s: PCIe probe success\n", __func__);
+	}
+
+	return ret;
+}
+
+static void exynos_pcie_rc_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct dw_pcie *pci = container_of(&dev, struct dw_pcie, dev);
+	struct exynos_pcie *exynos_pcie = to_exynos_pcie(pci);
+
+	dev_info(&pdev->dev, "%s\n", __func__);
+
+	mutex_destroy(&exynos_pcie->power_onoff_lock);
+}
+
+#if IS_ENABLED(CONFIG_PM)
+static int exynos_pcie_rc_suspend_noirq(struct device *dev)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	u32 val, val_irq0, val_irq1, val_irq2;
+
+	if (exynos_pcie->state == STATE_LINK_DOWN) {
+		dev_info(dev, "PCIe PMU ISOLATION\n");
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_ISOLATION);
+
+		return 0;
+	} else if (exynos_pcie->separated_msi && exynos_pcie->use_pcieon_sleep) {
+		dev_info(dev, "PCIe on sleep... suspend\n");
+
+		// handle IRQ0 interrupt
+		val_irq0 = exynos_elbi_read(exynos_pcie, PCIE_IRQ0);
+		exynos_elbi_write(exynos_pcie, val_irq0, PCIE_IRQ0);
+		dev_info(dev, "IRQ0 0x%x\n", val_irq0);
+
+		// handle IRQ1 interrupt
+		val_irq1 = exynos_elbi_read(exynos_pcie, PCIE_IRQ1);
+		exynos_elbi_write(exynos_pcie, val_irq1, PCIE_IRQ1);
+		dev_info(dev, "IRQ1 0x%x\n", val_irq1);
+
+		// handle IRQ2 interrupt
+		val_irq2 = exynos_elbi_read(exynos_pcie, PCIE_IRQ2);
+		exynos_elbi_write(exynos_pcie, val_irq2, PCIE_IRQ2);
+		dev_info(dev, "IRQ2 0x%x\n", val_irq2);
+
+		val = exynos_elbi_read(exynos_pcie, PCIE_IRQ2_EN);
+		val |= IRQ_MSI_CTRL_EN_RISING_EDG;
+		exynos_elbi_write(exynos_pcie, val, PCIE_IRQ2_EN);
+	}
+
+	if (exynos_pcie->use_pcieon_sleep && exynos_pcie->ep_pci_dev) {
+		dev_info(dev, "Default must_resume value : %d\n",
+				exynos_pcie->ep_pci_dev->dev.power.must_resume);
+		exynos_pcie->pcie_must_resume = exynos_pcie->ep_pci_dev->dev.power.must_resume;
+		if (exynos_pcie->ep_pci_dev->dev.power.must_resume)
+			exynos_pcie->ep_pci_dev->dev.power.must_resume = false;
+
+		dev_info(dev, "restore enable cnt = %d\n", exynos_pcie->pcieon_sleep_enable_cnt);
+		atomic_set(&exynos_pcie->ep_pci_dev->enable_cnt,
+				exynos_pcie->pcieon_sleep_enable_cnt);
+	}
+
+	return 0;
+}
+
+static int exynos_pcie_rc_resume_noirq(struct device *dev)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+	struct dw_pcie *pci = exynos_pcie->pci;
+	u32 val;
+
+	dev_info(dev, "## RESUME[%s] pcie_is_linkup: %d)\n", __func__, pcie_is_linkup);
+
+	if (exynos_pcie->state == STATE_LINK_DOWN) {
+		dev_info(dev, "[%s] dislink state after resume -> phy pwr off\n", __func__);
+		exynos_pcie_rc_resumed_phydown(&pci->pp);
+	} else if (exynos_pcie->separated_msi && exynos_pcie->use_pcieon_sleep) {
+		dev_info(dev, "PCIe on sleep resume...\n");
+		val = exynos_elbi_read(exynos_pcie, PCIE_IRQ2_EN);
+		val &= ~IRQ_MSI_CTRL_EN_RISING_EDG;
+		exynos_elbi_write(exynos_pcie, val, PCIE_IRQ2_EN);
+	}
+
+	return 0;
+}
+
+static int exynos_pcie_suspend_prepare(struct device *dev)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	if (exynos_pcie->use_phy_isol_con)
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_BYPASS);
+
+	if (exynos_pcie->use_pcieon_sleep && exynos_pcie->ep_pci_dev) {
+		exynos_pcie->pcieon_sleep_enable_cnt =
+			atomic_read(&exynos_pcie->ep_pci_dev->enable_cnt);
+		dev_info(dev, "remove enable cnt to fake enable = %d\n",
+				exynos_pcie->pcieon_sleep_enable_cnt);
+		atomic_set(&exynos_pcie->ep_pci_dev->enable_cnt, 0);
+	}
+
+	return 0;
+}
+
+static void exynos_pcie_resume_complete(struct device *dev)
+{
+	struct exynos_pcie *exynos_pcie = dev_get_drvdata(dev);
+
+	if (exynos_pcie->use_phy_isol_con &&
+	    exynos_pcie->state == STATE_LINK_DOWN)
+		exynos_pcie_phy_isolation(exynos_pcie, PCIE_PHY_ISOLATION);
+	else if (exynos_pcie->use_pcieon_sleep && exynos_pcie->ep_pci_dev) {
+		exynos_pcie->ep_pci_dev->dev.power.must_resume = exynos_pcie->pcie_must_resume;
+		dev_info(dev, "Default must_resume value : %d\n",
+				exynos_pcie->ep_pci_dev->dev.power.must_resume);
+	}
+}
+
+#endif
+
+static const struct dev_pm_ops exynos_pcie_rc_pm_ops = {
+	.suspend_noirq	= exynos_pcie_rc_suspend_noirq,
+	.resume_noirq	= exynos_pcie_rc_resume_noirq,
+	.prepare	= exynos_pcie_suspend_prepare,
+	.complete	= exynos_pcie_resume_complete,
+};
+
+static const struct of_device_id exynos_pcie_rc_of_match[] = {
+	{ .compatible = "samsung,exynos-pcie-rc", },
+	{},
+};
+
+static struct platform_driver exynos_pcie_rc_driver = {
+	.probe		= exynos_pcie_rc_probe,
+	.remove		= exynos_pcie_rc_remove,
+	.driver = {
+		.name		= "exynos-pcie-rc",
+		.owner		= THIS_MODULE,
+		.of_match_table = exynos_pcie_rc_of_match,
+		//.pm		= &exynos_pcie_rc_pm_ops,
+	},
+};
+module_platform_driver(exynos_pcie_rc_driver);
+
+MODULE_AUTHOR("Hongseock Kim <hongpooh.kim@samsung.com>");
+MODULE_DESCRIPTION("Samsung PCIe RC(RootComplex) controller driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/pci/controller/dwc/pcie-exynos-rc.h b/drivers/pci/controller/dwc/pcie-exynos-rc.h
new file mode 100644
index 000000000..402a7d484
--- /dev/null
+++ b/drivers/pci/controller/dwc/pcie-exynos-rc.h
@@ -0,0 +1,404 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe RC(RootComplex) controller driver for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ */
+
+#ifndef __PCIE_EXYNOS_RC_H
+#define __PCIE_EXYNOS_RC_H
+
+#define MSI_64CAP_MASK	(0x1 << 23)
+#define MSI_CONTROL	0x50
+/* SYSREG NCLKOFF */
+#define PCIE_SUB_CTRL_SLV_EN	(0x1 << 0)
+#define PCIE_SLV_BUS_NCLK_OFF	(0x1 << 1)
+#define PCIE_DBI_BUS_NCLK_OFF	(0x1 << 2)
+/* PCIe ELBI registers */
+#define PCIE_IRQ0			0x000
+#define IRQ_INTA_ASSERT			(0x1 << 14)
+#define IRQ_INTB_ASSERT			(0x1 << 16)
+#define IRQ_INTC_ASSERT			(0x1 << 18)
+#define IRQ_INTD_ASSERT			(0x1 << 20)
+#define IRQ_RADM_PM_TO_ACK              (0x1 << 29)
+#define PCIE_IRQ1			0x004
+#define IRQ_LINK_DOWN_ASSERT		(0x1 << 10)
+#define PCIE_IRQ2			0x008
+#define IRQ_MSI_FALLING_ASSERT		(0x1 << 16)
+#define IRQ_MSI_RISING_ASSERT		(0x1 << 17)
+#define IRQ_RADM_CPL_TIMEOUT_ASSERT	(0x1 << 24)
+#define PCIE_IRQ0_EN			0x010
+#define IRQ_INTA_ENABLE			(0x1 << 14)
+#define IRQ_INTB_ENABLE			(0x1 << 16)
+#define IRQ_INTC_ENABLE			(0x1 << 18)
+#define IRQ_INTD_ENABLE			(0x1 << 20)
+#define PCIE_IRQ1_EN			0x014
+#define IRQ_LINKDOWN_ENABLE_EVT1_1	(0x1 << 10) /* Exynos9820-EVT1.1 only */
+#define IRQ_LINK_DOWN_ENABLE		(0x1 << 10)
+#define PCIE_IRQ2_EN			0x018
+#define IRQ_LINKDOWN_ENABLE_old		(0x1 << 14) /* Exynos9820-EVT0.0 only */
+#define IRQ_MSI_CTRL_EN_FALLING_EDG	(0x1 << 16)
+#define IRQ_MSI_CTRL_EN_RISING_EDG	(0x1 << 17)
+#define IRQ_RADM_CPL_TIMEOUT_ENABLE	(0x1 << 24)
+#define PCIE_APP_LTSSM_ENABLE		0x054
+#define PCIE_ELBI_LTSSM_DISABLE		0x0
+#define PCIE_ELBI_LTSSM_ENABLE		0x1
+#define PCIE_APP_REQ_EXIT_L1		0x06C
+#define APP_INIT_RST			0x100
+#define XMIT_PME_TURNOFF		0x118
+#define PCIE_ELBI_RDLH_LINKUP		0x2C8
+#define PCIE_ELBI_LTSSM_STATE_MASK	0x003F
+#define PCIE_CXPL_DEBUG_INFO_H		0x2CC
+#define PCIE_PM_DSTATE			0x2E8
+#define PCIE_PM_DSTATE_MASK		0x0003
+#define PCIE_LINKDOWN_RST_CTRL_SEL	0x3A0
+#define PCIE_LINKDOWN_RST_MANUAL	(0x1 << 1)
+#define PCIE_LINKDOWN_RST_FSM		(0x1 << 0)
+#define PCIE_SOFT_RESET			0x3A4
+#define SOFT_CORE_RESET			(0x1 << 0)
+#define SOFT_PWR_RESET			(0x1 << 1)
+#define SOFT_STICKY_RESET		(0x1 << 2)
+#define SOFT_NON_STICKY_RESET		(0x1 << 3)
+#define SOFT_PHY_RESET			(0x1 << 4)
+#define PCIE_QCH_SEL			0x3A8
+#define CLOCK_GATING_IN_L12		0x1
+#define CLOCK_NOT_GATING		0x3
+#define CLOCK_GATING_MASK		0x3
+#define CLOCK_GATING_PMU_L1		(0x1 << 11)
+#define CLOCK_GATING_PMU_L23READY	(0x1 << 10)
+#define CLOCK_GATING_PMU_DETECT_QUIET	(0x1 << 9)
+#define CLOCK_GATING_PMU_L12		(0x1 << 8)
+#define CLOCK_GATING_PMU_ALL		(0xF << 8)
+#define CLOCK_GATING_PMU_MASK		(0xF << 8)
+#define CLOCK_GATING_APB_L1		(0x1 << 7)
+#define CLOCK_GATING_APB_L23READY	(0x1 << 6)
+#define CLOCK_GATING_APB_DETECT_QUIET	(0x1 << 5)
+#define CLOCK_GATING_APB_L12		(0X1 << 4)
+#define CLOCK_GATING_APB_ALL		(0xF << 4)
+#define CLOCK_GATING_APB_MASK		(0xF << 4)
+#define CLOCK_GATING_AXI_L1		(0x1 << 3)
+#define CLOCK_GATING_AXI_L23READY	(0x1 << 2)
+#define CLOCK_GATING_AXI_DETECT_QUIET	(0x1 << 1)
+#define CLOCK_GATING_AXI_L12		(0x1 << 0)
+#define CLOCK_GATING_AXI_ALL		(0xF << 0)
+#define CLOCK_GATING_AXI_MASK		(0xF << 0)
+#define PCIE_APP_REQ_EXIT_L1_MODE	0x3BC
+#define L1_REQ_NAK_CONTROL		(0x3 << 4)	/* need to check */
+#define L1_REQ_NAK_CONTROL_MASTER	(0x1 << 4)
+#define PCIE_SW_WAKE			0x3D4
+#define PCIE_STATE_HISTORY_CHECK	0xC00
+#define HISTORY_BUFFER_ENABLE		0x3
+#define HISTORY_BUFFER_CLEAR		(0x1 << 1)
+#define HISTORY_BUFFER_CONDITION_SEL	(0x1 << 2)
+#define PCIE_STATE_POWER_S		0xC04
+#define PCIE_STATE_POWER_M		0xC08
+#define PCIE_HISTORY_REG(x)		(0xC0C + ((x) * 0x4)) /* history_reg0: 0xC0C */
+#define LTSSM_STATE(x)			(((x) >> 16) & 0x3f)
+#define PM_DSTATE(x)			(((x) >> 8) & 0x7)
+#define L1SUB_STATE(x)			(((x) >> 0) & 0x7)
+#define PCIE_DMA_MONITOR1		0x460
+#define PCIE_DMA_MONITOR2		0x464
+#define PCIE_DMA_MONITOR3		0x468
+#define FSYS1_MON_SEL_MASK		0xf
+#define PCIE_MON_SEL_MASK		0xff
+
+#define PCIE_MSTR_PEND_SEL_NAK		0x474
+#define NACK_ENABLE			0x1
+
+#define PCIE_DBI_L1_EXIT_DISABLE	0x1078
+#define DBI_L1_EXIT_DISABLE		0x1
+
+/* PCIe PMU registers */
+#define IDLE_IP3_STATE			0x3EC
+#define IDLE_IP_RC1_SHIFT		(31)
+#define IDLE_IP_RC0_SHIFT		(30)
+#define IDLE_IP3_MASK			0x3FC
+#define WAKEUP_MASK			0x3944
+#define WAKEUP_MASK_PCIE_WIFI		16
+#define PCIE_PHY_CONTROL		0x071C
+#define PCIE_PHY_CONTROL_MASK		0x1
+
+/* PCIe DBI registers */
+#define PM_CAP_OFFSET			0x40
+#define PCIE_CAP_OFFSET			0x70
+#define PCIE_LINK_CTRL_STAT		0x80
+#define PCIE_LINK_CTRL_LINK_SPEED_MASK	0xf
+#define PCIE_LINK_CTRL2_STAT2		0xA0
+#define PCIE_LINK_CTRL2_TARGET_LINK_SPEED_MASK	0xfffffff0
+#define PCIE_CAP_LINK_SPEED		0xf
+#define PCIE_CAP_NEGO_LINK_WIDTH_MASK	0x3f
+#define PCI_EXP_LNKCAP_MLW_X1		(0x1 << 4)
+#define PCI_EXP_LNKCAP_L1EL_64USEC	(0x7 << 15)
+/* previous definition is in 'include/uapi/linux/pci_regs.h:661', PCI_EXP_LNKCTL2_TLS: 0xf */
+#define PCI_EXP_LNKCTL2_TLS_2_5GB	0x1
+#define PCI_EXP_LNKCTL2_TLS_5_0GB	0x2
+#define PCI_EXP_LNKCTL2_TLS_8_0GB	0x3
+#define PCIE_CAP_CPL_TIMEOUT_VAL_MASK	0xf
+#define PCIE_CAP_CPL_TIMEOUT_VAL_44MS_DEFALT	0x0
+#define PCIE_CAP_CPL_TIMEOUT_VAL_6_2MS	0x2
+#define PCIE_LINK_L1SS_CONTROL		0x19C
+#define PORT_LINK_TCOMMON_32US		(0x20 << 8)
+#define LTR_L12_THRESHOLD_SCALE_1NS	(0x0 << 29) /* Latency Tolerance Reporting */
+#define LTR_L12_THRESHOLD_SCALE_32NS	(0x1 << 29)
+#define LTR_L12_THRESHOLD_SCALE_1024NS	(0x2 << 29)
+#define LTR_L12_THRESHOLD_SCALE_32768NS	(0x3 << 29)
+#define LTR_L12_THRESHOLD_VALUE_160	(0xa0 << 16)
+#define PORT_LINK_L12_LTR_THRESHOLD     (0x40a0 << 16)
+#define PCIE_LINK_L1SS_CONTROL2		0x1A0
+#define PORT_LINK_L1SS_ENABLE		(0xf << 0)
+#define PORT_LINK_TPOWERON_10US		(0x28 << 0)
+#define PORT_LINK_TPOWERON_90US		(0x49 << 0)
+#define PORT_LINK_TPOWERON_130US	(0x69 << 0)
+#define PORT_LINK_TPOWERON_180US	(0x89 << 0)
+#define PORT_LINK_TPOWERON_200US	(0xA1 << 0)
+#define PORT_LINK_TPOWERON_3100US	(0xfa << 0)
+#define PORT_LINK_L1SS_T_PCLKACK	(0x3 << 6)
+#define PORT_LINK_L1SS_T_L1_2		(0x4 << 2)
+#define PORT_LINK_L1SS_T_POWER_OFF	(0x2 << 0)
+#define PCIE_ACK_F_ASPM_CONTROL		0x70C
+#define PCIE_L1_ENTERANCE_LATENCY      (0x7 << 27)
+#define PCIE_L1_ENTERANCE_LATENCY_8us  (0x3 << 27)
+#define PCIE_L1_ENTERANCE_LATENCY_16us (0x4 << 27)
+#define PCIE_L1_ENTERANCE_LATENCY_32us (0x5 << 27)
+#define PCIE_L1_ENTERANCE_LATENCY_64us (0x7 << 27)
+#define PCIE_PORT_LINK_CONTROL		0x710
+
+#define PCIE_MISC_CONTROL		0x8BC
+#define DBI_RO_WR_EN			0x1
+
+#define PCIE_COHERENCY_CONTROL_3_OFF	0x8E8
+
+#define PCIE_AUX_CLK_FREQ_OFF		0xB40
+#define PCIE_AUX_CLK_FREQ_24MHZ		0x18
+#define PCIE_AUX_CLK_FREQ_26MHZ		0x1A
+#define PCIE_L1_SUBSTATES_OFF		0xB44
+#define PCIE_L1_SUB_VAL			0xEA
+
+#define LINK_CONTROL2_LINK_STATUS2_REG	0xA0
+#define PCIE_CAP_TARGET_LINK_SPEED_MASK 0xfffffff0
+#define PCIE_LINK_WIDTH_SPEED_CONTROL	0x80C
+#define DIRECT_SPEED_CHANGE_ENABLE_MASK	0xfffdffff
+#define DIRECT_SPEED_CHANGE_ENABLE	0x20000
+#define EXYNOS_PORT_LOGIC_SPEED_CHANGE	(0x1 << 17)
+
+#define MULTI_LANE_CONTROL_OFF		0x8c0
+#define TARGET_LINK_WIDTH_MASK		0xffffffc0
+#define DIRECT_LINK_WIDTH_CHANGE_SET	0x40
+
+#define PCIE_ATU_VIEWPORT		0x900
+#define EXYNOS_PCIE_ATU_REGION_INBOUND	(0x1 << 31)
+#define EXYNOS_PCIE_ATU_REGION_OUTBOUND	(0x0 << 31)
+#define EXYNOS_PCIE_ATU_REGION_INDEX2	(0x2 << 0)
+#define EXYNOS_PCIE_ATU_REGION_INDEX1	(0x1 << 0)
+#define EXYNOS_PCIE_ATU_REGION_INDEX0	(0x0 << 0)
+#define PCIE_ATU_CR1			0x904
+#define EXYNOS_PCIE_ATU_TYPE_MEM	(0x0 << 0)
+#define EXYNOS_PCIE_ATU_TYPE_IO		(0x2 << 0)
+#define EXYNOS_PCIE_ATU_TYPE_CFG0	(0x4 << 0)
+#define EXYNOS_PCIE_ATU_TYPE_CFG1	(0x5 << 0)
+#define PCIE_ATU_CR2			0x908
+#define EXYNOS_PCIE_ATU_ENABLE		(0x1 << 31)
+#define EXYNOS_PCIE_ATU_BAR_MODE_ENABLE	(0x1 << 30)
+#define PCIE_ATU_LOWER_BASE		0x008 
+//0x90C
+#define PCIE_ATU_UPPER_BASE		0x00C
+//0x910
+#define PCIE_ATU_LIMIT	0x010
+//0x914
+#define PCIE_ATU_LOWER_TARGET		0x014
+//0x918
+#define EXYNOS_PCIE_ATU_BUS(x)		(((x) & 0xff) << 24)
+#define EXYNOS_PCIE_ATU_DEV(x)		(((x) & 0x1f) << 19)
+#define EXYNOS_PCIE_ATU_FUNC(x)		(((x) & 0x7) << 16)
+#define PCIE_ATU_UPPER_TARGET		0x018
+//0x91C
+
+#define PCIE_MSI_ADDR_LO		0x820
+#define PCIE_MSI_ADDR_HI		0x824
+#define PCIE_MSI_INTR0_ENABLE		0x828
+#define PCIE_MSI_INTR0_MASK		0x82C
+#define PCIE_MSI_INTR0_STATUS		0x830
+
+/* PCIe SYSREG registers */
+#define PCIE_WIFI0_PCIE_PHY_CONTROL	0xC
+#define BIFURCATION_MODE_DISABLE	(0x1 << 16)
+#define LINK1_ENABLE			(0x1 << 15)
+#define LINK0_ENABLE			(0x1 << 14)
+#define PCS_LANE1_ENABLE		(0x1 << 13)
+#define PCS_LANE0_ENABLE		(0x1 << 12)
+
+#define PCIE_SYSREG_SHARABILITY_CTRL	0x700
+#define PCIE_SYSREG_SHARABLE_OFFSET	8
+#define PCIE_SYSREG_SHARABLE_ENABLE	0x3
+#define PCIE_SYSREG_SHARABLE_DISABLE	0x0
+
+/* gs101: HSI1(GEN4A_0) & HSI2(GEN4A_1) */
+#define PCIE_SYSREG_HSI1_SHARABILITY_CTRL	0x704
+#define PCIE_SYSREG_HSI2_SHARABILITY_CTRL	0x730
+#define PCIE_SYSREG_HSIX_SHARABLE_OFFSET	0
+#define PCIE_SYSREG_HSIX_SHARABLE_ENABLE	0x3
+#define PCIE_SYSREG_HSIX_SHARABLE_DISABLE	0x0
+#define PCIE_SYSREG_HSIX_SHARABLE_MASK	0x3
+
+/* Definitions for WIFI L1.2 */
+#define WIFI_L1SS_CAPID			0x240
+#define WIFI_L1SS_CAP			0x244
+#define WIFI_L1SS_CONTROL		0x248
+#define WIFI_L1SS_CONTROL2		0x24C
+#define WIFI_L1SS_LTR_LATENCY		0x1B4
+#define WIFI_L1SS_LINKCTRL		0xBC
+#define WIFI_LINK_STATUS		0xBE
+#define WIFI_PCI_EXP_DEVCTL2		0xD4
+#define WIFI_PM_MNG_STATUS_CON		0x4C
+
+/* LINK Control Register */
+#define WIFI_ASPM_CONTROL_MASK		(0x3 << 0)
+#define WIFI_ASPM_L1_ENTRY_EN		(0x2 << 0)
+#define WIFI_USE_SAME_REF_CLK		(0x1 << 6)
+#define WIFI_CLK_REQ_EN			(0x1 << 8)
+
+/* L1SS Control Register */
+#define WIFI_ALL_PM_ENABEL		(0xf << 0)
+#define WIFI_PCIPM_L12_EN		(0x1 << 0)
+#define WIFI_PCIPM_L11_EN		(0x1 << 1)
+#define WIFI_ASPM_L12_EN		(0x1 << 2)
+#define WIFI_ASPM_L11_EN		(0x1 << 3)
+#define WIFI_COMMON_RESTORE_TIME	(0xa << 8)	/* Default Value */
+#define WIFI_QC_L12_LTR_THRESHOLD	(0x4096 << 16)
+
+/* L1SS LTR Latency Register */
+#define MAX_NO_SNOOP_LAT_VALUE_3	(3 << 16)
+#define MAX_SNOOP_LAT_VALUE_3		(3 << 0)
+#define MAX_NO_SNOOP_LAT_SCALE_MS	(0x4 << 26)	/* 0x4(b'100) value = 1,047,576 ns */
+#define MAX_SNOOP_LAT_SCALE_MS		(0x4 << 10)	/* 0x4(b'100) value = 1,047,576 ns */
+
+/* ETC definitions */
+#define IGNORE_ELECIDLE			1
+#define ENABLE_ELECIDLE			0
+#define	PCIE_DISABLE_CLOCK		0
+#define	PCIE_ENABLE_CLOCK		1
+#define PCIE_IS_IDLE			1
+#define PCIE_IS_ACTIVE			0
+
+/* PCIe PHY definitions */
+#define PHY_PLL_STATE			0xBC
+#define CHK_PHY_PLL_LOCK		0x3
+#define PM_POWER_STATE			0x188
+#define PM_STATE_MASK			0x07
+#define PHY_PCS_REFCLK_EN		(0x1 << 4)
+#define PHY_PCS_REFCLK_GATE_L12		(0x1 << 5)
+
+/* For Set NCLK OFF to avoid system hang */
+#define EXYNOS_PCIE_MAX_NAME_LEN        10
+#define PCIE_L12ERR_CTRL                0x2F0
+#define NCLK_OFF_OFFSET                 0x2
+
+#define PCIE_ATU_CR1_OUTBOUND0		0x300000
+#define PCIE_ATU_CR2_OUTBOUND0		0x300004
+#define PCIE_ATU_LOWER_BASE_OUTBOUND0	0x300008
+#define PCIE_ATU_UPPER_BASE_OUTBOUND0	0x30000C
+#define PCIE_ATU_LIMIT_OUTBOUND0	0x300010
+#define PCIE_ATU_LOWER_TARGET_OUTBOUND0	0x300014
+#define PCIE_ATU_UPPER_TARGET_OUTBOUND0	0x300018
+
+#define PCIE_ATU_CR1_OUTBOUND1		0x300200
+#define PCIE_ATU_CR2_OUTBOUND1		0x300204
+#define PCIE_ATU_LOWER_BASE_OUTBOUND1	0x300208
+#define PCIE_ATU_UPPER_BASE_OUTBOUND1	0x30020C
+#define PCIE_ATU_LIMIT_OUTBOUND1	0x300210
+#define PCIE_ATU_LOWER_TARGET_OUTBOUND1	0x300214
+#define PCIE_ATU_UPPER_TARGET_OUTBOUND1	0x300218
+
+#define PCIE_ATU_CR1_OUTBOUND2		0x300400
+#define PCIE_ATU_CR2_OUTBOUND2		0x300404
+#define PCIE_ATU_LOWER_BASE_OUTBOUND2	0x300408
+#define PCIE_ATU_UPPER_BASE_OUTBOUND2	0x30040C
+#define PCIE_ATU_LIMIT_OUTBOUND2	0x300410
+#define PCIE_ATU_LOWER_TARGET_OUTBOUND2	0x300414
+#define PCIE_ATU_UPPER_TARGET_OUTBOUND2	0x300418
+
+#define SECURE_ATU_ENABLE		0x5a5a5a5a
+#define SMC_SECURE_ATU_SETUP		0x820020D8
+
+#define EXYNOS_IP_VER_OF_WHI   0x984500
+
+#define EOM_PH_SEL_MAX		72
+#define EOM_DEF_VREF_MAX	256
+
+#define RX_CDR_LOCK			0xE0C
+#define RX_EFOM_DONE			0xE0C
+#define RX_EFOM_BIT_WIDTH_SEL		0xCA8
+#define ANA_RX_DFE_EOM_PI_STR_CTRL	0x988
+#define ANA_RX_DFE_EOM_PI_DIVSEL_G12	0x980
+#define ANA_RX_DFE_EOM_PI_DIVSEL_G34	0x984
+#define RX_EFOM_EOM_PH_SEL		0xCC4
+#define RX_EFOM_MODE			0xCA0
+#define MON_RX_EFOM_ERR_CNT_13_8	0xEBC
+#define MON_RX_EFOM_ERR_CNT_7_0		0xEC0
+#define RX_EFOM_DFE_VREF_CTRL		0xCB8
+#define RX_EFOM_NUMOF_SMPL_13_8		0xCAC
+#define RX_EFOM_NUMOF_SMPL_7_0		0xCB0
+
+struct pcie_eom_result {
+	unsigned int phase;
+	unsigned int vref;
+	unsigned long err_cnt;
+};
+
+void exynos_pcie_rc_phy_init(struct dw_pcie_rp *pp);
+
+#if !IS_ENABLED(CONFIG_EXYNOS_PCIE_IOMMU)
+extern struct dma_map_ops exynos_pcie_dma_ops;
+
+static void __maybe_unused pcie_sysmmu_enable(int hsi_block_num)
+{
+	pr_err("PCIe SysMMU is NOT Enabled!!!\n");
+}
+
+static void __maybe_unused pcie_sysmmu_disable(int hsi_block_num)
+{
+	pr_err("PCIe SysMMU is NOT Enabled!!!\n");
+}
+
+static int __maybe_unused pcie_iommu_map(unsigned long iova, phys_addr_t paddr,
+					 size_t size, int prot, int hsi_block_num)
+{
+	pr_err("PCIe SysMMU is NOT Enabled!!!\n");
+	return -ENODEV;
+}
+
+static size_t __maybe_unused pcie_iommu_unmap(unsigned long iova, size_t size,
+					    int hsi_block_num)
+{
+	pr_err("PCIe SysMMU is NOT Enabled!!!\n");
+	return 0;
+}
+
+static void __maybe_unused pcie_sysmmu_set_use_iocc(int hsi_block_num)
+{
+	pr_err("PCIe SysMMU is NOT Enabled!!!\n");
+}
+#endif
+
+#if !IS_ENABLED(CONFIG_GS_S2MPU)
+static void __maybe_unused s2mpu_update_refcnt(struct device *dev,
+					       dma_addr_t dma_addr, size_t size,
+					       bool incr, enum dma_data_direction dir)
+{
+	pr_err("PCIe S2MPU is NOT Enabled!!!\n");
+}
+#endif
+
+u32 exynos_pcie_rc_read_dbi(struct dw_pcie *dw_pcie, void __iomem *base, u32 reg, size_t size);
+void exynos_pcie_rc_write_dbi(struct dw_pcie *dw_pcie, void __iomem *base,
+			      u32 reg, size_t size, u32 val);
+int exynos_pcie_rc_poweron(int ch_num);
+void exynos_pcie_rc_poweroff(int ch_num);
+int exynos_pcie_rc_l1ss_ctrl(int enable, int id, int ch_num);
+int exynos_pcie_rc_set_outbound_atu(int ch_num, u32 target_addr, u32 offset, u32 size);
+int exynos_pcie_rc_check_link_speed(int ch_num);
+int exynos_pcie_rc_change_link_speed(int ch_num, int target_speed);
+int exynos_pcie_l1_exit(int ch_num);
+void exynos_pcie_rc_register_dump(int ch_num);
+#endif
diff --git a/include/dt-bindings/pci/pci.h b/include/dt-bindings/pci/pci.h
new file mode 100644
index 000000000..c2f053c9d
--- /dev/null
+++ b/include/dt-bindings/pci/pci.h
@@ -0,0 +1,30 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe biding header provides constants for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *              http://www.samsung.com
+ */
+
+#ifndef _DT_BINDINGS_EXYNOS_PCI_H
+#define _DT_BINDINGS_EXYNOS_PCI_H
+
+#define true		1
+#define false		0
+
+#define EP_NO_DEVICE		0
+#define EP_BCM_WIFI		1
+#define EP_SAMSUNG_S359		2
+#define EP_QC_MODEM		3
+#define EP_SAMSUNG_MODEM	4
+#define EP_QC_WIFI		5
+
+/*
+ * CAUTION - It SHOULD fit Target Link Speed Encoding
+ * in Link Control2 Register(offset 0xA0)
+ */
+#define LINK_SPEED_GEN1		1
+#define LINK_SPEED_GEN2		2
+#define LINK_SPEED_GEN3		3
+
+#endif
diff --git a/include/linux/exynos-pci-ctrl.h b/include/linux/exynos-pci-ctrl.h
new file mode 100644
index 000000000..d412b66b0
--- /dev/null
+++ b/include/linux/exynos-pci-ctrl.h
@@ -0,0 +1,27 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe driver header file for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *              http://www.samsung.com
+ */
+
+#ifndef __EXYNOS_PCIE_CTRL_H
+#define __EXYNOS_PCIE_CTRL_H
+
+/* PCIe L1SS Control ID */
+#define PCIE_L1SS_CTRL_ARGOS	(0x1 << 0)
+#define PCIE_L1SS_CTRL_BOOT	(0x1 << 1)
+#define PCIE_L1SS_CTRL_CAMERA	(0x1 << 2)
+#define PCIE_L1SS_CTRL_MODEM_IF	(0x1 << 3)
+#define PCIE_L1SS_CTRL_WIFI	(0x1 << 4)
+#define PCIE_L1SS_CTRL_TEST	(0x1 << 31)
+
+int exynos_pcie_pm_resume(int ch_num);
+int exynos_pcie_l1_exit(int ch_num);
+#if IS_ENABLED(CONFIG_PCI_EXYNOS)
+extern int exynos_pcie_l1ss_ctrl(int enable, int id);
+extern int exynos_pcie_rc_l1ss_ctrl(int enable, int id, int ch_num);
+#endif
+
+#endif
diff --git a/include/linux/exynos-pci-noti.h b/include/linux/exynos-pci-noti.h
new file mode 100644
index 000000000..b329e0272
--- /dev/null
+++ b/include/linux/exynos-pci-noti.h
@@ -0,0 +1,43 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * PCIe driver header file for gs101 SoC
+ *
+ * Copyright (C) 2020 Samsung Electronics Co., Ltd.
+ *              http://www.samsung.com
+ */
+
+#ifndef __PCI_NOTI_H
+#define __PCI_NOTI_H
+
+enum exynos_pcie_event {
+	EXYNOS_PCIE_EVENT_INVALID = 0,
+	EXYNOS_PCIE_EVENT_LINKDOWN = 0x1,
+	EXYNOS_PCIE_EVENT_LINKUP = 0x2,
+	EXYNOS_PCIE_EVENT_WAKEUP = 0x4,
+	EXYNOS_PCIE_EVENT_WAKE_RECOVERY = 0x8,
+	EXYNOS_PCIE_EVENT_NO_ACCESS = 0x10,
+	EXYNOS_PCIE_EVENT_CPL_TIMEOUT = 0x20,
+};
+
+enum exynos_pcie_trigger {
+	EXYNOS_PCIE_TRIGGER_CALLBACK,
+	EXYNOS_PCIE_TRIGGER_COMPLETION,
+};
+
+struct exynos_pcie_notify {
+	enum exynos_pcie_event event;
+	void *user;
+	void *data;
+	u32 options;
+};
+
+struct exynos_pcie_register_event {
+	u32 events;
+	void *user;
+	enum exynos_pcie_trigger mode;
+	void (*callback)(struct exynos_pcie_notify *notify);
+	struct exynos_pcie_notify notify;
+	struct completion *completion;
+	u32 options;
+};
+#endif
diff --git a/include/linux/soc/samsung/exynos-smc.h b/include/linux/soc/samsung/exynos-smc.h
new file mode 100644
index 000000000..87b0c5dee
--- /dev/null
+++ b/include/linux/soc/samsung/exynos-smc.h
@@ -0,0 +1,265 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2018 Samsung Electronics Co., Ltd.
+ *	      http://www.samsung.com/
+ *
+ * Exynos - SMC call
+ */
+
+#ifndef __EXYNOS_SMC_H__
+#define __EXYNOS_SMC_H__
+
+#include <linux/arm-smccc.h>
+
+/* For Power Management */
+#define SMC_CMD_SLEEP			(-3)
+#define SMC_CMD_CPU1BOOT		(-4)
+#define SMC_CMD_CPU0AFTR		(-5)
+#define SMC_CMD_SAVE			(-6)
+#define SMC_CMD_SHUTDOWN		(-7)
+
+#define SMC_CMD_CPUMAP			(-10)
+
+/* For Accessing CP15/SFR (General) */
+#define SMC_CMD_REG			(-101)
+
+/* For setting memory for debug */
+#define SMC_CMD_SET_DEBUG_MEM		(-120)
+#define SMC_CMD_GET_LOCKUP_REASON	(-121)
+#define SMC_CMD_KERNEL_PANIC_NOTICE	(0x8200007A)
+#define SMC_CMD_SET_SEH_ADDRESS		(-123)
+#define SMC_CMD_LOCKUP_NOTICE		(0x8200007C)
+#define SMC_CMD_GET_SJTAG_STATUS	(0x8200012E)
+
+/* For protecting kernel text area */
+#define SMC_CMD_PROTECT_KERNEL_TEXT	(-125)
+
+/* For Security Dump Manager */
+#define SMC_CMD_DUMP_SECURE_REGION	(-130)
+#define SMC_CMD_FLUSH_SECDRAM		(-131)
+
+/* For D-GPIO/D-TZPC */
+#define SMC_CMD_PREPARE_PD_ONOFF	(0x82000410)
+
+/* For accessing privileged registers */
+#define SMC_CMD_PRIV_REG		(0x82000504)
+
+/* For FMP/SMU Ctrl */
+#define SMC_CMD_FMP_SECURITY		(0xC2001810)
+#define SMC_CMD_FMP_DISK_KEY_STORED	(0xC2001820)
+#define SMC_CMD_FMP_DISK_KEY_SET	(0xC2001830)
+#define SMC_CMD_FMP_DISK_KEY_CLEAR	(0xC2001840)
+#define SMC_CMD_SMU			(0xC2001850)
+#define SMC_CMD_FMP_SMU_RESUME		(0xC2001860)
+#define SMC_CMD_FMP_SMU_DUMP		(0xC2001870)
+#define SMC_CMD_UFS_LOG			(0xC2001880)
+#define SMC_CMD_FMP_USE_OTP_KEY		(0xC2001890)
+
+/* SMU IDs (third parameter to FMP/SMU Ctrls) */
+#define SMU_EMBEDDED			0
+#define SMU_UFSCARD			1
+#define SMU_SDCARD			2
+
+/* SMU commands (second parameter to SMC_CMD_SMU) */
+#define SMU_INIT			0
+#define SMU_SET				1
+#define SMU_ABORT			2
+
+/* Fourth parameter to SMC_CMD_FMP_SECURITY */
+#define CFG_DESCTYPE_3			3
+
+/* Command ID for smc */
+#define SMC_PROTECTION_SET		(0x82002010)
+#define SMC_DRM_FW_LOADING		(0x82002011)
+#define SMC_DCPP_SUPPORT		(0x82002012)
+#define SMC_DRM_SECBUF_PROT		(0x82002020)
+#define SMC_DRM_SECBUF_UNPROT		(0x82002021)
+#define SMC_DRM_G2D_CMD_DATA		(0x8200202d)
+#define SMC_DRM_SECBUF_CFW_PROT		(0x82002030)
+#define SMC_DRM_SECBUF_CFW_UNPROT	(0x82002031)
+#define SMC_DRM_PPMP_PROT		(0x82002110)
+#define SMC_DRM_PPMP_UNPROT		(0x82002111)
+#define SMC_DRM_PPMP_MFCFW_PROT		(0x82002112)
+#define SMC_DRM_PPMP_MFCFW_UNPROT	(0x82002113)
+#define MC_FC_SET_CFW_PROT		(0x82002040)
+#define SMC_DRM_SEC_SMMU_INFO		(0x820020D0)
+#define MC_FC_DRM_SET_CFW_PROT		(0x10000000)
+#define SMC_SRPMB_WSM			(0x82003811)
+
+/* Deprecated */
+#define SMC_DRM_MAKE_PGTABLE		(0x81000003)
+#define SMC_DRM_CLEAR_PGTABLE		(0x81000004)
+#define SMC_MEM_PROT_SET		(0x81000005)
+#define SMC_DRM_SECMEM_INFO		(0x81000006)
+#define SMC_DRM_VIDEO_PROC		(0x81000007)
+
+/* Parameter for smc */
+#define SMC_PROTECTION_ENABLE		(1)
+#define SMC_PROTECTION_DISABLE		(0)
+
+/* For DTRNG Access */
+#define SMC_CMD_RANDOM			(0x82001012)
+
+/* For Secure log information */
+#define SMC_CMD_SEC_LOG_INFO		(0x82000610)
+
+/* For PPMPU fail information */
+#define SMC_CMD_GET_PPMPU_FAIL_INFO	(0x8200211A)
+#define SMC_CMD_CHECK_PPMPU_CH_NUM	(0x8200211B)
+
+/* For MMCache flush */
+#define SMC_CMD_MM_CACHE_OPERATION	(0x82000720)
+
+/* MACRO for SMC_CMD_REG */
+#define SMC_REG_CLASS_CP15		(0x0 << 30)
+#define SMC_REG_CLASS_SFR_W		(0x1 << 30)
+#define SMC_REG_CLASS_SFR_R		(0x3 << 30)
+#define SMC_REG_CLASS_MASK		(0x3 << 30)
+#define SMC_REG_ID_SFR_W(ADDR)		(SMC_REG_CLASS_SFR_W | ((ADDR) >> 2))
+#define SMC_REG_ID_SFR_R(ADDR)		(SMC_REG_CLASS_SFR_R | ((ADDR) >> 2))
+
+/* op type for SMC_CMD_SAVE and SMC_CMD_SHUTDOWN */
+#define OP_TYPE_CORE			(0x0)
+#define OP_TYPE_CLUSTER			(0x1)
+
+/* Power State required for SMC_CMD_SAVE and SMC_CMD_SHUTDOWN */
+#define SMC_POWERSTATE_SLEEP		(0x0)
+#define SMC_POWERSTATE_IDLE		(0x1)
+#define SMC_POWERSTATE_SWITCH		(0x2)
+
+/*
+ * For SMC CMD for SRPMB
+ */
+#define SMC_SRPMB_WSM			(0x82003811)
+
+/* For DTRNG Access */
+#define HWRNG_INIT			(0x0)
+#define HWRNG_EXIT			(0x1)
+#define HWRNG_GET_DATA			(0x2)
+#define HWRNG_RESUME			(0x3)
+
+/* For CFW group */
+#define CFW_DISP_RW			(3)
+#define CFW_VPP0			(5)
+#define CFW_VPP1			(6)
+
+#define SMC_TZPC_OK			(2)
+
+#define PROT_MFC			(0)
+#define PROT_MSCL0			(1)
+#define PROT_MSCL1			(2)
+#define PROT_L0				(3)
+#define PROT_L1				(4)
+#define PROT_L2				(5)
+#define PROT_L4				(6)
+#define PROT_L3				(9)
+#define PROT_L5				(10)
+#define PROT_L12			(11)
+#define PROT_G3D			(12)
+#define PROT_JPEG			(13)
+#define PROT_G2D			(14)
+#define PROT_MFC1			(23)
+
+#ifndef __ASSEMBLY__
+/* secure SysMMU SFR access */
+enum sec_sysmmu_sfr_access_t {
+	SEC_SMMU_SFR_READ,
+	SEC_SMMU_SFR_WRITE,
+};
+
+/* Return value from DRM LDFW */
+enum drmdrv_result_t {
+	DRMDRV_OK				= 0x0000,
+
+	/* Error lists for common driver */
+	E_DRMDRV_INVALID			= 0x1001,
+	E_DRMDRV_INVALID_ADDR_ALIGN		= 0x1002,
+	E_DRMDRV_INVALID_SIZE_ALIGN		= 0x1003,
+	E_DRMDRV_INVALID_MEMCPY_LENGTH		= 0x1004,
+	E_DRMDRV_ADDR_OUT_OF_DRAM		= 0x1005,
+	E_DRMDRV_ADDR_OUT_OF_SECMEM		= 0x1006,
+	E_DRMDRV_INVALID_ADDR			= 0x1007,
+	E_DRMDRV_INVALID_SIZE			= 0x1008,
+	E_DRMDRV_INVALID_CMD			= 0x1009,
+	E_DRMDRV_ADDR_OVERFLOWED		= 0x100A,
+	E_DRMDRV_ADDR_OVERLAP_SECOS		= 0x100B,
+
+	/* Error lists for TZASC driver */
+	E_DRMDRV_TZASC_ALIGN_CHECK		= 0x2001,
+	E_DRMDRV_TZASC_CONTIG_CHECK		= 0x2002,
+	E_DRMDRV_TZASC_GET_ORDER		= 0x2003,
+	E_DRMDRV_TZASC_INVALID_INDEX		= 0x2004,
+	E_DRMDRV_TZASC_INVALID_ENABLED		= 0x2005,
+	E_DRMDRV_TZASC_NOT_PROTECTED		= 0x2006,
+
+	/* Erorr lists for media driver */
+	E_DRMDRV_MEDIA_CHECK_POWER		= 0x3001,
+	E_DRMDRV_MEDIA_CHECK_CLOCK		= 0x3002,
+	E_DRMDRV_MEDIA_CHECK_SMMU_CLOCK		= 0x3003,
+	E_DRMDRV_MEDIA_CHECK_SMMU_ENABLED	= 0x3004,
+	E_DRMDRV_WB_CHECK_FAILED		= 0x3005,
+	E_DRMDRV_HDMI_WITH_NO_HDCP_FAILED	= 0x3006,
+	E_DRMDRV_MSCL_LOCAL_PATH_FAILED		= 0x3007,
+	E_DRMDRV_HDMI_POWER_OFF			= 0x3008,
+	E_DRMDRV_HDMI_CLOCK_OFF			= 0x3009,
+	E_DRMDRV_INVALID_REFCOUNT		= 0x300A,
+
+	/* Error lists for g2d driver */
+	E_DRMDRV_G2D_INVALID_PARAM		= 0x4001,
+	E_DRMDRV_G2D_BLIT_TIMEOUT		= 0x4002,
+
+	/* Error lists for RTC driver */
+	E_DRMDRV_GET_RTC_TIME			= 0x5001,
+	E_DRMDRV_SET_RTC_TIME			= 0x5002,
+	E_DRMDRV_GET_RTC_TICK_TIME		= 0x5003,
+	E_DRMDRV_SET_RTC_TICK_TIME		= 0x5004,
+
+	/* Error lists for CFW driver */
+	E_DRMDRV_CFW_ERROR			= 0x6001,
+	E_DRMDRV_CFW_BUFFER_LIST_FULL		= 0x6002,
+	E_DRMDRV_CFW_NOT_PROTECTED_BUFFER	= 0x6003,
+	E_DRMDRV_CFW_INVALID_DEV_ARG		= 0x6004,
+	E_DRMDRV_CFW_INIT_FAIL			= 0x6005,
+	E_DRMDRV_CFW_PROT_FAIL			= 0x6006,
+	E_DRMDRV_CFW_ENABLED_ALREADY		= 0x6007,
+	E_DRMDRV_CFW_NOT_EXIST_IN_CFW_BUFF_LIST	= 0x6008,
+	E_DRMDRV_CFW_DUPLICATED_BUFFER		= 0x6009,
+	E_DRMDRV_CFW_BUFFER_IS_OVERLAPPED	= 0x600A,
+
+	/* Error lists for Secure Buffer */
+	E_DRMDRV_DUPLICATED_BUFFER		= 0x7001,
+	E_DRMDRV_BUFFER_LIST_FULL		= 0x7002,
+	E_DRMDRV_NOT_EXIST_IN_SEC_BUFFER_LIST	= 0x7003,
+	E_DRMDRV_OVERLAP_RESERVED_ASP_REGION	= 0x7004,
+	E_DRMDRV_INVALID_BUFFER_TYPE		= 0x7005,
+
+	/* Error lists for MFC FW */
+	E_DRMDRV_MFC_FW_IS_NOT_PROTECTED	= 0x8001,
+	E_DRMDRV_MFC_FW_ARG_IS_NULL		= 0x8002,
+	E_DRMDRV_MFC_FW_INVALID_SIZE		= 0x8003,
+};
+
+static inline unsigned long exynos_smc(unsigned long cmd,
+				       unsigned long arg0,
+				       unsigned long arg1,
+				       unsigned long arg2)
+{
+	struct arm_smccc_res res;
+
+	arm_smccc_smc(cmd, arg0, arg1, arg2, 0, 0, 0, 0, &res);
+	return (unsigned long)res.a0;
+}
+
+static inline unsigned long exynos_smc_readsfr(unsigned long addr,
+					       unsigned long *val)
+{
+	struct arm_smccc_res res;
+
+	arm_smccc_smc(SMC_CMD_REG, addr, 0, 0, 0, 0, 0, 0, &res);
+	if (res.a0 == 0)
+		*val = (unsigned long)res.a2;
+	return (unsigned long)res.a0;
+}
+#endif
+
+#endif	/* __EXYNOS_SMC_H__ */
diff --git a/include/soc/google/shm_ipc.h b/include/soc/google/shm_ipc.h
new file mode 100644
index 000000000..af37c11c7
--- /dev/null
+++ b/include/soc/google/shm_ipc.h
@@ -0,0 +1,48 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2014-2019, Samsung Electronics.
+ *
+ */
+
+#ifndef __SHMEM_IPC_H__
+#define __SHMEM_IPC_H__
+
+#define MAX_CP_NUM	2
+
+#if IS_ENABLED(CONFIG_SHM_IPC)
+int cp_shmem_get_mem_map_on_cp_flag(u32 cp_num);
+void __iomem *cp_shmem_get_region(u32 cp, u32 idx);
+void __iomem *cp_shmem_get_nc_region(unsigned long base, u32 size);
+void cp_shmem_release_region(u32 cp, u32 idx);
+void cp_shmem_release_rmem(u32 cp, u32 idx, u32 headroom);
+unsigned long cp_shmem_get_base(u32 cp, u32 idx);
+u32 cp_shmem_get_size(u32 cp, u32 idx);
+
+/* Legacy functions */
+unsigned long shm_get_msi_base(void);
+void __iomem *shm_get_vss_region(void);
+unsigned long shm_get_vss_base(void);
+u32 shm_get_vss_size(void);
+void __iomem *shm_get_vparam_region(void);
+unsigned long shm_get_vparam_base(void);
+u32 shm_get_vparam_size(void);
+#else /* CONFIG_SHM_IPC */
+static inline int cp_shmem_get_mem_map_on_cp_flag(u32 cp_num) { return 0; }
+static inline void __iomem *cp_shmem_get_region(u32 cp, u32 idx) { return NULL; }
+static inline void __iomem *cp_shmem_get_nc_region(unsigned long base, u32 size) { return NULL; }
+static inline void cp_shmem_release_region(u32 cp, u32 idx) { return; }
+static inline void cp_shmem_release_rmem(u32 cp, u32 idx, u32 headroom) { return; }
+static inline unsigned long cp_shmem_get_base(u32 cp, u32 idx) { return 0; }
+static inline u32 cp_shmem_get_size(u32 cp, u32 idx) { return 0; }
+
+/* Legacy functions */
+static inline unsigned long shm_get_msi_base(void) { return 0; }
+static inline void __iomem *shm_get_vss_region(void) { return NULL; }
+static inline unsigned long shm_get_vss_base(void) { return 0; }
+static inline u32 shm_get_vss_size(void) { return 0; }
+static inline void __iomem *shm_get_vparam_region(void) { return NULL; }
+static inline unsigned long shm_get_vparam_base(void) { return 0; }
+static inline u32 shm_get_vparam_size(void) { return 0; }
+#endif /* CONFIG_SHM_IPC */
+
+#endif /* __SHMEM_IPC_H__ */
